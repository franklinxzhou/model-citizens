{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c521c1bd880490eae83728f0b1f536c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e66c1fe8102648079216119f8c22d957",
              "IPY_MODEL_65c4e492bdbf4555a9b248882fc561f6",
              "IPY_MODEL_f2672ad0063e4743b15498128b9d3c89"
            ],
            "layout": "IPY_MODEL_609bdc7207e348bda0e5c9cd1f1db45f"
          }
        },
        "e66c1fe8102648079216119f8c22d957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9564f52d72b471986f7ae1c1d708386",
            "placeholder": "​",
            "style": "IPY_MODEL_fd94a6ff7adc421b99b89fda3003441d",
            "value": "Best trial: 50. Best value: 0.606254: 100%"
          }
        },
        "65c4e492bdbf4555a9b248882fc561f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04b4415839d74752aa1826dfc7159340",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a3ee5e572e54286937d58c487860971",
            "value": 50
          }
        },
        "f2672ad0063e4743b15498128b9d3c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41483d0df22e41e187d6a353f994ff6f",
            "placeholder": "​",
            "style": "IPY_MODEL_ebc301b43f2f493d8c83a566c37877e0",
            "value": " 50/50 [18:14&lt;00:00, 20.67s/it]"
          }
        },
        "609bdc7207e348bda0e5c9cd1f1db45f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9564f52d72b471986f7ae1c1d708386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd94a6ff7adc421b99b89fda3003441d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04b4415839d74752aa1826dfc7159340": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a3ee5e572e54286937d58c487860971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41483d0df22e41e187d6a353f994ff6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebc301b43f2f493d8c83a566c37877e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# setup and configuration"
      ],
      "metadata": {
        "id": "saaCfeUqppjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# --- Setup ---\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 200)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "# --- Random Seed ---\n",
        "RANDOM_STATE = 123\n",
        "np.random.seed(RANDOM_STATE)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_STATE)\n",
        "\n",
        "# --- Model/Metrics ---\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, precision_score, recall_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# --- ML Models ---\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "\n",
        "# --- Tuning ---\n",
        "try:\n",
        "    import optuna\n",
        "except ImportError:\n",
        "    print(\"Installing Optuna...\")\n",
        "    !pip install optuna -q\n",
        "    import optuna\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "try:\n",
        "    import imblearn\n",
        "except ImportError:\n",
        "    print(\"Installing imbalanced-learn...\")\n",
        "    !pip install imbalanced-learn -q\n",
        "    import imblearn\n",
        "\n",
        "# --- Feature Engineering Helpers ---\n",
        "# REMOVED pyzipcode - we won't use 'state' feature due to data quality issues\n",
        "# try:\n",
        "#     from pyzipcode import ZipCodeDatabase\n",
        "# except ImportError:\n",
        "#     print(\"Installing pyzipcode...\")\n",
        "#     !pip install pyzipcode -q\n",
        "#     from pyzipcode import ZipCodeDatabase\n",
        "\n",
        "print(\"\\n✓ Setup complete. All libraries loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeUGEDgNpt_a",
        "outputId": "13da0425-0e6c-4983-fb86-4d70fabdffc2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Setup complete. All libraries loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data loading\n"
      ],
      "metadata": {
        "id": "KC0NdoqFqvBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Data ---\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    try:\n",
        "        train_df = pd.read_csv('Training_TriGuard.csv')\n",
        "        test_df = pd.read_csv('Testing_TriGuard.csv')\n",
        "        print(\"✓ Files loaded from local environment.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Please upload Training_TriGuard.csv:\")\n",
        "        uploaded_train = files.upload()\n",
        "        train_file = list(uploaded_train.keys())[0]\n",
        "        train_df = pd.read_csv(train_file)\n",
        "\n",
        "        print(\"\\nPlease upload Testing_TriGuard.csv:\")\n",
        "        uploaded_test = files.upload()\n",
        "        test_file = list(uploaded_test.keys())[0]\n",
        "        test_df = pd.read_csv(test_file)\n",
        "        print(\"✓ Files uploaded successfully.\")\n",
        "else:\n",
        "    # Local environment\n",
        "    possible_paths = [\n",
        "        'Training_TriGuard.csv',\n",
        "        'data/Training_TriGuard.csv',\n",
        "        '../Training_TriGuard.csv',\n",
        "        './Training_TriGuard.csv'\n",
        "    ]\n",
        "\n",
        "    train_path = None\n",
        "    test_path = None\n",
        "\n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            train_path = path\n",
        "            test_path = path.replace('Training', 'Testing')\n",
        "            if os.path.exists(test_path):\n",
        "                break\n",
        "\n",
        "    if train_path and test_path:\n",
        "        train_df = pd.read_csv(train_path)\n",
        "        test_df = pd.read_csv(test_path)\n",
        "        print(f\"✓ Files loaded from: {train_path} and {test_path}\")\n",
        "    else:\n",
        "        train_path = input(\"Enter path to Training_TriGuard.csv: \").strip()\n",
        "        test_path = input(\"Enter path to Testing_TriGuard.csv: \").strip()\n",
        "        train_df = pd.read_csv(train_path)\n",
        "        test_df = pd.read_csv(test_path)\n",
        "        print(\"✓ Files loaded successfully.\")\n",
        "\n",
        "# --- Critical Cleaning ---\n",
        "initial_train_count = len(train_df)\n",
        "train_df = train_df.dropna(subset=['subrogation'])\n",
        "print(f\"\\nCleaned training data: Removed {initial_train_count - len(train_df)} rows with NaN target.\")\n",
        "\n",
        "train_df['subrogation'] = train_df['subrogation'].astype(int)\n",
        "\n",
        "print(f\"\\n✓ Train shape: {train_df.shape}\")\n",
        "print(f\"✓ Test shape: {test_df.shape}\")\n",
        "print(f\"\\nTarget distribution (after cleaning):\")\n",
        "print(train_df['subrogation'].value_counts(normalize=True).to_string())\n",
        "\n",
        "test_ids = test_df['claim_number'].copy()\n",
        "\n",
        "print(\"✓ Data loading complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87vQef5vq4sW",
        "outputId": "76120800-5c8a-41de-def6-1ee2b7e033a4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Files loaded from local environment.\n",
            "\n",
            "Cleaned training data: Removed 2 rows with NaN target.\n",
            "\n",
            "✓ Train shape: (17999, 29)\n",
            "✓ Test shape: (12000, 28)\n",
            "\n",
            "Target distribution (after cleaning):\n",
            "subrogation\n",
            "0   0.771\n",
            "1   0.229\n",
            "✓ Data loading complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature engineering\n"
      ],
      "metadata": {
        "id": "sA9j7FNKrEal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineer(df):\n",
        "    \"\"\"Feature engineering WITHOUT vehicle_made_year/vehicle_age/state (data quality issues)\"\"\"\n",
        "    df_fe = df.copy()\n",
        "\n",
        "    # ========================================================================\n",
        "    # TEMPORAL FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['claim_date'] = pd.to_datetime(df_fe['claim_date'], errors='coerce')\n",
        "    df_fe['claim_year'] = df_fe['claim_date'].dt.year\n",
        "    df_fe['claim_month'] = df_fe['claim_date'].dt.month\n",
        "    df_fe['claim_day'] = df_fe['claim_date'].dt.day\n",
        "    df_fe['claim_quarter'] = df_fe['claim_date'].dt.quarter\n",
        "    df_fe['claim_dayofweek'] = df_fe['claim_date'].dt.dayofweek\n",
        "    df_fe['is_weekend'] = (df_fe['claim_dayofweek'] >= 5).astype(int)\n",
        "    df_fe['is_monday'] = (df_fe['claim_dayofweek'] == 0).astype(int)\n",
        "    df_fe['is_friday'] = (df_fe['claim_dayofweek'] == 4).astype(int)\n",
        "    df_fe['is_q4'] = (df_fe['claim_quarter'] == 4).astype(int)\n",
        "\n",
        "    # NEW: Time-of-day features from Doc 8\n",
        "    df_fe['claim_hour'] = df_fe['claim_date'].dt.hour\n",
        "    df_fe['rush_hour'] = df_fe['claim_hour'].isin([7, 8, 9, 16, 17, 18]).astype(int)\n",
        "    df_fe['late_night'] = df_fe['claim_hour'].isin([0, 1, 2, 3, 4, 5]).astype(int)\n",
        "\n",
        "    season_map = {\n",
        "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
        "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
        "        9: 'Fall', 10: 'Fall', 11: 'Fall',\n",
        "        12: 'Winter', 1: 'Winter', 2: 'Winter'\n",
        "    }\n",
        "    df_fe['season'] = df_fe['claim_month'].map(season_map).fillna('Unknown')\n",
        "\n",
        "    # ========================================================================\n",
        "    # DATA CLEANING\n",
        "    # ========================================================================\n",
        "    df_fe.loc[(df_fe['year_of_born'] < 1900) | (df_fe['year_of_born'] > 2025), 'year_of_born'] = np.nan\n",
        "\n",
        "    # ========================================================================\n",
        "    # BINARY CONVERSIONS (for interactions)\n",
        "    # ========================================================================\n",
        "    df_fe['witness_binary'] = (df_fe['witness_present_ind'] == 'Y').astype(int)\n",
        "    df_fe['police_binary'] = df_fe['policy_report_filed_ind']\n",
        "    df_fe['multicar_binary'] = df_fe['accident_type'].isin(['multi_vehicle_clear', 'multi_vehicle_unclear']).astype(int)\n",
        "    df_fe['highrisk_site_binary'] = df_fe['accident_site'].isin(['Highway/Intersection', 'Local']).astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # CRITICAL INTERACTION FEATURES (2-way)\n",
        "    # ========================================================================\n",
        "    df_fe['liab_x_witness'] = df_fe['liab_prct'] * df_fe['witness_binary']\n",
        "    df_fe['liab_x_police'] = df_fe['liab_prct'] * df_fe['police_binary']\n",
        "    df_fe['liab_x_multicar'] = df_fe['liab_prct'] * df_fe['multicar_binary']\n",
        "    df_fe['liab_x_highrisk_site'] = df_fe['liab_prct'] * df_fe['highrisk_site_binary']\n",
        "    df_fe['liab_x_evidence'] = df_fe['liab_prct'] * (df_fe['witness_binary'] + df_fe['police_binary'])\n",
        "    df_fe['liab_x_payout'] = df_fe['liab_prct'] * df_fe['claim_est_payout']\n",
        "    df_fe['liab_x_mileage'] = df_fe['liab_prct'] * df_fe['vehicle_mileage']\n",
        "\n",
        "    df_fe['witness_x_police'] = df_fe['witness_binary'] * df_fe['police_binary']\n",
        "    df_fe['witness_x_multicar'] = df_fe['witness_binary'] * df_fe['multicar_binary']\n",
        "    df_fe['police_x_multicar'] = df_fe['police_binary'] * df_fe['multicar_binary']\n",
        "    df_fe['multicar_x_highrisk'] = df_fe['multicar_binary'] * df_fe['highrisk_site_binary']\n",
        "    df_fe['weekend_highway'] = (df_fe['claim_dayofweek'] >= 5).astype(int) * (df_fe['accident_site'] == 'Highway/Intersection').astype(int)\n",
        "\n",
        "    # 3-way interaction\n",
        "    df_fe['witness_police_multicar'] = df_fe['witness_binary'] * df_fe['police_binary'] * df_fe['multicar_binary']\n",
        "\n",
        "    # ========================================================================\n",
        "    # POLYNOMIAL FEATURES (liability & key variables)\n",
        "    # ========================================================================\n",
        "    df_fe['liab_prct_squared'] = df_fe['liab_prct'] ** 2\n",
        "    df_fe['liab_prct_cubed'] = df_fe['liab_prct'] ** 3\n",
        "    df_fe['liab_prct_sqrt'] = np.sqrt(df_fe['liab_prct'])\n",
        "    df_fe['liab_prct_log'] = np.log1p(df_fe['liab_prct'])\n",
        "    df_fe['liab_inverse'] = 100 - df_fe['liab_prct']\n",
        "    df_fe['liab_inverse_squared'] = (100 - df_fe['liab_prct']) ** 2\n",
        "\n",
        "    df_fe['log_claim_est_payout'] = np.log1p(df_fe['claim_est_payout'])\n",
        "    df_fe['log_vehicle_mileage'] = np.log1p(df_fe['vehicle_mileage'])\n",
        "    df_fe['log_vehicle_price'] = np.log1p(df_fe['vehicle_price'])\n",
        "    df_fe['log_annual_income'] = np.log1p(df_fe['annual_income'])\n",
        "    df_fe['sqrt_vehicle_mileage'] = np.sqrt(df_fe['vehicle_mileage'])\n",
        "\n",
        "    # ========================================================================\n",
        "    # ACCIDENT TYPE FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['is_multi_vehicle_clear'] = (df_fe['accident_type'] == 'multi_vehicle_clear').astype(int)\n",
        "    df_fe['is_multi_vehicle_unclear'] = (df_fe['accident_type'] == 'multi_vehicle_unclear').astype(int)\n",
        "    df_fe['is_single_car'] = (df_fe['accident_type'] == 'single_car').astype(int)\n",
        "    df_fe['has_recovery_target'] = df_fe['multicar_binary']\n",
        "\n",
        "    df_fe['recovery_case_clarity'] = 0\n",
        "    df_fe.loc[df_fe['is_multi_vehicle_clear'] == 1, 'recovery_case_clarity'] = 3\n",
        "    df_fe.loc[df_fe['is_multi_vehicle_unclear'] == 1, 'recovery_case_clarity'] = 1\n",
        "\n",
        "    # ========================================================================\n",
        "    # LIABILITY BUCKETS (fine-grained)\n",
        "    # ========================================================================\n",
        "    df_fe['liab_under_10'] = (df_fe['liab_prct'] < 10).astype(int)\n",
        "    df_fe['liab_10_to_15'] = ((df_fe['liab_prct'] >= 10) & (df_fe['liab_prct'] < 15)).astype(int)\n",
        "    df_fe['liab_15_to_20'] = ((df_fe['liab_prct'] >= 15) & (df_fe['liab_prct'] < 20)).astype(int)\n",
        "    df_fe['liab_20_to_25'] = ((df_fe['liab_prct'] >= 20) & (df_fe['liab_prct'] < 25)).astype(int)\n",
        "    df_fe['liab_25_to_30'] = ((df_fe['liab_prct'] >= 25) & (df_fe['liab_prct'] < 30)).astype(int)\n",
        "    df_fe['liab_30_to_35'] = ((df_fe['liab_prct'] >= 30) & (df_fe['liab_prct'] < 35)).astype(int)\n",
        "    df_fe['liab_35_to_40'] = ((df_fe['liab_prct'] >= 35) & (df_fe['liab_prct'] < 40)).astype(int)\n",
        "    df_fe['liab_40_to_50'] = ((df_fe['liab_prct'] >= 40) & (df_fe['liab_prct'] < 50)).astype(int)\n",
        "    df_fe['liab_over_50'] = (df_fe['liab_prct'] >= 50).astype(int)\n",
        "\n",
        "    df_fe['not_at_fault'] = df_fe['liab_under_10']\n",
        "    df_fe['minimal_fault'] = (df_fe['liab_prct'] < 25).astype(int)\n",
        "    df_fe['low_fault'] = (df_fe['liab_prct'] < 35).astype(int)\n",
        "    df_fe['shared_fault'] = ((df_fe['liab_prct'] >= 35) & (df_fe['liab_prct'] < 50)).astype(int)\n",
        "    df_fe['high_fault'] = (df_fe['liab_prct'] >= 50).astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # EVIDENCE QUALITY FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['witness_present'] = df_fe['witness_binary']\n",
        "    df_fe['police_report'] = df_fe['police_binary']\n",
        "\n",
        "    df_fe['evidence_none'] = ((df_fe['witness_present'] == 0) & (df_fe['police_report'] == 0)).astype(int)\n",
        "    df_fe['evidence_weak'] = (((df_fe['witness_present'] == 1) & (df_fe['police_report'] == 0)) |\n",
        "                              ((df_fe['witness_present'] == 0) & (df_fe['police_report'] == 1))).astype(int)\n",
        "    df_fe['evidence_strong'] = ((df_fe['witness_present'] == 1) & (df_fe['police_report'] == 1)).astype(int)\n",
        "    df_fe['evidence_very_strong'] = ((df_fe['witness_present'] == 1) & (df_fe['police_report'] == 1) &\n",
        "                                      (df_fe['liab_prct'] < 20)).astype(int)\n",
        "    df_fe['evidence_score'] = df_fe['witness_present'] + df_fe['police_report']\n",
        "\n",
        "    # ========================================================================\n",
        "    # ACCIDENT SITE FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['high_risk_site'] = df_fe['highrisk_site_binary']\n",
        "    df_fe['parking_accident'] = (df_fe['accident_site'] == 'Parking Area').astype(int)\n",
        "    df_fe['unknown_site'] = (df_fe['accident_site'] == 'Unknown').astype(int)\n",
        "    df_fe['highway_accident'] = (df_fe['accident_site'] == 'Highway/Intersection').astype(int)\n",
        "    df_fe['local_accident'] = (df_fe['accident_site'] == 'Local').astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # DRIVER AGE & EXPERIENCE\n",
        "    # ========================================================================\n",
        "    df_fe['driver_age'] = df_fe['claim_year'] - df_fe['year_of_born']\n",
        "    df_fe.loc[(df_fe['driver_age'] < 16) | (df_fe['driver_age'] > 100), 'driver_age'] = np.nan\n",
        "\n",
        "    df_fe['young_driver'] = ((df_fe['driver_age'] >= 16) & (df_fe['driver_age'] <= 25)).astype(int)\n",
        "    df_fe['prime_driver'] = ((df_fe['driver_age'] > 25) & (df_fe['driver_age'] <= 45)).astype(int)\n",
        "    df_fe['middle_age_driver'] = ((df_fe['driver_age'] > 45) & (df_fe['driver_age'] <= 65)).astype(int)\n",
        "    df_fe['senior_driver'] = (df_fe['driver_age'] > 65).astype(int)\n",
        "\n",
        "    df_fe['driving_experience'] = (df_fe['driver_age'] - df_fe['age_of_DL']).clip(lower=0)\n",
        "    df_fe.loc[df_fe['driving_experience'] < 0, 'driving_experience'] = np.nan\n",
        "\n",
        "    df_fe['novice_driver'] = (df_fe['driving_experience'] < 3).astype(int)\n",
        "    df_fe['experienced_driver'] = ((df_fe['driving_experience'] >= 3) & (df_fe['driving_experience'] <= 10)).astype(int)\n",
        "    df_fe['veteran_driver'] = (df_fe['driving_experience'] > 10).astype(int)\n",
        "\n",
        "    df_fe['experience_x_safety'] = df_fe['driving_experience'] * df_fe['safety_rating']\n",
        "    df_fe['driver_age_x_safety'] = df_fe['driver_age'] * df_fe['safety_rating']\n",
        "\n",
        "    # NEW: Driver risk interactions from Doc 8\n",
        "    df_fe['young_novice'] = df_fe['young_driver'] * df_fe['novice_driver']\n",
        "\n",
        "    # ========================================================================\n",
        "    # VEHICLE FEATURES (without vehicle_age)\n",
        "    # ========================================================================\n",
        "    df_fe['luxury_vehicle'] = (df_fe['vehicle_price'] > 50000).astype(int)\n",
        "    df_fe['mid_price_vehicle'] = ((df_fe['vehicle_price'] >= 20000) & (df_fe['vehicle_price'] <= 50000)).astype(int)\n",
        "    df_fe['economy_vehicle'] = (df_fe['vehicle_price'] < 20000).astype(int)\n",
        "\n",
        "    df_fe['heavy_vehicle'] = (df_fe['vehicle_weight'] > 30000).astype(int)\n",
        "    df_fe['light_vehicle'] = (df_fe['vehicle_weight'] < 15000).astype(int)\n",
        "    df_fe['medium_weight'] = ((df_fe['vehicle_weight'] >= 15000) & (df_fe['vehicle_weight'] <= 30000)).astype(int)\n",
        "\n",
        "    df_fe['is_large_vehicle'] = (df_fe['vehicle_category'] == 'Large').astype(int)\n",
        "    df_fe['is_compact_vehicle'] = (df_fe['vehicle_category'] == 'Compact').astype(int)\n",
        "    df_fe['is_medium_vehicle'] = (df_fe['vehicle_category'] == 'Medium').astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # CLAIM CHARACTERISTICS\n",
        "    # ========================================================================\n",
        "    df_fe['high_mileage'] = (df_fe['vehicle_mileage'] > 100000).astype(int)\n",
        "    df_fe['low_mileage'] = (df_fe['vehicle_mileage'] < 50000).astype(int)\n",
        "    df_fe['very_high_mileage'] = (df_fe['vehicle_mileage'] > 150000).astype(int)\n",
        "    df_fe['medium_mileage'] = ((df_fe['vehicle_mileage'] >= 50000) & (df_fe['vehicle_mileage'] <= 100000)).astype(int)\n",
        "\n",
        "    df_fe['frequent_claimer'] = (df_fe['past_num_of_claims'] > 5).astype(int)\n",
        "    df_fe['moderate_claimer'] = ((df_fe['past_num_of_claims'] >= 1) & (df_fe['past_num_of_claims'] <= 5)).astype(int)\n",
        "    df_fe['first_time_claimer'] = (df_fe['past_num_of_claims'] == 0).astype(int)\n",
        "    df_fe['very_frequent_claimer'] = (df_fe['past_num_of_claims'] > 10).astype(int)\n",
        "\n",
        "    df_fe['large_payout'] = (df_fe['claim_est_payout'] > 5000).astype(int)\n",
        "    df_fe['medium_payout'] = ((df_fe['claim_est_payout'] >= 2000) & (df_fe['claim_est_payout'] <= 5000)).astype(int)\n",
        "    df_fe['small_payout'] = (df_fe['claim_est_payout'] < 2000).astype(int)\n",
        "    df_fe['very_large_payout'] = (df_fe['claim_est_payout'] > 8000).astype(int)\n",
        "\n",
        "    df_fe['safety_x_prior_claims'] = df_fe['safety_rating'] / (1 + df_fe['past_num_of_claims'])\n",
        "    df_fe['mileage_x_claims'] = df_fe['vehicle_mileage'] * df_fe['past_num_of_claims']\n",
        "\n",
        "    # NEW: Claims risk interactions from Doc 8\n",
        "    df_fe['senior_frequent_claimer'] = df_fe['senior_driver'] * df_fe['frequent_claimer']\n",
        "    df_fe['low_safety_high_claims'] = ((df_fe['safety_rating'] < 60) & (df_fe['past_num_of_claims'] > 3)).astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # RATIO FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['payout_to_price_ratio'] = df_fe['claim_est_payout'] / (df_fe['vehicle_price'] + 1)\n",
        "    df_fe['severe_damage'] = (df_fe['payout_to_price_ratio'] > 0.3).astype(int)\n",
        "    df_fe['moderate_damage'] = ((df_fe['payout_to_price_ratio'] >= 0.1) & (df_fe['payout_to_price_ratio'] <= 0.3)).astype(int)\n",
        "    df_fe['minor_damage'] = (df_fe['payout_to_price_ratio'] < 0.1).astype(int)\n",
        "\n",
        "    df_fe['income_to_vehicle_price'] = df_fe['annual_income'] / (df_fe['vehicle_price'] + 1)\n",
        "    df_fe['can_afford_vehicle'] = (df_fe['income_to_vehicle_price'] >= 0.5).astype(int)\n",
        "    df_fe['expensive_for_income'] = (df_fe['income_to_vehicle_price'] < 0.3).astype(int)\n",
        "\n",
        "    df_fe['claims_per_year_driving'] = df_fe['past_num_of_claims'] / (df_fe['driving_experience'] + 1)\n",
        "    df_fe['claim_frequency_high'] = (df_fe['claims_per_year_driving'] > 0.5).astype(int)\n",
        "\n",
        "    df_fe['safety_to_liability'] = df_fe['safety_rating'] / (df_fe['liab_prct'] + 1)\n",
        "    df_fe['payout_to_income'] = df_fe['claim_est_payout'] / (df_fe['annual_income'] + 1)\n",
        "    df_fe['mileage_to_price'] = df_fe['vehicle_mileage'] / (df_fe['vehicle_price'] + 1)\n",
        "    df_fe['weight_to_price'] = df_fe['vehicle_weight'] / (df_fe['vehicle_price'] + 1)\n",
        "\n",
        "    # ========================================================================\n",
        "    # POLICYHOLDER CHARACTERISTICS\n",
        "    # ========================================================================\n",
        "    df_fe['high_income'] = (df_fe['annual_income'] > 70000).astype(int)\n",
        "    df_fe['mid_income'] = ((df_fe['annual_income'] >= 40000) & (df_fe['annual_income'] <= 70000)).astype(int)\n",
        "    df_fe['low_income'] = (df_fe['annual_income'] < 40000).astype(int)\n",
        "    df_fe['very_high_income'] = (df_fe['annual_income'] > 100000).astype(int)\n",
        "\n",
        "    df_fe['high_safety_rating'] = (df_fe['safety_rating'] > 80).astype(int)\n",
        "    df_fe['low_safety_rating'] = (df_fe['safety_rating'] < 60).astype(int)\n",
        "    df_fe['very_high_safety'] = (df_fe['safety_rating'] > 90).astype(int)\n",
        "    df_fe['medium_safety'] = ((df_fe['safety_rating'] >= 60) & (df_fe['safety_rating'] <= 80)).astype(int)\n",
        "\n",
        "    df_fe['contact_available'] = df_fe['email_or_tel_available']\n",
        "    df_fe['has_education'] = df_fe['high_education_ind']\n",
        "    df_fe['recent_move'] = df_fe['address_change_ind']\n",
        "    df_fe['home_owner'] = (df_fe['living_status'] == 'Own').astype(int)\n",
        "    df_fe['renter'] = (df_fe['living_status'] == 'Rent').astype(int)\n",
        "    df_fe['female'] = (df_fe['gender'] == 'F').astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # CHANNEL FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['via_broker'] = (df_fe['channel'] == 'Broker').astype(int)\n",
        "    df_fe['via_online'] = (df_fe['channel'] == 'Online').astype(int)\n",
        "    df_fe['via_phone'] = (df_fe['channel'] == 'Phone').astype(int)\n",
        "    df_fe['in_network_repair'] = (df_fe['in_network_bodyshop'] == 'yes').astype(int)\n",
        "    df_fe['out_network_repair'] = (df_fe['in_network_bodyshop'] == 'no').astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # COMPOSITE RECOVERY SCORES\n",
        "    # ========================================================================\n",
        "    liability_score = np.sqrt((100 - df_fe['liab_prct']) / 100.0)\n",
        "    evidence_score_composite = (df_fe['evidence_none'] * 0.0 + df_fe['evidence_weak'] * 0.4 +\n",
        "                      df_fe['evidence_strong'] * 0.7 + df_fe['evidence_very_strong'] * 1.0)\n",
        "    clarity_score = df_fe['recovery_case_clarity'] / 3.0\n",
        "    site_score = df_fe['high_risk_site'] * 0.7 + (1 - df_fe['unknown_site']) * 0.3\n",
        "\n",
        "    df_fe['recovery_feasibility_score'] = (0.35 * liability_score + 0.30 * df_fe['has_recovery_target'] +\n",
        "                                           0.20 * evidence_score_composite + 0.10 * clarity_score + 0.05 * site_score)\n",
        "\n",
        "    # NEW: Alternative recovery potential score from Doc 8\n",
        "    df_fe['recovery_potential'] = (\n",
        "        (100 - df_fe['liab_prct']) * 0.4 +\n",
        "        df_fe['evidence_score'] * 20 * 0.3 +\n",
        "        df_fe['multicar_binary'] * 30 * 0.2 +\n",
        "        (df_fe['claim_est_payout'] / 100) * 0.1\n",
        "    )\n",
        "\n",
        "    # ========================================================================\n",
        "    # DOMAIN LOGIC FLAGS (CRITICAL FOR F1)\n",
        "    # ========================================================================\n",
        "    df_fe['perfect_case'] = ((df_fe['liab_prct'] < 15) & (df_fe['witness_present'] == 1) &\n",
        "                             (df_fe['police_report'] == 1) & (df_fe['has_recovery_target'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['strong_case'] = ((df_fe['liab_prct'] < 25) & (df_fe['evidence_strong'] == 1) &\n",
        "                            (df_fe['has_recovery_target'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['good_case'] = ((df_fe['liab_prct'] < 35) & (df_fe['evidence_score'] >= 1) &\n",
        "                          (df_fe['has_recovery_target'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['weak_case'] = ((df_fe['liab_prct'] > 40) | (df_fe['is_single_car'] == 1) |\n",
        "                          (df_fe['evidence_none'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['no_case'] = ((df_fe['liab_prct'] > 60) | ((df_fe['is_single_car'] == 1) & (df_fe['evidence_none'] == 1))).astype(int)\n",
        "\n",
        "    df_fe['high_value_opportunity'] = ((df_fe['claim_est_payout'] > 3000) & (df_fe['liab_prct'] < 30) &\n",
        "                                       (df_fe['has_recovery_target'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['slam_dunk_case'] = ((df_fe['liab_prct'] < 10) & (df_fe['witness_present'] == 1) &\n",
        "                               (df_fe['police_report'] == 1) & (df_fe['multicar_binary'] == 1) &\n",
        "                               (df_fe['high_risk_site'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['low_liab_high_payout'] = ((df_fe['liab_prct'] < 20) & (df_fe['claim_est_payout'] > 5000)).astype(int)\n",
        "    df_fe['clear_fault_case'] = ((df_fe['liab_prct'] < 15) & (df_fe['multicar_binary'] == 1)).astype(int)\n",
        "    df_fe['high_mileage_low_fault'] = ((df_fe['vehicle_mileage'] > 100000) & (df_fe['liab_prct'] < 30)).astype(int)\n",
        "\n",
        "    # NEW: More interaction flags from Doc 8\n",
        "    df_fe['low_liab_witness_police'] = ((df_fe['liab_prct'] < 20) & (df_fe['witness_binary'] == 1) &\n",
        "                                         (df_fe['police_binary'] == 1)).astype(int)\n",
        "    df_fe['multicar_low_liab'] = ((df_fe['multicar_binary'] == 1) & (df_fe['liab_prct'] < 25)).astype(int)\n",
        "    df_fe['high_payout_evidence'] = ((df_fe['claim_est_payout'] > 5000) & (df_fe['evidence_score'] >= 1)).astype(int)\n",
        "    df_fe['severe_damage_low_fault'] = ((df_fe['payout_to_price_ratio'] > 0.3) & (df_fe['liab_prct'] < 30)).astype(int)\n",
        "    df_fe['minor_damage_high_fault'] = ((df_fe['payout_to_price_ratio'] < 0.1) & (df_fe['liab_prct'] > 50)).astype(int)\n",
        "\n",
        "    # --- Temporal & Behavior Dynamics ---\n",
        "    df_fe['claim_early_in_year'] = (df_fe['claim_month'] <= 3).astype(int)\n",
        "    df_fe['claim_end_of_year'] = (df_fe['claim_month'] >= 10).astype(int)\n",
        "    df_fe['weekend_parking'] = df_fe['is_weekend'] * (df_fe['accident_site'] == 'Parking Area').astype(int)\n",
        "    df_fe['winter_claim_high_payout'] = ((df_fe['season'] == 'Winter') & (df_fe['claim_est_payout'] > 5000)).astype(int)\n",
        "\n",
        "    # --- Vehicle Utilization Proxies (without vehicle_age) ---\n",
        "    df_fe['mileage_x_weight'] = df_fe['vehicle_mileage'] * df_fe['vehicle_weight']\n",
        "    df_fe['mileage_per_dollar'] = df_fe['vehicle_mileage'] / (df_fe['vehicle_price'] + 1)\n",
        "    df_fe['payout_to_weight'] = df_fe['claim_est_payout'] / (df_fe['vehicle_weight'] + 1)\n",
        "\n",
        "    # --- Policyholder Risk Profile ---\n",
        "    df_fe['unstable_policyholder'] = ((df_fe['recent_move'] == 1) & (df_fe['renter'] == 1)).astype(int)\n",
        "    df_fe['financial_stress_risk'] = ((df_fe['expensive_for_income'] == 1) & (df_fe['large_payout'] == 1)).astype(int)\n",
        "    df_fe['young_driver_highway'] = df_fe['young_driver'] * df_fe['highway_accident']\n",
        "    df_fe['senior_driver_parking'] = df_fe['senior_driver'] * df_fe['parking_accident']\n",
        "\n",
        "    # --- Liability & Evidence Interaction Insights ---\n",
        "    df_fe['low_liab_weak_evidence'] = ((df_fe['liab_prct'] < 20) & (df_fe['evidence_weak'] == 1)).astype(int)\n",
        "    df_fe['high_liab_strong_evidence'] = ((df_fe['liab_prct'] > 50) & (df_fe['evidence_strong'] == 1)).astype(int)\n",
        "\n",
        "    # Composite confidence / case quality index\n",
        "    df_fe['case_confidence_score'] = (\n",
        "        0.4 * (100 - df_fe['liab_prct']) / 100 +\n",
        "        0.4 * df_fe['evidence_score'] / 2 +\n",
        "        0.2 * df_fe['recovery_case_clarity'] / 3\n",
        "    )\n",
        "\n",
        "    # --- Statistical Normalization & Percentile Features ---\n",
        "    for col in ['claim_est_payout', 'vehicle_mileage', 'annual_income']:\n",
        "        df_fe[f'{col}_z'] = (df_fe[col] - df_fe[col].mean()) / (df_fe[col].std() + 1e-9)\n",
        "\n",
        "    try:\n",
        "        df_fe['liab_percentile'] = pd.qcut(df_fe['liab_prct'], 10, labels=False, duplicates='drop')\n",
        "        df_fe['payout_percentile'] = pd.qcut(df_fe['claim_est_payout'], 10, labels=False, duplicates='drop')\n",
        "    except Exception:\n",
        "        df_fe['liab_percentile'] = np.nan\n",
        "        df_fe['payout_percentile'] = np.nan\n",
        "\n",
        "    # --- Aggregate / Hybrid Indices ---\n",
        "    df_fe['case_strength_index'] = df_fe['evidence_score'] * (1 - df_fe['liab_prct'] / 100)\n",
        "    df_fe['financial_exposure_index'] = (\n",
        "        (df_fe['claim_est_payout'] / (df_fe['annual_income'] + 1)) * (1 + df_fe['liab_prct'] / 100)\n",
        "    )\n",
        "    df_fe['behavioral_risk_index'] = (\n",
        "        df_fe['claims_per_year_driving'] * (100 - df_fe['safety_rating']) / 100\n",
        "    )\n",
        "\n",
        "    return df_fe\n",
        "\n",
        "print(\"✓ Feature engineering function defined (190+ features)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGix2VMUrOME",
        "outputId": "a42b2f29-6db9-40c2-9c2f-67fc67f5990c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Feature engineering function defined (190+ features)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pre-modeling with target encoding\n"
      ],
      "metadata": {
        "id": "tU8YI01crqp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Running Feature Engineering on train and test data...\")\n",
        "\n",
        "train_fe = feature_engineer(train_df)\n",
        "test_fe = feature_engineer(test_df)\n",
        "print(\"✓ Feature engineering complete.\")\n",
        "\n",
        "# Define Categorical Feature Lists\n",
        "features_to_target_encode = [\n",
        "    'gender', 'living_status', 'accident_site',\n",
        "    'channel', 'vehicle_category', 'vehicle_color', 'accident_type',\n",
        "    'in_network_bodyshop', 'season', 'zip_code'\n",
        "]\n",
        "\n",
        "# Apply Target Encoding\n",
        "print(f\"\\nApplying Smoothed Target Encoding to {len(features_to_target_encode)} features...\")\n",
        "global_mean = train_fe['subrogation'].mean()\n",
        "categorical_features_for_lgbm = []\n",
        "\n",
        "for col in features_to_target_encode:\n",
        "    target_mean = train_fe.groupby(col)['subrogation'].mean()\n",
        "    category_counts = train_fe.groupby(col).size()\n",
        "    smoothing = 20\n",
        "\n",
        "    smoothed_mean = (target_mean * category_counts + global_mean * smoothing) / (category_counts + smoothing)\n",
        "\n",
        "    new_col_name = f'{col}_target_enc'\n",
        "    train_fe[new_col_name] = train_fe[col].map(smoothed_mean)\n",
        "    test_fe[new_col_name] = test_fe[col].map(smoothed_mean)\n",
        "\n",
        "    test_fe[new_col_name] = test_fe[new_col_name].fillna(global_mean)\n",
        "\n",
        "    categorical_features_for_lgbm.append(new_col_name)\n",
        "\n",
        "print(\"✓ Target encoding complete.\")\n",
        "\n",
        "# Create Final X, y, and X_test\n",
        "y_all = train_fe['subrogation'].copy()\n",
        "\n",
        "drop_cols = [\n",
        "    'subrogation', 'claim_number', 'claim_date', 'year_of_born',\n",
        "    'witness_present_ind', 'policy_report_filed_ind',\n",
        "    'vehicle_made_year',  # Bad data quality\n",
        "    'claim_hour'  # Drop raw hour (we keep rush_hour and late_night flags)\n",
        "]\n",
        "drop_cols.extend(features_to_target_encode)\n",
        "\n",
        "feature_cols = [col for col in train_fe.columns if col not in drop_cols]\n",
        "X_all = train_fe[feature_cols].copy()\n",
        "X_test_all = test_fe[feature_cols].copy()\n",
        "\n",
        "# Apply Label Encoding (if any object columns remain)\n",
        "other_cat_cols = X_all.select_dtypes(include='object').columns.tolist()\n",
        "if other_cat_cols:\n",
        "    print(f\"\\nApplying Label Encoding to {len(other_cat_cols)} remaining features...\")\n",
        "    for col in other_cat_cols:\n",
        "        le = LabelEncoder()\n",
        "        all_values = pd.concat([X_all[col].astype(str), X_test_all[col].astype(str)]).unique()\n",
        "        le.fit(all_values)\n",
        "        X_all[col] = le.transform(X_all[col].astype(str))\n",
        "        X_test_all[col] = le.transform(X_test_all[col].astype(str))\n",
        "    print(\"✓ Label encoding complete.\")\n",
        "\n",
        "# Impute NaN values with median\n",
        "print(\"\\nImputing NaN values with the median from the training data...\")\n",
        "X_all_median = X_all.median()\n",
        "X_all = X_all.fillna(X_all_median)\n",
        "X_test_all = X_test_all.fillna(X_all_median)\n",
        "print(\"✓ NaN values imputed.\")\n",
        "\n",
        "# Calculate scale_pos_weight\n",
        "scale_pos_weight = (y_all == 0).sum() / (y_all == 1).sum()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRE-MODELING COMPLETE\")\n",
        "print(f\"✓ X_all shape: {X_all.shape}\")\n",
        "print(f\"✓ y_all shape: {y_all.shape}\")\n",
        "print(f\"✓ X_test_all shape: {X_test_all.shape}\")\n",
        "print(f\"✓ Total features: {len(feature_cols)}\")\n",
        "print(f\"✓ scale_pos_weight (for F1 score): {scale_pos_weight:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84kTpL7TrtRZ",
        "outputId": "e01acd01-9a5e-457f-9617-57585c283d1d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Running Feature Engineering on train and test data...\n",
            "✓ Feature engineering complete.\n",
            "\n",
            "Applying Smoothed Target Encoding to 10 features...\n",
            "✓ Target encoding complete.\n",
            "\n",
            "Applying Label Encoding to 1 remaining features...\n",
            "✓ Label encoding complete.\n",
            "\n",
            "Imputing NaN values with the median from the training data...\n",
            "✓ NaN values imputed.\n",
            "\n",
            "================================================================================\n",
            "PRE-MODELING COMPLETE\n",
            "✓ X_all shape: (17999, 201)\n",
            "✓ y_all shape: (17999,)\n",
            "✓ X_test_all shape: (12000, 201)\n",
            "✓ Total features: 201\n",
            "✓ scale_pos_weight (for F1 score): 3.3740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# optuna + smote\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ylBTKRGnsYTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install if needed\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "except ImportError:\n",
        "    print(\"\\nInstalling imbalanced-learn...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'imbalanced-learn', '-q'])\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    print(\"✓ imbalanced-learn installed\")\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score, roc_auc_score, precision_recall_curve,\n",
        "    f1_score, precision_score, recall_score, confusion_matrix\n",
        ")\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"STEP 1: OPTUNA HYPERPARAMETER TUNING WITH SMOTE\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "def objective_smote(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
        "        'max_depth': trial.suggest_int('max_depth', 4, 15),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
        "        # NOTE: scale_pos_weight REMOVED when using SMOTE\n",
        "        'random_state': RANDOM_STATE,\n",
        "        'verbose': -1,\n",
        "        'n_jobs': -1,\n",
        "    }\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "    fold_scores = []\n",
        "    smote = SMOTE(random_state=RANDOM_STATE, sampling_strategy=1.0)  # Balance to 1:1\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_all, y_all), start=1):\n",
        "        X_tr, X_va = X_all.iloc[train_idx], X_all.iloc[val_idx]\n",
        "        y_tr, y_va = y_all.iloc[train_idx], y_all.iloc[val_idx]\n",
        "\n",
        "        # Apply SMOTE only to training fold\n",
        "        X_tr_resampled, y_tr_resampled = smote.fit_resample(X_tr, y_tr)\n",
        "\n",
        "        model = lgb.LGBMClassifier(**params)\n",
        "        model.fit(\n",
        "            X_tr_resampled, y_tr_resampled,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "            eval_metric=\"auc\",\n",
        "            callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(period=0)]\n",
        "        )\n",
        "\n",
        "        if hasattr(model, \"best_iteration_\") and model.best_iteration_ is not None:\n",
        "            proba = model.predict_proba(X_va, num_iteration=model.best_iteration_)[:, 1]\n",
        "        else:\n",
        "            proba = model.predict_proba(X_va)[:, 1]\n",
        "\n",
        "        score = average_precision_score(y_va, proba)\n",
        "        fold_scores.append(score)\n",
        "\n",
        "        trial.report(float(np.mean(fold_scores)), step=fold_idx)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    return float(np.mean(fold_scores))\n",
        "\n",
        "print(\"\\nRunning Optuna with SMOTE (50 trials)...\")\n",
        "print(\"This will take longer due to synthetic sample generation...\")\n",
        "\n",
        "storage_smote = \"sqlite:///lgbm_optuna_smote.db\"\n",
        "study_smote = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=\"lgbm_smote_prauc\",\n",
        "    storage=storage_smote,\n",
        "    load_if_exists=True,\n",
        "    pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=1, reduction_factor=3)\n",
        ")\n",
        "\n",
        "try:\n",
        "    study_smote.optimize(objective_smote, n_trials=50, show_progress_bar=True)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"OPTUNA RESULTS (SMOTE)\")\n",
        "print(\"=\"*90)\n",
        "print(f\"✓ Best Mean PR-AUC (5-fold, SMOTE): {study_smote.best_value:.4f}\")\n",
        "print(\"\\n✓ Best parameters:\")\n",
        "for k, v in study_smote.best_params.items():\n",
        "    print(f\"  - {k}: {v}\")\n",
        "\n",
        "best_lgbm_params_smote = study_smote.best_params.copy()\n",
        "best_lgbm_params_smote.update({\n",
        "    'random_state': RANDOM_STATE,\n",
        "    'verbose': -1\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460,
          "referenced_widgets": [
            "0c521c1bd880490eae83728f0b1f536c",
            "e66c1fe8102648079216119f8c22d957",
            "65c4e492bdbf4555a9b248882fc561f6",
            "f2672ad0063e4743b15498128b9d3c89",
            "609bdc7207e348bda0e5c9cd1f1db45f",
            "c9564f52d72b471986f7ae1c1d708386",
            "fd94a6ff7adc421b99b89fda3003441d",
            "04b4415839d74752aa1826dfc7159340",
            "1a3ee5e572e54286937d58c487860971",
            "41483d0df22e41e187d6a353f994ff6f",
            "ebc301b43f2f493d8c83a566c37877e0"
          ]
        },
        "id": "wY2Beo4lsdcS",
        "outputId": "e03ae9e0-6430-4cc8-8c54-dae91ca4f6e1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "STEP 1: OPTUNA HYPERPARAMETER TUNING WITH SMOTE\n",
            "==========================================================================================\n",
            "\n",
            "Running Optuna with SMOTE (50 trials)...\n",
            "This will take longer due to synthetic sample generation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c521c1bd880490eae83728f0b1f536c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "OPTUNA RESULTS (SMOTE)\n",
            "==========================================================================================\n",
            "✓ Best Mean PR-AUC (5-fold, SMOTE): 0.6063\n",
            "\n",
            "✓ Best parameters:\n",
            "  - n_estimators: 1531\n",
            "  - learning_rate: 0.038660328981200656\n",
            "  - num_leaves: 100\n",
            "  - max_depth: 4\n",
            "  - min_child_samples: 82\n",
            "  - subsample: 0.7274453329581444\n",
            "  - colsample_bytree: 0.6636273421672354\n",
            "  - reg_alpha: 0.7603683245942066\n",
            "  - reg_lambda: 0.7904730731479439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# calibrated oof\n"
      ],
      "metadata": {
        "id": "S4rxwdJ1XL52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## CELL 6: GET ROBUST CALIBRATED OOF PREDICTIONS (LGBM + SMOTE)\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\"STEP 2: GENERATE CALIBRATED OOF PREDICTIONS WITH SMOTE\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "# NEW: Import F1 metrics\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_curve, f1_score, precision_score, recall_score,\n",
        "    roc_auc_score, average_precision_score\n",
        ")\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_calibrated_oof_preds_lgbm_smote(params, X, y, random_state=123):\n",
        "    \"\"\"\n",
        "    5-fold CV with SMOTE and calibration inside each fold.\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "    oof_preds = np.zeros(len(y), dtype=float)\n",
        "    smote = SMOTE(random_state=random_state, sampling_strategy=1.0)\n",
        "\n",
        "    print(f\"\\nGenerating calibrated OOF predictions with SMOTE...\")\n",
        "    print(f\"Total samples: {len(y)}\")\n",
        "    print(f\"Original class balance: {(y==1).sum()/len(y)*100:.2f}% positive\\n\")\n",
        "\n",
        "    fold_metrics = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "        print(f\"{'='*90}\")\n",
        "        print(f\"Fold {fold}/5\")\n",
        "        print(f\"{'='*90}\")\n",
        "\n",
        "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        print(f\"  Original fold: {len(X_train_fold)} train, {len(X_val_fold)} val\")\n",
        "\n",
        "        # Split for train/calibration (80/20)\n",
        "        X_tr, X_cal, y_tr, y_cal = train_test_split(\n",
        "            X_train_fold, y_train_fold,\n",
        "            test_size=0.20,\n",
        "            random_state=random_state,\n",
        "            stratify=y_train_fold\n",
        "        )\n",
        "\n",
        "        # Further split for early stopping\n",
        "        X_tr_base, X_es, y_tr_base, y_es = train_test_split(\n",
        "            X_tr, y_tr,\n",
        "            test_size=0.15,\n",
        "            random_state=random_state,\n",
        "            stratify=y_tr\n",
        "        )\n",
        "\n",
        "        print(f\"  Before SMOTE: {len(X_tr_base)} samples ({(y_tr_base==1).sum()} pos, {(y_tr_base==0).sum()} neg)\")\n",
        "\n",
        "        # Apply SMOTE\n",
        "        X_tr_resampled, y_tr_resampled = smote.fit_resample(X_tr_base, y_tr_base)\n",
        "        print(f\"  After SMOTE:  {len(X_tr_resampled)} samples ({(y_tr_resampled==1).sum()} pos, {(y_tr_resampled==0).sum()} neg)\")\n",
        "        print(f\"  Ratio increase: {len(X_tr_resampled)/len(X_tr_base):.2f}x\")\n",
        "\n",
        "        # Train on SMOTE data, validate on original\n",
        "        base_model = lgb.LGBMClassifier(**params)\n",
        "        base_model.fit(\n",
        "            X_tr_resampled, y_tr_resampled,\n",
        "            eval_set=[(X_es, y_es)],\n",
        "            eval_metric=\"auc\",\n",
        "            callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(period=0)]\n",
        "        )\n",
        "\n",
        "        best_iter = base_model.best_iteration_ if hasattr(base_model, 'best_iteration_') else params.get('n_estimators', 1000)\n",
        "        print(f\"  Base model trained (best iteration: {best_iter})\")\n",
        "\n",
        "        # Calibrate on original calibration set\n",
        "        calibrated_model = CalibratedClassifierCV(base_model, method='sigmoid', cv='prefit')\n",
        "        calibrated_model.fit(X_cal, y_cal)\n",
        "        print(f\"  Model calibrated on {len(X_cal)} original samples\")\n",
        "\n",
        "        # Get OOF predictions on original validation data\n",
        "        oof_preds[val_idx] = calibrated_model.predict_proba(X_val_fold)[:, 1]\n",
        "\n",
        "        # --- NEW: Calculate F1 score for the fold ---\n",
        "        prec, rec, thr = precision_recall_curve(y_val_fold, oof_preds[val_idx])\n",
        "        f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
        "        best_idx_pr = int(np.nanargmax(f1s[:-1]))\n",
        "        fold_thresh = float(thr[max(0, best_idx_pr)])\n",
        "        fold_f1 = float(f1s[best_idx_pr])\n",
        "        # --- End NEW ---\n",
        "\n",
        "        # Fold performance\n",
        "        fold_auc = roc_auc_score(y_val_fold, oof_preds[val_idx])\n",
        "        fold_ap = average_precision_score(y_val_fold, oof_preds[val_idx])\n",
        "\n",
        "        # MODIFIED: Add F1 metrics\n",
        "        fold_metrics.append({\n",
        "            'fold': fold,\n",
        "            'auc': fold_auc,\n",
        "            'ap': fold_ap,\n",
        "            'f1': fold_f1,\n",
        "            'threshold': fold_thresh\n",
        "        })\n",
        "\n",
        "        # MODIFIED: Update print statement\n",
        "        print(f\"  Fold ROC-AUC: {fold_auc:.4f} | PR-AUC: {fold_ap:.4f} | Best F1: {fold_f1:.4f} (at Thresh={fold_thresh:.4f})\")\n",
        "        print(f\"  ✓ Fold {fold} complete\\n\")\n",
        "\n",
        "    print(\"=\"*90)\n",
        "    print(\"✓ CALIBRATED OOF PREDICTIONS (SMOTE) COMPLETE\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    # Overall metrics\n",
        "    oof_auc = roc_auc_score(y, oof_preds)\n",
        "    oof_ap = average_precision_score(y, oof_preds)\n",
        "\n",
        "    print(f\"\\nOverall OOF Performance (Threshold-Free):\")\n",
        "    print(f\"  ROC-AUC: {oof_auc:.4f}\")\n",
        "    print(f\"  PR-AUC:  {oof_ap:.4f}\")\n",
        "\n",
        "    # MODIFIED: Update per-fold breakdown table\n",
        "    print(f\"\\nPer-Fold Breakdown:\")\n",
        "    print(f\"{'Fold':<8} {'ROC-AUC':<10} {'PR-AUC':<10} {'Best F1':<10} {'Threshold':<10}\")\n",
        "    print(\"-\"*50)\n",
        "    for m in fold_metrics:\n",
        "        print(f\"{m['fold']:<8} {m['auc']:<10.4f} {m['ap']:<10.4f} {m['f1']:<10.4f} {m['threshold']:<10.4f}\")\n",
        "\n",
        "    # --- NEW: Global OOF F1 Optimization ---\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"GLOBAL OOF THRESHOLD OPTIMIZATION (on Calibrated SMOTE Probs)\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    prec_g, rec_g, thr_g = precision_recall_curve(y, oof_preds)\n",
        "    f1s_g = 2 * (prec_g * rec_g) / (prec_g * rec_g + 1e-12)\n",
        "\n",
        "    best_idx_g = int(np.nanargmax(f1s_g[:-1]))\n",
        "    global_thresh = float(thr_g[max(0, best_idx_g)])\n",
        "    global_f1 = float(f1s_g[best_idx_g])\n",
        "\n",
        "    global_preds = (oof_preds >= global_thresh).astype(int)\n",
        "    global_prec = precision_score(y, global_preds)\n",
        "    global_rec = recall_score(y, global_preds)\n",
        "\n",
        "    print(f\"🎯 Best Global Threshold: {global_thresh:.4f}\")\n",
        "    print(f\"   - OOF F1 Score:  {global_f1:.4f}\")\n",
        "    print(f\"   - OOF Precision: {global_prec:.4f}\")\n",
        "    print(f\"   - OOF Recall:    {global_rec:.4f}\")\n",
        "    # --- End NEW ---\n",
        "\n",
        "    print(f\"\\nOOF Probability Statistics:\")\n",
        "    print(f\"  Mean:   {oof_preds.mean():.4f}\")\n",
        "    print(f\"  Std:    {oof_preds.std():.4f}\")\n",
        "    print(f\"  Median: {np.median(oof_preds):.4f}\")\n",
        "    print(f\"  25th percentile: {np.percentile(oof_preds, 25):.4f}\")\n",
        "    print(f\"  75th percentile: {np.percentile(oof_preds, 75):.4f}\")\n",
        "    print(f\"  Min:    {oof_preds.min():.4f}\")\n",
        "    print(f\"  Max:    {oof_preds.max():.4f}\")\n",
        "\n",
        "    return oof_preds, global_thresh # NEW: Return threshold as well\n",
        "\n",
        "# Generate OOF and get the final threshold\n",
        "best_lgbm_params_smote['n_estimators'] = 2000\n",
        "oof_lgbm_calibrated_smote, final_lgbm_threshold = get_calibrated_oof_preds_lgbm_smote(\n",
        "    best_lgbm_params_smote,\n",
        "    X_all,\n",
        "    y_all,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# NEW: Print a final confirmation of the threshold to be used\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(f\"✅ FINAL THRESHOLD SELECTED: {final_lgbm_threshold:.4f}\")\n",
        "print(\"=\"*90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYeeoWatXOPo",
        "outputId": "8525b338-8a47-4adf-b9ae-093664b8fcf4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "STEP 2: GENERATE CALIBRATED OOF PREDICTIONS WITH SMOTE\n",
            "==========================================================================================\n",
            "\n",
            "Generating calibrated OOF predictions with SMOTE...\n",
            "Total samples: 17999\n",
            "Original class balance: 22.86% positive\n",
            "\n",
            "==========================================================================================\n",
            "Fold 1/5\n",
            "==========================================================================================\n",
            "  Original fold: 14399 train, 3600 val\n",
            "  Before SMOTE: 9791 samples (2239 pos, 7552 neg)\n",
            "  After SMOTE:  15104 samples (7552 pos, 7552 neg)\n",
            "  Ratio increase: 1.54x\n",
            "  Base model trained (best iteration: 241)\n",
            "  Model calibrated on 2880 original samples\n",
            "  Fold ROC-AUC: 0.8309 | PR-AUC: 0.5808 | Best F1: 0.5828 (at Thresh=0.2152)\n",
            "  ✓ Fold 1 complete\n",
            "\n",
            "==========================================================================================\n",
            "Fold 2/5\n",
            "==========================================================================================\n",
            "  Original fold: 14399 train, 3600 val\n",
            "  Before SMOTE: 9791 samples (2239 pos, 7552 neg)\n",
            "  After SMOTE:  15104 samples (7552 pos, 7552 neg)\n",
            "  Ratio increase: 1.54x\n",
            "  Base model trained (best iteration: 244)\n",
            "  Model calibrated on 2880 original samples\n",
            "  Fold ROC-AUC: 0.8418 | PR-AUC: 0.6133 | Best F1: 0.5980 (at Thresh=0.2296)\n",
            "  ✓ Fold 2 complete\n",
            "\n",
            "==========================================================================================\n",
            "Fold 3/5\n",
            "==========================================================================================\n",
            "  Original fold: 14399 train, 3600 val\n",
            "  Before SMOTE: 9791 samples (2239 pos, 7552 neg)\n",
            "  After SMOTE:  15104 samples (7552 pos, 7552 neg)\n",
            "  Ratio increase: 1.54x\n",
            "  Base model trained (best iteration: 250)\n",
            "  Model calibrated on 2880 original samples\n",
            "  Fold ROC-AUC: 0.8511 | PR-AUC: 0.6049 | Best F1: 0.6090 (at Thresh=0.2166)\n",
            "  ✓ Fold 3 complete\n",
            "\n",
            "==========================================================================================\n",
            "Fold 4/5\n",
            "==========================================================================================\n",
            "  Original fold: 14399 train, 3600 val\n",
            "  Before SMOTE: 9791 samples (2239 pos, 7552 neg)\n",
            "  After SMOTE:  15104 samples (7552 pos, 7552 neg)\n",
            "  Ratio increase: 1.54x\n",
            "  Base model trained (best iteration: 202)\n",
            "  Model calibrated on 2880 original samples\n",
            "  Fold ROC-AUC: 0.8360 | PR-AUC: 0.6103 | Best F1: 0.5921 (at Thresh=0.2580)\n",
            "  ✓ Fold 4 complete\n",
            "\n",
            "==========================================================================================\n",
            "Fold 5/5\n",
            "==========================================================================================\n",
            "  Original fold: 14400 train, 3599 val\n",
            "  Before SMOTE: 9792 samples (2239 pos, 7553 neg)\n",
            "  After SMOTE:  15106 samples (7553 pos, 7553 neg)\n",
            "  Ratio increase: 1.54x\n",
            "  Base model trained (best iteration: 222)\n",
            "  Model calibrated on 2880 original samples\n",
            "  Fold ROC-AUC: 0.8412 | PR-AUC: 0.5889 | Best F1: 0.5944 (at Thresh=0.2397)\n",
            "  ✓ Fold 5 complete\n",
            "\n",
            "==========================================================================================\n",
            "✓ CALIBRATED OOF PREDICTIONS (SMOTE) COMPLETE\n",
            "==========================================================================================\n",
            "\n",
            "Overall OOF Performance (Threshold-Free):\n",
            "  ROC-AUC: 0.8401\n",
            "  PR-AUC:  0.5978\n",
            "\n",
            "Per-Fold Breakdown:\n",
            "Fold     ROC-AUC    PR-AUC     Best F1    Threshold \n",
            "--------------------------------------------------\n",
            "1        0.8309     0.5808     0.5828     0.2152    \n",
            "2        0.8418     0.6133     0.5980     0.2296    \n",
            "3        0.8511     0.6049     0.6090     0.2166    \n",
            "4        0.8360     0.6103     0.5921     0.2580    \n",
            "5        0.8412     0.5889     0.5944     0.2397    \n",
            "\n",
            "==========================================================================================\n",
            "GLOBAL OOF THRESHOLD OPTIMIZATION (on Calibrated SMOTE Probs)\n",
            "==========================================================================================\n",
            "🎯 Best Global Threshold: 0.1726\n",
            "   - OOF F1 Score:  2.0000\n",
            "   - OOF Precision: 0.4623\n",
            "   - OOF Recall:    0.7966\n",
            "\n",
            "OOF Probability Statistics:\n",
            "  Mean:   0.2297\n",
            "  Std:    0.2224\n",
            "  Median: 0.1174\n",
            "  25th percentile: 0.0680\n",
            "  75th percentile: 0.3260\n",
            "  Min:    0.0489\n",
            "  Max:    0.9127\n",
            "\n",
            "==========================================================================================\n",
            "✅ FINAL THRESHOLD SELECTED: 0.1726\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# threshold selection"
      ],
      "metadata": {
        "id": "sGSWEnbHj1iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"STEP 3: OPTIMIZE THRESHOLD ON CALIBRATED OOF (SMOTE)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(\"\\nSearching for optimal threshold...\")\n",
        "\n",
        "# Method 1: PR Curve\n",
        "prec, rec, thr = precision_recall_curve(y_all, oof_lgbm_calibrated_smote)\n",
        "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
        "best_idx_pr = int(np.nanargmax(f1s[:-1]))\n",
        "threshold_pr = float(thr[max(0, best_idx_pr)])\n",
        "f1_pr = float(f1s[best_idx_pr])\n",
        "\n",
        "# Method 2: Grid Search\n",
        "thresholds_grid = np.arange(0.15, 0.55, 0.01)\n",
        "f1_scores_grid = []\n",
        "for t in thresholds_grid:\n",
        "    preds = (oof_lgbm_calibrated_smote >= t).astype(int)\n",
        "    f1_scores_grid.append(f1_score(y_all, preds))\n",
        "\n",
        "best_idx_grid = int(np.argmax(f1_scores_grid))\n",
        "threshold_grid = float(thresholds_grid[best_idx_grid])\n",
        "f1_grid = float(f1_scores_grid[best_idx_grid])\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"THRESHOLD COMPARISON (SMOTE Calibrated OOF)\")\n",
        "print(\"=\"*90)\n",
        "print(f\"{'Method':<20} {'Threshold':<12} {'F1 Score':<12} {'Precision':<12} {'Recall':<10}\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "methods = [\n",
        "    (\"PR Curve\", threshold_pr, f1_pr),\n",
        "    (\"Grid Search\", threshold_grid, f1_grid),\n",
        "]\n",
        "\n",
        "best_f1_overall = 0\n",
        "for method_name, thresh, f1 in methods:\n",
        "    preds = (oof_lgbm_calibrated_smote >= thresh).astype(int)\n",
        "    prec = precision_score(y_all, preds, zero_division=0)\n",
        "    rec = recall_score(y_all, preds, zero_division=0)\n",
        "\n",
        "    marker = \"\"\n",
        "    if f1 > best_f1_overall:\n",
        "        best_f1_overall = f1\n",
        "        final_lgbm_threshold = thresh\n",
        "        best_method = method_name\n",
        "        final_prec = prec\n",
        "        final_rec = rec\n",
        "        marker = \" ← BEST\"\n",
        "\n",
        "    print(f\"{method_name:<20} {thresh:<12.4f} {f1:<12.4f} {prec:<12.4f} {rec:<10.4f}{marker}\")\n",
        "\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(f\"\\n{'='*90}\")\n",
        "print(\"FINAL THRESHOLD SELECTION (SMOTE)\")\n",
        "print(f\"{'='*90}\")\n",
        "print(f\"✅ Selected Method: {best_method}\")\n",
        "print(f\"✅ Optimal Threshold: {final_lgbm_threshold:.4f}\")\n",
        "print(f\"✅ Expected OOF F1: {best_f1_overall:.4f}\")\n",
        "print(f\"\\nPerformance Breakdown:\")\n",
        "print(f\"  Precision: {final_prec:.4f}\")\n",
        "print(f\"  Recall:    {final_rec:.4f}\")\n",
        "print(f\"  F1 Score:  {best_f1_overall:.4f}\")\n",
        "\n",
        "preds_final = (oof_lgbm_calibrated_smote >= final_lgbm_threshold).astype(int)\n",
        "tn, fp, fn, tp = confusion_matrix(y_all, preds_final).ravel()\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"  TN: {tn:5d} | FP: {fp:5d}\")\n",
        "print(f\"  FN: {fn:5d} | TP: {tp:5d}\")\n",
        "print(f\"{'='*90}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLCiC7wtj3bP",
        "outputId": "23f5619b-850a-4520-ad88-4ab29b24ff36"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "STEP 3: OPTIMIZE THRESHOLD ON CALIBRATED OOF (SMOTE)\n",
            "==========================================================================================\n",
            "\n",
            "Searching for optimal threshold...\n",
            "\n",
            "==========================================================================================\n",
            "THRESHOLD COMPARISON (SMOTE Calibrated OOF)\n",
            "==========================================================================================\n",
            "Method               Threshold    F1 Score     Precision    Recall    \n",
            "==========================================================================================\n",
            "PR Curve             0.2349       0.5909       0.5055       0.7111     ← BEST\n",
            "Grid Search          0.2200       0.5908       0.4956       0.7312    \n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            "FINAL THRESHOLD SELECTION (SMOTE)\n",
            "==========================================================================================\n",
            "✅ Selected Method: PR Curve\n",
            "✅ Optimal Threshold: 0.2349\n",
            "✅ Expected OOF F1: 0.5909\n",
            "\n",
            "Performance Breakdown:\n",
            "  Precision: 0.5055\n",
            "  Recall:    0.7111\n",
            "  F1 Score:  0.5909\n",
            "\n",
            "Confusion Matrix:\n",
            "  TN: 11022 | FP:  2862\n",
            "  FN:  1189 | TP:  2926\n",
            "==========================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train final model + prediction"
      ],
      "metadata": {
        "id": "HXFN2Slp-oiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*90)\n",
        "print(\"STEP 4: TRAIN FINAL CALIBRATED MODEL WITH SMOTE & PREDICT\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Split: 85% train, 15% calibration\n",
        "X_train_final, X_cal_final, y_train_final, y_cal_final = train_test_split(\n",
        "    X_all, y_all,\n",
        "    test_size=0.15,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_all\n",
        ")\n",
        "\n",
        "print(f\"\\nFinal data split:\")\n",
        "print(f\"  Training: {X_train_final.shape[0]} samples\")\n",
        "print(f\"  Calibration: {X_cal_final.shape[0]} samples\")\n",
        "\n",
        "# Apply SMOTE to training data\n",
        "print(f\"\\nApplying SMOTE to final training data...\")\n",
        "print(f\"  Before: {len(y_train_final)} samples ({(y_train_final==1).sum()} pos, {(y_train_final==0).sum()} neg)\")\n",
        "\n",
        "smote_final = SMOTE(random_state=RANDOM_STATE, sampling_strategy=1.0)\n",
        "X_tr_resampled_final, y_tr_resampled_final = smote_final.fit_resample(X_train_final, y_train_final)\n",
        "\n",
        "print(f\"  After:  {len(y_tr_resampled_final)} samples ({(y_tr_resampled_final==1).sum()} pos, {(y_tr_resampled_final==0).sum()} neg)\")\n",
        "print(f\"  Ratio increase: {len(y_tr_resampled_final)/len(y_train_final):.2f}x\")\n",
        "\n",
        "# Train on SMOTE data\n",
        "print(f\"\\nTraining final LightGBM on SMOTE data...\")\n",
        "final_lgbm_params = best_lgbm_params_smote.copy()\n",
        "final_lgbm_params['n_estimators'] = study_smote.best_params.get('n_estimators', 1000)\n",
        "\n",
        "final_lgbm_base = lgb.LGBMClassifier(**final_lgbm_params)\n",
        "final_lgbm_base.fit(X_tr_resampled_final, y_tr_resampled_final)\n",
        "print(f\"✓ Base model trained\")\n",
        "\n",
        "# Calibrate on original calibration data\n",
        "print(f\"Calibrating on original (non-SMOTE) calibration data...\")\n",
        "final_lgbm = CalibratedClassifierCV(final_lgbm_base, method='sigmoid', cv='prefit')\n",
        "final_lgbm.fit(X_cal_final, y_cal_final)\n",
        "print(f\"✓ Model calibrated\")\n",
        "\n",
        "# Generate test predictions\n",
        "print(f\"\\n{'='*90}\")\n",
        "print(f\"GENERATING TEST PREDICTIONS\")\n",
        "print(f\"{'='*90}\")\n",
        "\n",
        "print(f\"\\nGenerating calibrated probabilities for {X_test_all.shape[0]} test samples...\")\n",
        "test_probabilities = final_lgbm.predict_proba(X_test_all)[:, 1]\n",
        "\n",
        "print(f\"\\nTest Probability Statistics:\")\n",
        "print(f\"  Mean:   {test_probabilities.mean():.4f}\")\n",
        "print(f\"  Std:    {test_probabilities.std():.4f}\")\n",
        "print(f\"  Median: {np.median(test_probabilities):.4f}\")\n",
        "print(f\"  Min:    {test_probabilities.min():.4f}\")\n",
        "print(f\"  Max:    {test_probabilities.max():.4f}\")\n",
        "\n",
        "# Apply threshold\n",
        "test_predictions = (test_probabilities >= final_lgbm_threshold).astype(int)\n",
        "\n",
        "print(f\"\\n✓ Applied threshold: {final_lgbm_threshold:.4f}\")\n",
        "print(f\"\\nFinal Prediction Distribution:\")\n",
        "print(f\"  Class 0 (No subrogation): {(test_predictions == 0).sum():5d} ({(test_predictions == 0).sum() / len(test_predictions) * 100:5.1f}%)\")\n",
        "print(f\"  Class 1 (Subrogation):    {(test_predictions == 1).sum():5d} ({(test_predictions == 1).sum() / len(test_predictions) * 100:5.1f}%)\")\n",
        "\n",
        "# Create submission\n",
        "submission = pd.DataFrame({\n",
        "    'claim_number': test_fe['claim_number'],\n",
        "    'subrogation': test_predictions\n",
        "})\n",
        "\n",
        "output_filename = f'TriGuard_SMOTE_CalOOF_thresh_{int(final_lgbm_threshold*10000)}.csv'\n",
        "submission.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\n{'='*90}\")\n",
        "print(f\"✓ SUBMISSION FILE SAVED: {output_filename}\")\n",
        "print(f\"{'='*90}\")\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_filename)\n",
        "    print(\"✓ File downloaded!\")\n",
        "except ImportError:\n",
        "    print(\"(Not in Colab - file saved locally)\")\n",
        "\n",
        "print(f\"\\n{'='*90}\")\n",
        "print(\"SMOTE PIPELINE COMPLETE!\")\n",
        "print(f\"{'='*90}\")\n",
        "print(f\"\\n🎯 Expected Performance: F1 ≈ {best_f1_overall:.4f} (based on SMOTE calibrated OOF)\")\n",
        "print(f\"📊 Method: {best_method}\")\n",
        "print(f\"🔧 Threshold: {final_lgbm_threshold:.4f}\")\n",
        "print(f\"\\n⚠️  Remember to COMPARE with your non-SMOTE baseline!\")\n",
        "print(f\"   Non-SMOTE baseline F1: ~0.59\")\n",
        "print(f\"   SMOTE expected F1: {best_f1_overall:.4f}\")\n",
        "print(f\"   Improvement: {best_f1_overall - 0.59:+.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "id": "cFljKjaf-tU0",
        "outputId": "0d411431-30b6-4026-ea5b-c99059910673"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "STEP 4: TRAIN FINAL CALIBRATED MODEL WITH SMOTE & PREDICT\n",
            "==========================================================================================\n",
            "\n",
            "Final data split:\n",
            "  Training: 15299 samples\n",
            "  Calibration: 2700 samples\n",
            "\n",
            "Applying SMOTE to final training data...\n",
            "  Before: 15299 samples (3498 pos, 11801 neg)\n",
            "  After:  23602 samples (11801 pos, 11801 neg)\n",
            "  Ratio increase: 1.54x\n",
            "\n",
            "Training final LightGBM on SMOTE data...\n",
            "✓ Base model trained\n",
            "Calibrating on original (non-SMOTE) calibration data...\n",
            "✓ Model calibrated\n",
            "\n",
            "==========================================================================================\n",
            "GENERATING TEST PREDICTIONS\n",
            "==========================================================================================\n",
            "\n",
            "Generating calibrated probabilities for 12000 test samples...\n",
            "\n",
            "Test Probability Statistics:\n",
            "  Mean:   0.2290\n",
            "  Std:    0.2094\n",
            "  Median: 0.1237\n",
            "  Min:    0.0734\n",
            "  Max:    0.8834\n",
            "\n",
            "✓ Applied threshold: 0.2349\n",
            "\n",
            "Final Prediction Distribution:\n",
            "  Class 0 (No subrogation):  8304 ( 69.2%)\n",
            "  Class 1 (Subrogation):     3696 ( 30.8%)\n",
            "\n",
            "==========================================================================================\n",
            "✓ SUBMISSION FILE SAVED: TriGuard_SMOTE_CalOOF_thresh_2348.csv\n",
            "==========================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e0916be1-5835-4a64-bc8a-b299911fe837\", \"TriGuard_SMOTE_CalOOF_thresh_2348.csv\", 120025)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ File downloaded!\n",
            "\n",
            "==========================================================================================\n",
            "SMOTE PIPELINE COMPLETE!\n",
            "==========================================================================================\n",
            "\n",
            "🎯 Expected Performance: F1 ≈ 0.5909 (based on SMOTE calibrated OOF)\n",
            "📊 Method: PR Curve\n",
            "🔧 Threshold: 0.2349\n",
            "\n",
            "⚠️  Remember to COMPARE with your non-SMOTE baseline!\n",
            "   Non-SMOTE baseline F1: ~0.59\n",
            "   SMOTE expected F1: 0.5909\n",
            "   Improvement: +0.0009\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============\n"
      ],
      "metadata": {
        "id": "pA6jfotqn1P2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "PHw2oHzPn3ez"
      }
    }
  ]
}
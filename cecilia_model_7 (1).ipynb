{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "091ba666a7284fb7a1eca1b5124edc4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_635e3c475bfd43259c48e8b5a3353388",
              "IPY_MODEL_efed44b81b19454691cf6608f797842d",
              "IPY_MODEL_e3dd593530a84874844393fb0e962976"
            ],
            "layout": "IPY_MODEL_223cb1959bb840a3b8a1870e7c716003"
          }
        },
        "635e3c475bfd43259c48e8b5a3353388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9579b7a658cb48318f51f81968eb8de5",
            "placeholder": "​",
            "style": "IPY_MODEL_aee3b1ed33314bd0b619ed5e113e94b3",
            "value": "Best trial: 51. Best value: 0.600408:  38%"
          }
        },
        "efed44b81b19454691cf6608f797842d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9909546a6e41433f85cfb9790edaa421",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6edcbf2277b14faba22fb9c5ef1a78fc",
            "value": 19
          }
        },
        "e3dd593530a84874844393fb0e962976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c6eade5614c44899da65aabc5e22c32",
            "placeholder": "​",
            "style": "IPY_MODEL_ff77477f279f4c12917c0794d5351eec",
            "value": " 19/50 [01:58&lt;02:48,  5.44s/it]"
          }
        },
        "223cb1959bb840a3b8a1870e7c716003": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9579b7a658cb48318f51f81968eb8de5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aee3b1ed33314bd0b619ed5e113e94b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9909546a6e41433f85cfb9790edaa421": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6edcbf2277b14faba22fb9c5ef1a78fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c6eade5614c44899da65aabc5e22c32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff77477f279f4c12917c0794d5351eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# setup and configuration"
      ],
      "metadata": {
        "id": "saaCfeUqppjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# --- Setup ---\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 200)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "# --- Random Seed ---\n",
        "RANDOM_STATE = 123\n",
        "np.random.seed(RANDOM_STATE)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_STATE)\n",
        "\n",
        "# --- Model/Metrics ---\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, precision_score, recall_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# --- ML Models ---\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "\n",
        "# --- Tuning ---\n",
        "try:\n",
        "    import optuna\n",
        "except ImportError:\n",
        "    print(\"Installing Optuna...\")\n",
        "    !pip install optuna -q\n",
        "    import optuna\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# --- Feature Engineering Helpers ---\n",
        "# REMOVED pyzipcode - we won't use 'state' feature due to data quality issues\n",
        "# try:\n",
        "#     from pyzipcode import ZipCodeDatabase\n",
        "# except ImportError:\n",
        "#     print(\"Installing pyzipcode...\")\n",
        "#     !pip install pyzipcode -q\n",
        "#     from pyzipcode import ZipCodeDatabase\n",
        "\n",
        "print(\"\\n✓ Setup complete. All libraries loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeUGEDgNpt_a",
        "outputId": "ee2e3016-c301-4d8c-888a-b75cfd9feb9a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Setup complete. All libraries loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data loading\n"
      ],
      "metadata": {
        "id": "KC0NdoqFqvBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Data ---\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    try:\n",
        "        train_df = pd.read_csv('Training_TriGuard.csv')\n",
        "        test_df = pd.read_csv('Testing_TriGuard.csv')\n",
        "        print(\"✓ Files loaded from local environment.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Please upload Training_TriGuard.csv:\")\n",
        "        uploaded_train = files.upload()\n",
        "        train_file = list(uploaded_train.keys())[0]\n",
        "        train_df = pd.read_csv(train_file)\n",
        "\n",
        "        print(\"\\nPlease upload Testing_TriGuard.csv:\")\n",
        "        uploaded_test = files.upload()\n",
        "        test_file = list(uploaded_test.keys())[0]\n",
        "        test_df = pd.read_csv(test_file)\n",
        "        print(\"✓ Files uploaded successfully.\")\n",
        "else:\n",
        "    # Local environment\n",
        "    possible_paths = [\n",
        "        'Training_TriGuard.csv',\n",
        "        'data/Training_TriGuard.csv',\n",
        "        '../Training_TriGuard.csv',\n",
        "        './Training_TriGuard.csv'\n",
        "    ]\n",
        "\n",
        "    train_path = None\n",
        "    test_path = None\n",
        "\n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            train_path = path\n",
        "            test_path = path.replace('Training', 'Testing')\n",
        "            if os.path.exists(test_path):\n",
        "                break\n",
        "\n",
        "    if train_path and test_path:\n",
        "        train_df = pd.read_csv(train_path)\n",
        "        test_df = pd.read_csv(test_path)\n",
        "        print(f\"✓ Files loaded from: {train_path} and {test_path}\")\n",
        "    else:\n",
        "        train_path = input(\"Enter path to Training_TriGuard.csv: \").strip()\n",
        "        test_path = input(\"Enter path to Testing_TriGuard.csv: \").strip()\n",
        "        train_df = pd.read_csv(train_path)\n",
        "        test_df = pd.read_csv(test_path)\n",
        "        print(\"✓ Files loaded successfully.\")\n",
        "\n",
        "# --- Critical Cleaning ---\n",
        "initial_train_count = len(train_df)\n",
        "train_df = train_df.dropna(subset=['subrogation'])\n",
        "print(f\"\\nCleaned training data: Removed {initial_train_count - len(train_df)} rows with NaN target.\")\n",
        "\n",
        "train_df['subrogation'] = train_df['subrogation'].astype(int)\n",
        "\n",
        "print(f\"\\n✓ Train shape: {train_df.shape}\")\n",
        "print(f\"✓ Test shape: {test_df.shape}\")\n",
        "print(f\"\\nTarget distribution (after cleaning):\")\n",
        "print(train_df['subrogation'].value_counts(normalize=True).to_string())\n",
        "\n",
        "test_ids = test_df['claim_number'].copy()\n",
        "\n",
        "print(\"✓ Data loading complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87vQef5vq4sW",
        "outputId": "f7bf238a-c1e7-447d-f9ef-ccf213062162"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Files loaded from local environment.\n",
            "\n",
            "Cleaned training data: Removed 2 rows with NaN target.\n",
            "\n",
            "✓ Train shape: (17999, 29)\n",
            "✓ Test shape: (12000, 28)\n",
            "\n",
            "Target distribution (after cleaning):\n",
            "subrogation\n",
            "0   0.771\n",
            "1   0.229\n",
            "✓ Data loading complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature engineering\n"
      ],
      "metadata": {
        "id": "sA9j7FNKrEal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineer(df):\n",
        "    \"\"\"Feature engineering WITHOUT vehicle_made_year/vehicle_age/state (data quality issues)\"\"\"\n",
        "    df_fe = df.copy()\n",
        "\n",
        "    # ========================================================================\n",
        "    # TEMPORAL FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['claim_date'] = pd.to_datetime(df_fe['claim_date'], errors='coerce')\n",
        "    df_fe['claim_year'] = df_fe['claim_date'].dt.year\n",
        "    df_fe['claim_month'] = df_fe['claim_date'].dt.month\n",
        "    df_fe['claim_day'] = df_fe['claim_date'].dt.day\n",
        "    df_fe['claim_quarter'] = df_fe['claim_date'].dt.quarter\n",
        "    df_fe['claim_dayofweek'] = df_fe['claim_date'].dt.dayofweek\n",
        "    df_fe['is_weekend'] = (df_fe['claim_dayofweek'] >= 5).astype(int)\n",
        "    df_fe['is_monday'] = (df_fe['claim_dayofweek'] == 0).astype(int)\n",
        "    df_fe['is_friday'] = (df_fe['claim_dayofweek'] == 4).astype(int)\n",
        "    df_fe['is_q4'] = (df_fe['claim_quarter'] == 4).astype(int)\n",
        "\n",
        "    # NEW: Time-of-day features from Doc 8\n",
        "    df_fe['claim_hour'] = df_fe['claim_date'].dt.hour\n",
        "    df_fe['rush_hour'] = df_fe['claim_hour'].isin([7, 8, 9, 16, 17, 18]).astype(int)\n",
        "    df_fe['late_night'] = df_fe['claim_hour'].isin([0, 1, 2, 3, 4, 5]).astype(int)\n",
        "\n",
        "    season_map = {\n",
        "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
        "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
        "        9: 'Fall', 10: 'Fall', 11: 'Fall',\n",
        "        12: 'Winter', 1: 'Winter', 2: 'Winter'\n",
        "    }\n",
        "    df_fe['season'] = df_fe['claim_month'].map(season_map).fillna('Unknown')\n",
        "\n",
        "    # ========================================================================\n",
        "    # DATA CLEANING\n",
        "    # ========================================================================\n",
        "    df_fe.loc[(df_fe['year_of_born'] < 1900) | (df_fe['year_of_born'] > 2025), 'year_of_born'] = np.nan\n",
        "\n",
        "    # ========================================================================\n",
        "    # BINARY CONVERSIONS (for interactions)\n",
        "    # ========================================================================\n",
        "    df_fe['witness_binary'] = (df_fe['witness_present_ind'] == 'Y').astype(int)\n",
        "    df_fe['police_binary'] = df_fe['policy_report_filed_ind']\n",
        "    df_fe['multicar_binary'] = df_fe['accident_type'].isin(['multi_vehicle_clear', 'multi_vehicle_unclear']).astype(int)\n",
        "    df_fe['highrisk_site_binary'] = df_fe['accident_site'].isin(['Highway/Intersection', 'Local']).astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # CRITICAL INTERACTION FEATURES (2-way)\n",
        "    # ========================================================================\n",
        "    df_fe['liab_x_witness'] = df_fe['liab_prct'] * df_fe['witness_binary']\n",
        "    df_fe['liab_x_police'] = df_fe['liab_prct'] * df_fe['police_binary']\n",
        "    df_fe['liab_x_multicar'] = df_fe['liab_prct'] * df_fe['multicar_binary']\n",
        "    df_fe['liab_x_highrisk_site'] = df_fe['liab_prct'] * df_fe['highrisk_site_binary']\n",
        "    df_fe['liab_x_evidence'] = df_fe['liab_prct'] * (df_fe['witness_binary'] + df_fe['police_binary'])\n",
        "    df_fe['liab_x_payout'] = df_fe['liab_prct'] * df_fe['claim_est_payout']\n",
        "    df_fe['liab_x_mileage'] = df_fe['liab_prct'] * df_fe['vehicle_mileage']\n",
        "\n",
        "    df_fe['witness_x_police'] = df_fe['witness_binary'] * df_fe['police_binary']\n",
        "    df_fe['witness_x_multicar'] = df_fe['witness_binary'] * df_fe['multicar_binary']\n",
        "    df_fe['police_x_multicar'] = df_fe['police_binary'] * df_fe['multicar_binary']\n",
        "    df_fe['multicar_x_highrisk'] = df_fe['multicar_binary'] * df_fe['highrisk_site_binary']\n",
        "    df_fe['weekend_highway'] = (df_fe['claim_dayofweek'] >= 5).astype(int) * (df_fe['accident_site'] == 'Highway/Intersection').astype(int)\n",
        "\n",
        "    # 3-way interaction\n",
        "    df_fe['witness_police_multicar'] = df_fe['witness_binary'] * df_fe['police_binary'] * df_fe['multicar_binary']\n",
        "\n",
        "    # ========================================================================\n",
        "    # POLYNOMIAL FEATURES (liability & key variables)\n",
        "    # ========================================================================\n",
        "    df_fe['liab_prct_squared'] = df_fe['liab_prct'] ** 2\n",
        "    df_fe['liab_prct_cubed'] = df_fe['liab_prct'] ** 3\n",
        "    df_fe['liab_prct_sqrt'] = np.sqrt(df_fe['liab_prct'])\n",
        "    df_fe['liab_prct_log'] = np.log1p(df_fe['liab_prct'])\n",
        "    df_fe['liab_inverse'] = 100 - df_fe['liab_prct']\n",
        "    df_fe['liab_inverse_squared'] = (100 - df_fe['liab_prct']) ** 2\n",
        "\n",
        "    df_fe['log_claim_est_payout'] = np.log1p(df_fe['claim_est_payout'])\n",
        "    df_fe['log_vehicle_mileage'] = np.log1p(df_fe['vehicle_mileage'])\n",
        "    df_fe['log_vehicle_price'] = np.log1p(df_fe['vehicle_price'])\n",
        "    df_fe['log_annual_income'] = np.log1p(df_fe['annual_income'])\n",
        "    df_fe['sqrt_vehicle_mileage'] = np.sqrt(df_fe['vehicle_mileage'])\n",
        "\n",
        "    # ========================================================================\n",
        "    # ACCIDENT TYPE FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['is_multi_vehicle_clear'] = (df_fe['accident_type'] == 'multi_vehicle_clear').astype(int)\n",
        "    df_fe['is_multi_vehicle_unclear'] = (df_fe['accident_type'] == 'multi_vehicle_unclear').astype(int)\n",
        "    df_fe['is_single_car'] = (df_fe['accident_type'] == 'single_car').astype(int)\n",
        "    df_fe['has_recovery_target'] = df_fe['multicar_binary']\n",
        "\n",
        "    df_fe['recovery_case_clarity'] = 0\n",
        "    df_fe.loc[df_fe['is_multi_vehicle_clear'] == 1, 'recovery_case_clarity'] = 3\n",
        "    df_fe.loc[df_fe['is_multi_vehicle_unclear'] == 1, 'recovery_case_clarity'] = 1\n",
        "\n",
        "    # ========================================================================\n",
        "    # LIABILITY BUCKETS (fine-grained)\n",
        "    # ========================================================================\n",
        "    df_fe['liab_under_10'] = (df_fe['liab_prct'] < 10).astype(int)\n",
        "    df_fe['liab_10_to_15'] = ((df_fe['liab_prct'] >= 10) & (df_fe['liab_prct'] < 15)).astype(int)\n",
        "    df_fe['liab_15_to_20'] = ((df_fe['liab_prct'] >= 15) & (df_fe['liab_prct'] < 20)).astype(int)\n",
        "    df_fe['liab_20_to_25'] = ((df_fe['liab_prct'] >= 20) & (df_fe['liab_prct'] < 25)).astype(int)\n",
        "    df_fe['liab_25_to_30'] = ((df_fe['liab_prct'] >= 25) & (df_fe['liab_prct'] < 30)).astype(int)\n",
        "    df_fe['liab_30_to_35'] = ((df_fe['liab_prct'] >= 30) & (df_fe['liab_prct'] < 35)).astype(int)\n",
        "    df_fe['liab_35_to_40'] = ((df_fe['liab_prct'] >= 35) & (df_fe['liab_prct'] < 40)).astype(int)\n",
        "    df_fe['liab_40_to_50'] = ((df_fe['liab_prct'] >= 40) & (df_fe['liab_prct'] < 50)).astype(int)\n",
        "    df_fe['liab_over_50'] = (df_fe['liab_prct'] >= 50).astype(int)\n",
        "\n",
        "    df_fe['not_at_fault'] = df_fe['liab_under_10']\n",
        "    df_fe['minimal_fault'] = (df_fe['liab_prct'] < 25).astype(int)\n",
        "    df_fe['low_fault'] = (df_fe['liab_prct'] < 35).astype(int)\n",
        "    df_fe['shared_fault'] = ((df_fe['liab_prct'] >= 35) & (df_fe['liab_prct'] < 50)).astype(int)\n",
        "    df_fe['high_fault'] = (df_fe['liab_prct'] >= 50).astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # EVIDENCE QUALITY FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['witness_present'] = df_fe['witness_binary']\n",
        "    df_fe['police_report'] = df_fe['police_binary']\n",
        "\n",
        "    df_fe['evidence_none'] = ((df_fe['witness_present'] == 0) & (df_fe['police_report'] == 0)).astype(int)\n",
        "    df_fe['evidence_weak'] = (((df_fe['witness_present'] == 1) & (df_fe['police_report'] == 0)) |\n",
        "                              ((df_fe['witness_present'] == 0) & (df_fe['police_report'] == 1))).astype(int)\n",
        "    df_fe['evidence_strong'] = ((df_fe['witness_present'] == 1) & (df_fe['police_report'] == 1)).astype(int)\n",
        "    df_fe['evidence_very_strong'] = ((df_fe['witness_present'] == 1) & (df_fe['police_report'] == 1) &\n",
        "                                      (df_fe['liab_prct'] < 20)).astype(int)\n",
        "    df_fe['evidence_score'] = df_fe['witness_present'] + df_fe['police_report']\n",
        "\n",
        "    # ========================================================================\n",
        "    # ACCIDENT SITE FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['high_risk_site'] = df_fe['highrisk_site_binary']\n",
        "    df_fe['parking_accident'] = (df_fe['accident_site'] == 'Parking Area').astype(int)\n",
        "    df_fe['unknown_site'] = (df_fe['accident_site'] == 'Unknown').astype(int)\n",
        "    df_fe['highway_accident'] = (df_fe['accident_site'] == 'Highway/Intersection').astype(int)\n",
        "    df_fe['local_accident'] = (df_fe['accident_site'] == 'Local').astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # DRIVER AGE & EXPERIENCE\n",
        "    # ========================================================================\n",
        "    df_fe['driver_age'] = df_fe['claim_year'] - df_fe['year_of_born']\n",
        "    df_fe.loc[(df_fe['driver_age'] < 16) | (df_fe['driver_age'] > 100), 'driver_age'] = np.nan\n",
        "\n",
        "    df_fe['young_driver'] = ((df_fe['driver_age'] >= 16) & (df_fe['driver_age'] <= 25)).astype(int)\n",
        "    df_fe['prime_driver'] = ((df_fe['driver_age'] > 25) & (df_fe['driver_age'] <= 45)).astype(int)\n",
        "    df_fe['middle_age_driver'] = ((df_fe['driver_age'] > 45) & (df_fe['driver_age'] <= 65)).astype(int)\n",
        "    df_fe['senior_driver'] = (df_fe['driver_age'] > 65).astype(int)\n",
        "\n",
        "    df_fe['driving_experience'] = (df_fe['driver_age'] - df_fe['age_of_DL']).clip(lower=0)\n",
        "    df_fe.loc[df_fe['driving_experience'] < 0, 'driving_experience'] = np.nan\n",
        "\n",
        "    df_fe['novice_driver'] = (df_fe['driving_experience'] < 3).astype(int)\n",
        "    df_fe['experienced_driver'] = ((df_fe['driving_experience'] >= 3) & (df_fe['driving_experience'] <= 10)).astype(int)\n",
        "    df_fe['veteran_driver'] = (df_fe['driving_experience'] > 10).astype(int)\n",
        "\n",
        "    df_fe['experience_x_safety'] = df_fe['driving_experience'] * df_fe['safety_rating']\n",
        "    df_fe['driver_age_x_safety'] = df_fe['driver_age'] * df_fe['safety_rating']\n",
        "\n",
        "    # NEW: Driver risk interactions from Doc 8\n",
        "    df_fe['young_novice'] = df_fe['young_driver'] * df_fe['novice_driver']\n",
        "\n",
        "    # ========================================================================\n",
        "    # VEHICLE FEATURES (without vehicle_age)\n",
        "    # ========================================================================\n",
        "    df_fe['luxury_vehicle'] = (df_fe['vehicle_price'] > 50000).astype(int)\n",
        "    df_fe['mid_price_vehicle'] = ((df_fe['vehicle_price'] >= 20000) & (df_fe['vehicle_price'] <= 50000)).astype(int)\n",
        "    df_fe['economy_vehicle'] = (df_fe['vehicle_price'] < 20000).astype(int)\n",
        "\n",
        "    df_fe['heavy_vehicle'] = (df_fe['vehicle_weight'] > 30000).astype(int)\n",
        "    df_fe['light_vehicle'] = (df_fe['vehicle_weight'] < 15000).astype(int)\n",
        "    df_fe['medium_weight'] = ((df_fe['vehicle_weight'] >= 15000) & (df_fe['vehicle_weight'] <= 30000)).astype(int)\n",
        "\n",
        "    df_fe['is_large_vehicle'] = (df_fe['vehicle_category'] == 'Large').astype(int)\n",
        "    df_fe['is_compact_vehicle'] = (df_fe['vehicle_category'] == 'Compact').astype(int)\n",
        "    df_fe['is_medium_vehicle'] = (df_fe['vehicle_category'] == 'Medium').astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # CLAIM CHARACTERISTICS\n",
        "    # ========================================================================\n",
        "    df_fe['high_mileage'] = (df_fe['vehicle_mileage'] > 100000).astype(int)\n",
        "    df_fe['low_mileage'] = (df_fe['vehicle_mileage'] < 50000).astype(int)\n",
        "    df_fe['very_high_mileage'] = (df_fe['vehicle_mileage'] > 150000).astype(int)\n",
        "    df_fe['medium_mileage'] = ((df_fe['vehicle_mileage'] >= 50000) & (df_fe['vehicle_mileage'] <= 100000)).astype(int)\n",
        "\n",
        "    df_fe['frequent_claimer'] = (df_fe['past_num_of_claims'] > 5).astype(int)\n",
        "    df_fe['moderate_claimer'] = ((df_fe['past_num_of_claims'] >= 1) & (df_fe['past_num_of_claims'] <= 5)).astype(int)\n",
        "    df_fe['first_time_claimer'] = (df_fe['past_num_of_claims'] == 0).astype(int)\n",
        "    df_fe['very_frequent_claimer'] = (df_fe['past_num_of_claims'] > 10).astype(int)\n",
        "\n",
        "    df_fe['large_payout'] = (df_fe['claim_est_payout'] > 5000).astype(int)\n",
        "    df_fe['medium_payout'] = ((df_fe['claim_est_payout'] >= 2000) & (df_fe['claim_est_payout'] <= 5000)).astype(int)\n",
        "    df_fe['small_payout'] = (df_fe['claim_est_payout'] < 2000).astype(int)\n",
        "    df_fe['very_large_payout'] = (df_fe['claim_est_payout'] > 8000).astype(int)\n",
        "\n",
        "    df_fe['safety_x_prior_claims'] = df_fe['safety_rating'] / (1 + df_fe['past_num_of_claims'])\n",
        "    df_fe['mileage_x_claims'] = df_fe['vehicle_mileage'] * df_fe['past_num_of_claims']\n",
        "\n",
        "    # NEW: Claims risk interactions from Doc 8\n",
        "    df_fe['senior_frequent_claimer'] = df_fe['senior_driver'] * df_fe['frequent_claimer']\n",
        "    df_fe['low_safety_high_claims'] = ((df_fe['safety_rating'] < 60) & (df_fe['past_num_of_claims'] > 3)).astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # RATIO FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['payout_to_price_ratio'] = df_fe['claim_est_payout'] / (df_fe['vehicle_price'] + 1)\n",
        "    df_fe['severe_damage'] = (df_fe['payout_to_price_ratio'] > 0.3).astype(int)\n",
        "    df_fe['moderate_damage'] = ((df_fe['payout_to_price_ratio'] >= 0.1) & (df_fe['payout_to_price_ratio'] <= 0.3)).astype(int)\n",
        "    df_fe['minor_damage'] = (df_fe['payout_to_price_ratio'] < 0.1).astype(int)\n",
        "\n",
        "    df_fe['income_to_vehicle_price'] = df_fe['annual_income'] / (df_fe['vehicle_price'] + 1)\n",
        "    df_fe['can_afford_vehicle'] = (df_fe['income_to_vehicle_price'] >= 0.5).astype(int)\n",
        "    df_fe['expensive_for_income'] = (df_fe['income_to_vehicle_price'] < 0.3).astype(int)\n",
        "\n",
        "    df_fe['claims_per_year_driving'] = df_fe['past_num_of_claims'] / (df_fe['driving_experience'] + 1)\n",
        "    df_fe['claim_frequency_high'] = (df_fe['claims_per_year_driving'] > 0.5).astype(int)\n",
        "\n",
        "    df_fe['safety_to_liability'] = df_fe['safety_rating'] / (df_fe['liab_prct'] + 1)\n",
        "    df_fe['payout_to_income'] = df_fe['claim_est_payout'] / (df_fe['annual_income'] + 1)\n",
        "    df_fe['mileage_to_price'] = df_fe['vehicle_mileage'] / (df_fe['vehicle_price'] + 1)\n",
        "    df_fe['weight_to_price'] = df_fe['vehicle_weight'] / (df_fe['vehicle_price'] + 1)\n",
        "\n",
        "    # ========================================================================\n",
        "    # POLICYHOLDER CHARACTERISTICS\n",
        "    # ========================================================================\n",
        "    df_fe['high_income'] = (df_fe['annual_income'] > 70000).astype(int)\n",
        "    df_fe['mid_income'] = ((df_fe['annual_income'] >= 40000) & (df_fe['annual_income'] <= 70000)).astype(int)\n",
        "    df_fe['low_income'] = (df_fe['annual_income'] < 40000).astype(int)\n",
        "    df_fe['very_high_income'] = (df_fe['annual_income'] > 100000).astype(int)\n",
        "\n",
        "    df_fe['high_safety_rating'] = (df_fe['safety_rating'] > 80).astype(int)\n",
        "    df_fe['low_safety_rating'] = (df_fe['safety_rating'] < 60).astype(int)\n",
        "    df_fe['very_high_safety'] = (df_fe['safety_rating'] > 90).astype(int)\n",
        "    df_fe['medium_safety'] = ((df_fe['safety_rating'] >= 60) & (df_fe['safety_rating'] <= 80)).astype(int)\n",
        "\n",
        "    df_fe['contact_available'] = df_fe['email_or_tel_available']\n",
        "    df_fe['has_education'] = df_fe['high_education_ind']\n",
        "    df_fe['recent_move'] = df_fe['address_change_ind']\n",
        "    df_fe['home_owner'] = (df_fe['living_status'] == 'Own').astype(int)\n",
        "    df_fe['renter'] = (df_fe['living_status'] == 'Rent').astype(int)\n",
        "    df_fe['female'] = (df_fe['gender'] == 'F').astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # CHANNEL FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['via_broker'] = (df_fe['channel'] == 'Broker').astype(int)\n",
        "    df_fe['via_online'] = (df_fe['channel'] == 'Online').astype(int)\n",
        "    df_fe['via_phone'] = (df_fe['channel'] == 'Phone').astype(int)\n",
        "    df_fe['in_network_repair'] = (df_fe['in_network_bodyshop'] == 'yes').astype(int)\n",
        "    df_fe['out_network_repair'] = (df_fe['in_network_bodyshop'] == 'no').astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # COMPOSITE RECOVERY SCORES\n",
        "    # ========================================================================\n",
        "    liability_score = np.sqrt((100 - df_fe['liab_prct']) / 100.0)\n",
        "    evidence_score_composite = (df_fe['evidence_none'] * 0.0 + df_fe['evidence_weak'] * 0.4 +\n",
        "                      df_fe['evidence_strong'] * 0.7 + df_fe['evidence_very_strong'] * 1.0)\n",
        "    clarity_score = df_fe['recovery_case_clarity'] / 3.0\n",
        "    site_score = df_fe['high_risk_site'] * 0.7 + (1 - df_fe['unknown_site']) * 0.3\n",
        "\n",
        "    df_fe['recovery_feasibility_score'] = (0.35 * liability_score + 0.30 * df_fe['has_recovery_target'] +\n",
        "                                           0.20 * evidence_score_composite + 0.10 * clarity_score + 0.05 * site_score)\n",
        "\n",
        "    # NEW: Alternative recovery potential score from Doc 8\n",
        "    df_fe['recovery_potential'] = (\n",
        "        (100 - df_fe['liab_prct']) * 0.4 +\n",
        "        df_fe['evidence_score'] * 20 * 0.3 +\n",
        "        df_fe['multicar_binary'] * 30 * 0.2 +\n",
        "        (df_fe['claim_est_payout'] / 100) * 0.1\n",
        "    )\n",
        "\n",
        "    # ========================================================================\n",
        "    # DOMAIN LOGIC FLAGS (CRITICAL FOR F1)\n",
        "    # ========================================================================\n",
        "    df_fe['perfect_case'] = ((df_fe['liab_prct'] < 15) & (df_fe['witness_present'] == 1) &\n",
        "                             (df_fe['police_report'] == 1) & (df_fe['has_recovery_target'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['strong_case'] = ((df_fe['liab_prct'] < 25) & (df_fe['evidence_strong'] == 1) &\n",
        "                            (df_fe['has_recovery_target'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['good_case'] = ((df_fe['liab_prct'] < 35) & (df_fe['evidence_score'] >= 1) &\n",
        "                          (df_fe['has_recovery_target'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['weak_case'] = ((df_fe['liab_prct'] > 40) | (df_fe['is_single_car'] == 1) |\n",
        "                          (df_fe['evidence_none'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['no_case'] = ((df_fe['liab_prct'] > 60) | ((df_fe['is_single_car'] == 1) & (df_fe['evidence_none'] == 1))).astype(int)\n",
        "\n",
        "    df_fe['high_value_opportunity'] = ((df_fe['claim_est_payout'] > 3000) & (df_fe['liab_prct'] < 30) &\n",
        "                                       (df_fe['has_recovery_target'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['slam_dunk_case'] = ((df_fe['liab_prct'] < 10) & (df_fe['witness_present'] == 1) &\n",
        "                               (df_fe['police_report'] == 1) & (df_fe['multicar_binary'] == 1) &\n",
        "                               (df_fe['high_risk_site'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['low_liab_high_payout'] = ((df_fe['liab_prct'] < 20) & (df_fe['claim_est_payout'] > 5000)).astype(int)\n",
        "    df_fe['clear_fault_case'] = ((df_fe['liab_prct'] < 15) & (df_fe['multicar_binary'] == 1)).astype(int)\n",
        "    df_fe['high_mileage_low_fault'] = ((df_fe['vehicle_mileage'] > 100000) & (df_fe['liab_prct'] < 30)).astype(int)\n",
        "\n",
        "    # NEW: More interaction flags from Doc 8\n",
        "    df_fe['low_liab_witness_police'] = ((df_fe['liab_prct'] < 20) & (df_fe['witness_binary'] == 1) &\n",
        "                                         (df_fe['police_binary'] == 1)).astype(int)\n",
        "    df_fe['multicar_low_liab'] = ((df_fe['multicar_binary'] == 1) & (df_fe['liab_prct'] < 25)).astype(int)\n",
        "    df_fe['high_payout_evidence'] = ((df_fe['claim_est_payout'] > 5000) & (df_fe['evidence_score'] >= 1)).astype(int)\n",
        "    df_fe['severe_damage_low_fault'] = ((df_fe['payout_to_price_ratio'] > 0.3) & (df_fe['liab_prct'] < 30)).astype(int)\n",
        "    df_fe['minor_damage_high_fault'] = ((df_fe['payout_to_price_ratio'] < 0.1) & (df_fe['liab_prct'] > 50)).astype(int)\n",
        "\n",
        "    # --- Temporal & Behavior Dynamics ---\n",
        "    df_fe['claim_early_in_year'] = (df_fe['claim_month'] <= 3).astype(int)\n",
        "    df_fe['claim_end_of_year'] = (df_fe['claim_month'] >= 10).astype(int)\n",
        "    df_fe['weekend_parking'] = df_fe['is_weekend'] * (df_fe['accident_site'] == 'Parking Area').astype(int)\n",
        "    df_fe['winter_claim_high_payout'] = ((df_fe['season'] == 'Winter') & (df_fe['claim_est_payout'] > 5000)).astype(int)\n",
        "\n",
        "    # --- Vehicle Utilization Proxies (without vehicle_age) ---\n",
        "    df_fe['mileage_x_weight'] = df_fe['vehicle_mileage'] * df_fe['vehicle_weight']\n",
        "    df_fe['mileage_per_dollar'] = df_fe['vehicle_mileage'] / (df_fe['vehicle_price'] + 1)\n",
        "    df_fe['payout_to_weight'] = df_fe['claim_est_payout'] / (df_fe['vehicle_weight'] + 1)\n",
        "\n",
        "    # --- Policyholder Risk Profile ---\n",
        "    df_fe['unstable_policyholder'] = ((df_fe['recent_move'] == 1) & (df_fe['renter'] == 1)).astype(int)\n",
        "    df_fe['financial_stress_risk'] = ((df_fe['expensive_for_income'] == 1) & (df_fe['large_payout'] == 1)).astype(int)\n",
        "    df_fe['young_driver_highway'] = df_fe['young_driver'] * df_fe['highway_accident']\n",
        "    df_fe['senior_driver_parking'] = df_fe['senior_driver'] * df_fe['parking_accident']\n",
        "\n",
        "    # --- Liability & Evidence Interaction Insights ---\n",
        "    df_fe['low_liab_weak_evidence'] = ((df_fe['liab_prct'] < 20) & (df_fe['evidence_weak'] == 1)).astype(int)\n",
        "    df_fe['high_liab_strong_evidence'] = ((df_fe['liab_prct'] > 50) & (df_fe['evidence_strong'] == 1)).astype(int)\n",
        "\n",
        "    # Composite confidence / case quality index\n",
        "    df_fe['case_confidence_score'] = (\n",
        "        0.4 * (100 - df_fe['liab_prct']) / 100 +\n",
        "        0.4 * df_fe['evidence_score'] / 2 +\n",
        "        0.2 * df_fe['recovery_case_clarity'] / 3\n",
        "    )\n",
        "\n",
        "    # --- Statistical Normalization & Percentile Features ---\n",
        "    for col in ['claim_est_payout', 'vehicle_mileage', 'annual_income']:\n",
        "        df_fe[f'{col}_z'] = (df_fe[col] - df_fe[col].mean()) / (df_fe[col].std() + 1e-9)\n",
        "\n",
        "    try:\n",
        "        df_fe['liab_percentile'] = pd.qcut(df_fe['liab_prct'], 10, labels=False, duplicates='drop')\n",
        "        df_fe['payout_percentile'] = pd.qcut(df_fe['claim_est_payout'], 10, labels=False, duplicates='drop')\n",
        "    except Exception:\n",
        "        df_fe['liab_percentile'] = np.nan\n",
        "        df_fe['payout_percentile'] = np.nan\n",
        "\n",
        "    # --- Aggregate / Hybrid Indices ---\n",
        "    df_fe['case_strength_index'] = df_fe['evidence_score'] * (1 - df_fe['liab_prct'] / 100)\n",
        "    df_fe['financial_exposure_index'] = (\n",
        "        (df_fe['claim_est_payout'] / (df_fe['annual_income'] + 1)) * (1 + df_fe['liab_prct'] / 100)\n",
        "    )\n",
        "    df_fe['behavioral_risk_index'] = (\n",
        "        df_fe['claims_per_year_driving'] * (100 - df_fe['safety_rating']) / 100\n",
        "    )\n",
        "\n",
        "    return df_fe\n",
        "\n",
        "print(\"✓ Feature engineering function defined (190+ features)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGix2VMUrOME",
        "outputId": "574059b4-4fab-4bfe-e307-5e9164248c0c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Feature engineering function defined (190+ features)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pre-modeling with target encoding\n"
      ],
      "metadata": {
        "id": "tU8YI01crqp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Running Feature Engineering on train and test data...\")\n",
        "\n",
        "train_fe = feature_engineer(train_df)\n",
        "test_fe = feature_engineer(test_df)\n",
        "print(\"✓ Feature engineering complete.\")\n",
        "\n",
        "# Define Categorical Feature Lists\n",
        "features_to_target_encode = [\n",
        "    'gender', 'living_status', 'accident_site',\n",
        "    'channel', 'vehicle_category', 'vehicle_color', 'accident_type',\n",
        "    'in_network_bodyshop', 'season', 'zip_code'\n",
        "]\n",
        "\n",
        "# Apply Target Encoding\n",
        "print(f\"\\nApplying Smoothed Target Encoding to {len(features_to_target_encode)} features...\")\n",
        "global_mean = train_fe['subrogation'].mean()\n",
        "categorical_features_for_lgbm = []\n",
        "\n",
        "for col in features_to_target_encode:\n",
        "    target_mean = train_fe.groupby(col)['subrogation'].mean()\n",
        "    category_counts = train_fe.groupby(col).size()\n",
        "    smoothing = 20\n",
        "\n",
        "    smoothed_mean = (target_mean * category_counts + global_mean * smoothing) / (category_counts + smoothing)\n",
        "\n",
        "    new_col_name = f'{col}_target_enc'\n",
        "    train_fe[new_col_name] = train_fe[col].map(smoothed_mean)\n",
        "    test_fe[new_col_name] = test_fe[col].map(smoothed_mean)\n",
        "\n",
        "    test_fe[new_col_name] = test_fe[new_col_name].fillna(global_mean)\n",
        "\n",
        "    categorical_features_for_lgbm.append(new_col_name)\n",
        "\n",
        "print(\"✓ Target encoding complete.\")\n",
        "\n",
        "# Create Final X, y, and X_test\n",
        "y_all = train_fe['subrogation'].copy()\n",
        "\n",
        "drop_cols = [\n",
        "    'subrogation', 'claim_number', 'claim_date', 'year_of_born',\n",
        "    'witness_present_ind', 'policy_report_filed_ind',\n",
        "    'vehicle_made_year',  # Bad data quality\n",
        "    'claim_hour'  # Drop raw hour (we keep rush_hour and late_night flags)\n",
        "]\n",
        "drop_cols.extend(features_to_target_encode)\n",
        "\n",
        "feature_cols = [col for col in train_fe.columns if col not in drop_cols]\n",
        "X_all = train_fe[feature_cols].copy()\n",
        "X_test_all = test_fe[feature_cols].copy()\n",
        "\n",
        "# Apply Label Encoding (if any object columns remain)\n",
        "other_cat_cols = X_all.select_dtypes(include='object').columns.tolist()\n",
        "if other_cat_cols:\n",
        "    print(f\"\\nApplying Label Encoding to {len(other_cat_cols)} remaining features...\")\n",
        "    for col in other_cat_cols:\n",
        "        le = LabelEncoder()\n",
        "        all_values = pd.concat([X_all[col].astype(str), X_test_all[col].astype(str)]).unique()\n",
        "        le.fit(all_values)\n",
        "        X_all[col] = le.transform(X_all[col].astype(str))\n",
        "        X_test_all[col] = le.transform(X_test_all[col].astype(str))\n",
        "    print(\"✓ Label encoding complete.\")\n",
        "\n",
        "# Impute NaN values with median\n",
        "print(\"\\nImputing NaN values with the median from the training data...\")\n",
        "X_all_median = X_all.median()\n",
        "X_all = X_all.fillna(X_all_median)\n",
        "X_test_all = X_test_all.fillna(X_all_median)\n",
        "print(\"✓ NaN values imputed.\")\n",
        "\n",
        "# Calculate scale_pos_weight\n",
        "scale_pos_weight = (y_all == 0).sum() / (y_all == 1).sum()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRE-MODELING COMPLETE\")\n",
        "print(f\"✓ X_all shape: {X_all.shape}\")\n",
        "print(f\"✓ y_all shape: {y_all.shape}\")\n",
        "print(f\"✓ X_test_all shape: {X_test_all.shape}\")\n",
        "print(f\"✓ Total features: {len(feature_cols)}\")\n",
        "print(f\"✓ scale_pos_weight (for F1 score): {scale_pos_weight:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84kTpL7TrtRZ",
        "outputId": "dedc68fb-980a-4641-c0a8-35ea5c11d2a4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Running Feature Engineering on train and test data...\n",
            "✓ Feature engineering complete.\n",
            "\n",
            "Applying Smoothed Target Encoding to 10 features...\n",
            "✓ Target encoding complete.\n",
            "\n",
            "Applying Label Encoding to 1 remaining features...\n",
            "✓ Label encoding complete.\n",
            "\n",
            "Imputing NaN values with the median from the training data...\n",
            "✓ NaN values imputed.\n",
            "\n",
            "================================================================================\n",
            "PRE-MODELING COMPLETE\n",
            "✓ X_all shape: (17999, 201)\n",
            "✓ y_all shape: (17999,)\n",
            "✓ X_test_all shape: (12000, 201)\n",
            "✓ Total features: 201\n",
            "✓ scale_pos_weight (for F1 score): 3.3740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# modified optuna with kfold + early stopping\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ylBTKRGnsYTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 1: OPTUNA WITH 5-FOLD CV (PR-AUC + F1 TRACKING)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
        "\n",
        "# ---- Toggle which metric to optimize\n",
        "TARGET = \"roc\"   # \"roc\" or \"pr\"\n",
        "\n",
        "# ---- Persistent storage\n",
        "storage = \"sqlite:///lgbm_optuna_pr.db\"\n",
        "pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource=1, reduction_factor=3)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=\"lgbm_kfold_prauc\",\n",
        "    storage=storage,\n",
        "    load_if_exists=True,\n",
        "    pruner=pruner,\n",
        ")\n",
        "\n",
        "# ---- Objective with 5-fold CV + PR-AUC optimization + F1 tracking\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
        "        'max_depth': trial.suggest_int('max_depth', 4, 20),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
        "        'scale_pos_weight': scale_pos_weight,\n",
        "        'random_state': RANDOM_STATE,\n",
        "        'verbose': -1,\n",
        "        'n_jobs': -1,\n",
        "    }\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "    fold_scores = []\n",
        "    fold_f1s = []  # Track F1 scores\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_all, y_all), start=1):\n",
        "        X_tr, X_va = X_all.iloc[train_idx], X_all.iloc[val_idx]\n",
        "        y_tr, y_va = y_all.iloc[train_idx], y_all.iloc[val_idx]\n",
        "\n",
        "        model = lgb.LGBMClassifier(**params)\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "            eval_metric=\"auc\",\n",
        "            callbacks=[lgb.early_stopping(100, verbose=False)]\n",
        "        )\n",
        "\n",
        "        if hasattr(model, \"best_iteration_\") and model.best_iteration_ is not None:\n",
        "            proba = model.predict_proba(X_va, num_iteration=model.best_iteration_)[:, 1]\n",
        "        else:\n",
        "            proba = model.predict_proba(X_va)[:, 1]\n",
        "\n",
        "        # Calculate PR-AUC (what we optimize)\n",
        "        score = average_precision_score(y_va, proba)\n",
        "        fold_scores.append(score)\n",
        "\n",
        "        # Also calculate best F1 for this fold (for tracking only)\n",
        "        prec, rec, thr = precision_recall_curve(y_va, proba)\n",
        "        f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
        "        best_f1 = float(np.nanmax(f1s[:-1]))\n",
        "        fold_f1s.append(best_f1)\n",
        "\n",
        "        trial.report(float(np.mean(fold_scores)), step=fold_idx)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    # Set user attributes to track F1 (won't affect optimization)\n",
        "    trial.set_user_attr(\"mean_f1\", float(np.mean(fold_f1s)))\n",
        "    trial.set_user_attr(\"std_f1\", float(np.std(fold_f1s)))\n",
        "\n",
        "    return float(np.mean(fold_scores))\n",
        "\n",
        "print(\"\\nRunning Optuna with 5-fold CV (optimizing PR-AUC, tracking F1)...\")\n",
        "print(\"Progress being written to lgbm_optuna_pr.db\\n\")\n",
        "\n",
        "try:\n",
        "    study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OPTUNA RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"✓ Best Mean PR-AUC (5-fold): {study.best_value:.4f}\")\n",
        "print(f\"✓ Best Mean F1 (5-fold):     {study.best_trial.user_attrs['mean_f1']:.4f} ± {study.best_trial.user_attrs['std_f1']:.4f}\")\n",
        "\n",
        "print(\"\\n✓ Best parameters:\")\n",
        "for k, v in study.best_params.items():\n",
        "    print(f\"  - {k}: {v}\")\n",
        "\n",
        "# Show top 5 trials with their F1 scores\n",
        "print(\"\\n📊 Top 5 Trials (by PR-AUC):\")\n",
        "print(f\"{'Trial':<8} {'PR-AUC':<10} {'Mean F1':<10} {'Std F1':<10}\")\n",
        "print(\"=\"*40)\n",
        "top_trials = sorted(study.trials, key=lambda t: t.value if t.value is not None else -1, reverse=True)[:5]\n",
        "for t in top_trials:\n",
        "    if t.value is not None and 'mean_f1' in t.user_attrs:\n",
        "        print(f\"{t.number:<8} {t.value:<10.4f} {t.user_attrs['mean_f1']:<10.4f} {t.user_attrs['std_f1']:<10.4f}\")\n",
        "\n",
        "# Save best params\n",
        "best_lgbm_params = study.best_params.copy()\n",
        "best_lgbm_params.update({\n",
        "    'scale_pos_weight': scale_pos_weight,\n",
        "    'random_state': RANDOM_STATE,\n",
        "    'verbose': -1\n",
        "})\n",
        "with open(\"best_lgbm_params_pr.json\", \"w\") as f:\n",
        "    json.dump(best_lgbm_params, f)\n",
        "\n",
        "print(\"\\n✓ Best parameters saved to best_lgbm_params_pr.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174,
          "referenced_widgets": [
            "091ba666a7284fb7a1eca1b5124edc4f",
            "635e3c475bfd43259c48e8b5a3353388",
            "efed44b81b19454691cf6608f797842d",
            "e3dd593530a84874844393fb0e962976",
            "223cb1959bb840a3b8a1870e7c716003",
            "9579b7a658cb48318f51f81968eb8de5",
            "aee3b1ed33314bd0b619ed5e113e94b3",
            "9909546a6e41433f85cfb9790edaa421",
            "6edcbf2277b14faba22fb9c5ef1a78fc",
            "2c6eade5614c44899da65aabc5e22c32",
            "ff77477f279f4c12917c0794d5351eec"
          ]
        },
        "id": "wY2Beo4lsdcS",
        "outputId": "c74d56c2-91a5-4b82-d65f-90d550c2083e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 1: OPTUNA WITH 5-FOLD CV (PR-AUC + F1 TRACKING)\n",
            "================================================================================\n",
            "\n",
            "Running Optuna with 5-fold CV (optimizing PR-AUC, tracking F1)...\n",
            "Progress being written to lgbm_optuna_pr.db\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "091ba666a7284fb7a1eca1b5124edc4f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# kfold cv"
      ],
      "metadata": {
        "id": "S4rxwdJ1XL52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL 5-FOLD CV WITH BEST PARAMETERS — detailed diagnostics & thresholding\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, log_loss, brier_score_loss,\n",
        "    precision_recall_curve, roc_curve, f1_score, precision_score, recall_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL 5-FOLD CV WITH BEST PARAMETERS (Detailed Analysis)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "assert 'X_all' in globals() and 'y_all' in globals(), \"X_all / y_all not found.\"\n",
        "assert 'best_lgbm_params' in globals(), \"best_lgbm_params not found.\"\n",
        "assert 'RANDOM_STATE' in globals(), \"RANDOM_STATE not found.\"\n",
        "\n",
        "# Identify categorical features (if any)\n",
        "if isinstance(X_all, pd.DataFrame):\n",
        "    cat_features = X_all.select_dtypes(include='category').columns.tolist()\n",
        "else:\n",
        "    cat_features = []\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# Storage for OOF and per-fold details\n",
        "oof_probs = np.zeros(len(y_all), dtype=float)\n",
        "fold_rows = []\n",
        "pos_rate = float(np.mean(y_all))\n",
        "\n",
        "print(f\"\\nClass balance: positives = {pos_rate*100:.2f}%  |  negatives = {(1-pos_rate)*100:.2f}%\")\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_all, y_all), 1):\n",
        "    print(f\"\\n{'='*40}\\nFold {fold}/5\\n{'='*40}\")\n",
        "\n",
        "    X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
        "    y_tr, y_va = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
        "\n",
        "    model = lgb.LGBMClassifier(**best_lgbm_params)\n",
        "\n",
        "    # Train with early stopping via callbacks (compatible with older LightGBM)\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=\"auc\",\n",
        "        callbacks=[lgb.early_stopping(200, verbose=False), lgb.log_evaluation(period=0)],\n",
        "        categorical_feature=cat_features if len(cat_features) > 0 else \"auto\",\n",
        "    )\n",
        "\n",
        "    # Predict using best_iteration_ if available\n",
        "    if hasattr(model, \"best_iteration_\") and model.best_iteration_ is not None:\n",
        "        probs = model.predict_proba(X_va, num_iteration=model.best_iteration_)[:, 1]\n",
        "        best_iters = int(model.best_iteration_)\n",
        "    else:\n",
        "        probs = model.predict_proba(X_va)[:, 1]\n",
        "        best_iters = int(best_lgbm_params.get('n_estimators', 0)) or None\n",
        "\n",
        "    # Store OOF probabilities\n",
        "    oof_probs[va_idx] = probs\n",
        "\n",
        "    # ---- Fold metrics (threshold-free)\n",
        "    fold_roc = roc_auc_score(y_va, probs)\n",
        "    fold_ap  = average_precision_score(y_va, probs)\n",
        "    # log_loss needs probs in (0,1); clip to be safe\n",
        "    fold_logloss = log_loss(y_va, np.clip(probs, 1e-7, 1-1e-7))\n",
        "    fold_brier   = brier_score_loss(y_va, probs)\n",
        "    fpr, tpr, _  = roc_curve(y_va, probs)\n",
        "    fold_ks      = float(np.max(tpr - fpr))\n",
        "\n",
        "    # ---- Fold-optimal threshold by PR curve (maximize F1)\n",
        "    prec, rec, thr = precision_recall_curve(y_va, probs)\n",
        "    f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
        "    # thresholds has length len(prec)-1; align indices\n",
        "    best_idx = int(np.nanargmax(f1s[:-1])) if len(f1s) > 1 else 0\n",
        "    fold_thresh = float(thr[max(0, best_idx)])\n",
        "\n",
        "    y_pred_fold = (probs >= fold_thresh).astype(int)\n",
        "    fold_f1     = f1_score(y_va, y_pred_fold)\n",
        "    fold_prec   = precision_score(y_va, y_pred_fold, zero_division=0)\n",
        "    fold_rec    = recall_score(y_va, y_pred_fold, zero_division=0)\n",
        "\n",
        "    print(f\"  AUC-ROC: {fold_roc:.4f} | PR-AUC (AP): {fold_ap:.4f} | LogLoss: {fold_logloss:.4f} | Brier: {fold_brier:.4f} | KS: {fold_ks:.4f}\")\n",
        "    print(f\"  Fold-opt Threshold: {fold_thresh:.4f} | F1: {fold_f1:.4f} | Precision: {fold_prec:.4f} | Recall: {fold_rec:.4f} | Best iters: {best_iters}\")\n",
        "\n",
        "    fold_rows.append({\n",
        "        \"Fold\": fold,\n",
        "        \"ROC_AUC\": fold_roc,\n",
        "        \"PR_AUC\": fold_ap,\n",
        "        \"LogLoss\": fold_logloss,\n",
        "        \"Brier\": fold_brier,\n",
        "        \"KS\": fold_ks,\n",
        "        \"Fold_Threshold\": fold_thresh,\n",
        "        \"Fold_F1\": fold_f1,\n",
        "        \"Fold_Precision\": fold_prec,\n",
        "        \"Fold_Recall\": fold_rec,\n",
        "        \"Best_Iterations\": best_iters,\n",
        "    })\n",
        "\n",
        "# =========================\n",
        "# OOF Summary (threshold-free)\n",
        "# =========================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"OOF SUMMARY (Threshold-Free)\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "oof_roc = roc_auc_score(y_all, oof_probs)\n",
        "oof_ap  = average_precision_score(y_all, oof_probs)\n",
        "oof_logloss = log_loss(y_all, np.clip(oof_probs, 1e-7, 1-1e-7))\n",
        "oof_brier   = brier_score_loss(y_all, oof_probs)\n",
        "fpr, tpr, _ = roc_curve(y_all, oof_probs)\n",
        "oof_ks      = float(np.max(tpr - fpr))\n",
        "\n",
        "print(f\"✓ OOF AUC-ROC: {oof_roc:.4f}\")\n",
        "print(f\"✓ OOF PR-AUC (AP): {oof_ap:.4f}\")\n",
        "print(f\"✓ OOF LogLoss: {oof_logloss:.4f}\")\n",
        "print(f\"✓ OOF Brier score: {oof_brier:.4f}\")\n",
        "print(f\"✓ OOF KS statistic: {oof_ks:.4f}\")\n",
        "\n",
        "# =========================\n",
        "# Global OOF threshold search (maximize F1)\n",
        "# =========================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"GLOBAL OOF THRESHOLD OPTIMIZATION (by F1)\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "prec, rec, thr = precision_recall_curve(y_all, oof_probs)\n",
        "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
        "best_idx = int(np.nanargmax(f1s[:-1])) if len(f1s) > 1 else 0\n",
        "global_thresh = float(thr[max(0, best_idx)])\n",
        "global_f1 = float(f1s[best_idx])\n",
        "\n",
        "global_preds = (oof_probs >= global_thresh).astype(int)\n",
        "global_prec = precision_score(y_all, global_preds, zero_division=0)\n",
        "global_rec  = recall_score(y_all, global_preds, zero_division=0)\n",
        "tn, fp, fn, tp = confusion_matrix(y_all, global_preds).ravel()\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "\n",
        "print(f\"✅ Best Global Threshold: {global_thresh:.4f}\")\n",
        "print(f\"✅ OOF F1 at Global Threshold: {global_f1:.4f}\")\n",
        "print(f\"   Precision: {global_prec:.4f} | Recall: {global_rec:.4f} | Specificity: {specificity:.4f}\")\n",
        "print(f\"   Confusion Matrix @ threshold {global_thresh:.4f} -> TN:{tn}  FP:{fp}  FN:{fn}  TP:{tp}\")\n",
        "\n",
        "# =========================\n",
        "# Per-fold table & dispersion\n",
        "# =========================\n",
        "fold_df = pd.DataFrame(fold_rows)\n",
        "pd.set_option('display.float_format', lambda x: f\"{x:.4f}\")\n",
        "\n",
        "print(\"\\n📊 Per-Fold Results:\")\n",
        "print(fold_df.to_string(index=False))\n",
        "\n",
        "def _mean_std(col):\n",
        "    return f\"{fold_df[col].mean():.4f} ± {fold_df[col].std():.4f}\"\n",
        "\n",
        "print(\"\\nSummary (mean ± std across folds):\")\n",
        "print(f\"  ROC_AUC       : {_mean_std('ROC_AUC')}\")\n",
        "print(f\"  PR_AUC (AP)   : {_mean_std('PR_AUC')}\")\n",
        "print(f\"  LogLoss       : {_mean_std('LogLoss')}\")\n",
        "print(f\"  Brier         : {_mean_std('Brier')}\")\n",
        "print(f\"  KS            : {_mean_std('KS')}\")\n",
        "print(f\"  Fold_F1       : {_mean_std('Fold_F1')}\")\n",
        "print(f\"  Fold_Threshold: {_mean_std('Fold_Threshold')}\")\n",
        "print(f\"  Best_Iterations: {_mean_std('Best_Iterations') if fold_df['Best_Iterations'].notna().any() else 'n/a'}\")\n",
        "\n",
        "# Final recommended threshold for deployment (from OOF global search)\n",
        "final_threshold = global_thresh\n",
        "print(f\"\\n🎯 Recommended Operating Threshold (from OOF): {final_threshold:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYeeoWatXOPo",
        "outputId": "c30e2936-14ad-4c04-9206-e5de818f7e9c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL 5-FOLD CV WITH BEST PARAMETERS (Detailed Analysis)\n",
            "================================================================================\n",
            "\n",
            "Class balance: positives = 22.86%  |  negatives = 77.14%\n",
            "\n",
            "========================================\n",
            "Fold 1/5\n",
            "========================================\n",
            "  AUC-ROC: 0.8357 | PR-AUC (AP): 0.5884 | LogLoss: 0.4527 | Brier: 0.1509 | KS: 0.5172\n",
            "  Fold-opt Threshold: 0.5381 | F1: 0.5882 | Precision: 0.5237 | Recall: 0.6707 | Best iters: 72\n",
            "\n",
            "========================================\n",
            "Fold 2/5\n",
            "========================================\n",
            "  AUC-ROC: 0.8378 | PR-AUC (AP): 0.6114 | LogLoss: 0.4606 | Brier: 0.1539 | KS: 0.5258\n",
            "  Fold-opt Threshold: 0.6001 | F1: 0.5943 | Precision: 0.5508 | Recall: 0.6452 | Best iters: 62\n",
            "\n",
            "========================================\n",
            "Fold 3/5\n",
            "========================================\n",
            "  AUC-ROC: 0.8530 | PR-AUC (AP): 0.6143 | LogLoss: 0.4496 | Brier: 0.1495 | KS: 0.5479\n",
            "  Fold-opt Threshold: 0.5643 | F1: 0.6200 | Precision: 0.5440 | Recall: 0.7205 | Best iters: 69\n",
            "\n",
            "========================================\n",
            "Fold 4/5\n",
            "========================================\n",
            "  AUC-ROC: 0.8321 | PR-AUC (AP): 0.5946 | LogLoss: 0.4653 | Brier: 0.1556 | KS: 0.5157\n",
            "  Fold-opt Threshold: 0.5859 | F1: 0.5875 | Precision: 0.5343 | Recall: 0.6525 | Best iters: 68\n",
            "\n",
            "========================================\n",
            "Fold 5/5\n",
            "========================================\n",
            "  AUC-ROC: 0.8372 | PR-AUC (AP): 0.5917 | LogLoss: 0.4603 | Brier: 0.1539 | KS: 0.5204\n",
            "  Fold-opt Threshold: 0.5901 | F1: 0.5958 | Precision: 0.5473 | Recall: 0.6537 | Best iters: 84\n",
            "\n",
            "================================================================================\n",
            "OOF SUMMARY (Threshold-Free)\n",
            "================================================================================\n",
            "✓ OOF AUC-ROC: 0.8390\n",
            "✓ OOF PR-AUC (AP): 0.5979\n",
            "✓ OOF LogLoss: 0.4577\n",
            "✓ OOF Brier score: 0.1527\n",
            "✓ OOF KS statistic: 0.5226\n",
            "\n",
            "================================================================================\n",
            "GLOBAL OOF THRESHOLD OPTIMIZATION (by F1)\n",
            "================================================================================\n",
            "✅ Best Global Threshold: 0.5515\n",
            "✅ OOF F1 at Global Threshold: 0.5937\n",
            "   Precision: 0.5214 | Recall: 0.6892 | Specificity: 0.8125\n",
            "   Confusion Matrix @ threshold 0.5515 -> TN:11281  FP:2603  FN:1279  TP:2836\n",
            "\n",
            "📊 Per-Fold Results:\n",
            " Fold  ROC_AUC  PR_AUC  LogLoss  Brier     KS  Fold_Threshold  Fold_F1  Fold_Precision  Fold_Recall  Best_Iterations\n",
            "    1   0.8357  0.5884   0.4527 0.1509 0.5172          0.5381   0.5882          0.5237       0.6707               72\n",
            "    2   0.8378  0.6114   0.4606 0.1539 0.5258          0.6001   0.5943          0.5508       0.6452               62\n",
            "    3   0.8530  0.6143   0.4496 0.1495 0.5479          0.5643   0.6200          0.5440       0.7205               69\n",
            "    4   0.8321  0.5946   0.4653 0.1556 0.5157          0.5859   0.5875          0.5343       0.6525               68\n",
            "    5   0.8372  0.5917   0.4603 0.1539 0.5204          0.5901   0.5958          0.5473       0.6537               84\n",
            "\n",
            "Summary (mean ± std across folds):\n",
            "  ROC_AUC       : 0.8392 ± 0.0080\n",
            "  PR_AUC (AP)   : 0.6001 ± 0.0119\n",
            "  LogLoss       : 0.4577 ± 0.0064\n",
            "  Brier         : 0.1527 ± 0.0025\n",
            "  KS            : 0.5254 ± 0.0132\n",
            "  Fold_F1       : 0.5972 ± 0.0133\n",
            "  Fold_Threshold: 0.5757 ± 0.0247\n",
            "  Best_Iterations: 71.0000 ± 8.1240\n",
            "\n",
            "🎯 Recommended Operating Threshold (from OOF): 0.5515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# threshold selection"
      ],
      "metadata": {
        "id": "HXFN2Slp-oiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 3: FIND OPTIMAL THRESHOLD ON CALIBRATED VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Split for threshold finding: 85% train/cal, 15% validation\n",
        "X_train_cal, X_val_thresh, y_train_cal, y_val_thresh = train_test_split(\n",
        "    X_all, y_all,\n",
        "    test_size=0.15,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_all\n",
        ")\n",
        "\n",
        "# Further split train_cal: 85% train, 15% calibration\n",
        "X_train_sub, X_cal_sub, y_train_sub, y_cal_sub = train_test_split(\n",
        "    X_train_cal, y_train_cal,\n",
        "    test_size=0.15 / 0.85,  # To get final 72.25% train, 12.75% cal, 15% val\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_train_cal\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train_sub.shape[0]} samples\")\n",
        "print(f\"Calibration: {X_cal_sub.shape[0]} samples\")\n",
        "print(f\"Validation (for threshold): {X_val_thresh.shape[0]} samples\")\n",
        "\n",
        "# Internal ES split\n",
        "X_tr_es, X_es, y_tr_es, y_es = train_test_split(\n",
        "    X_train_sub, y_train_sub,\n",
        "    test_size=0.10,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_train_sub\n",
        ")\n",
        "\n",
        "cat_features = X_all.select_dtypes(include='category').columns.tolist() if isinstance(X_all, pd.DataFrame) else []\n",
        "\n",
        "# Train base model\n",
        "print(\"\\nTraining base model with early stopping...\")\n",
        "base_model = lgb.LGBMClassifier(**best_lgbm_params)\n",
        "base_model.fit(\n",
        "    X_tr_es, y_tr_es,\n",
        "    eval_set=[(X_es, y_es)],\n",
        "    eval_metric=\"auc\",\n",
        "    callbacks=[lgb.early_stopping(200, verbose=False), lgb.log_evaluation(period=0)],\n",
        "    categorical_feature=cat_features if len(cat_features) > 0 else \"auto\"\n",
        ")\n",
        "print(f\"✓ Base model trained (best_iteration: {base_model.best_iteration_})\")\n",
        "\n",
        "# Calibrate\n",
        "print(\"Calibrating model...\")\n",
        "calibrated_model = CalibratedClassifierCV(\n",
        "    base_model,\n",
        "    method='sigmoid',\n",
        "    cv='prefit'\n",
        ")\n",
        "calibrated_model.fit(X_cal_sub, y_cal_sub)\n",
        "print(\"✓ Model calibrated\")\n",
        "\n",
        "# Get calibrated probabilities on validation set\n",
        "val_probs_calibrated = calibrated_model.predict_proba(X_val_thresh)[:, 1]\n",
        "\n",
        "print(f\"\\nValidation probability statistics:\")\n",
        "print(f\"  Mean: {val_probs_calibrated.mean():.4f}\")\n",
        "print(f\"  Std:  {val_probs_calibrated.std():.4f}\")\n",
        "print(f\"  Median: {np.median(val_probs_calibrated):.4f}\")\n",
        "\n",
        "# Find optimal threshold using multiple methods\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"THRESHOLD SEARCH ON CALIBRATED VALIDATION SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "\n",
        "# Method 1: PR Curve\n",
        "prec, rec, thr = precision_recall_curve(y_val_thresh, val_probs_calibrated)\n",
        "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
        "best_idx = np.argmax(f1s[:-1])\n",
        "threshold_pr = float(thr[best_idx])\n",
        "f1_pr = float(f1s[best_idx])\n",
        "\n",
        "# Method 2: Grid Search\n",
        "thresholds_grid = np.arange(0.15, 0.50, 0.01)\n",
        "f1_scores_grid = []\n",
        "for t in thresholds_grid:\n",
        "    preds = (val_probs_calibrated >= t).astype(int)\n",
        "    f1_scores_grid.append(f1_score(y_val_thresh, preds))\n",
        "\n",
        "best_idx_grid = np.argmax(f1_scores_grid)\n",
        "threshold_grid = float(thresholds_grid[best_idx_grid])\n",
        "f1_grid = float(f1_scores_grid[best_idx_grid])\n",
        "\n",
        "# Method 3: Match training distribution\n",
        "positive_rate = (y_all == 1).sum() / len(y_all)\n",
        "threshold_percentile = float(np.percentile(val_probs_calibrated, (1 - positive_rate) * 100))\n",
        "preds_pct = (val_probs_calibrated >= threshold_percentile).astype(int)\n",
        "f1_pct = float(f1_score(y_val_thresh, preds_pct))\n",
        "\n",
        "print(f\"\\n{'Method':<25} {'Threshold':<12} {'Val F1':<10} {'Precision':<12} {'Recall':<10}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results = [\n",
        "    (\"PR Curve\", threshold_pr, f1_pr),\n",
        "    (\"Grid Search\", threshold_grid, f1_grid),\n",
        "    (\"Percentile Match\", threshold_percentile, f1_pct),\n",
        "]\n",
        "\n",
        "best_f1 = 0\n",
        "for method_name, thresh, f1 in results:\n",
        "    preds = (val_probs_calibrated >= thresh).astype(int)\n",
        "    prec = precision_score(y_val_thresh, preds, zero_division=0)\n",
        "    rec = recall_score(y_val_thresh, preds, zero_division=0)\n",
        "\n",
        "    marker = \"\"\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        optimal_threshold = thresh\n",
        "        best_method = method_name\n",
        "        marker = \" ← BEST\"\n",
        "\n",
        "    print(f\"{method_name:<25} {thresh:<12.4f} {f1:<10.4f} {prec:<12.4f} {rec:<10.4f}{marker}\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n✅ SELECTED: {best_method}\")\n",
        "print(f\"✅ Threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"✅ Validation F1: {best_f1:.4f}\")\n",
        "\n",
        "preds_final = (val_probs_calibrated >= optimal_threshold).astype(int)\n",
        "final_prec = precision_score(y_val_thresh, preds_final, zero_division=0)\n",
        "final_rec = recall_score(y_val_thresh, preds_final, zero_division=0)\n",
        "\n",
        "print(f\"\\nPerformance at selected threshold:\")\n",
        "print(f\"  Precision: {final_prec:.4f}\")\n",
        "print(f\"  Recall:    {final_rec:.4f}\")\n",
        "print(f\"  F1:        {best_f1:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"✓ Threshold selection complete\")\n",
        "print(f\"{'='*80}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFljKjaf-tU0",
        "outputId": "8f5fd3b0-aca4-4d5a-c13d-4af229ae5745"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 3: FIND OPTIMAL THRESHOLD ON CALIBRATED VALIDATION\n",
            "================================================================================\n",
            "Train: 12599 samples\n",
            "Calibration: 2700 samples\n",
            "Validation (for threshold): 2700 samples\n",
            "\n",
            "Training base model with early stopping...\n",
            "✓ Base model trained (best_iteration: 60)\n",
            "Calibrating model...\n",
            "✓ Model calibrated\n",
            "\n",
            "Validation probability statistics:\n",
            "  Mean: 0.2198\n",
            "  Std:  0.2130\n",
            "  Median: 0.1099\n",
            "\n",
            "================================================================================\n",
            "THRESHOLD SEARCH ON CALIBRATED VALIDATION SET\n",
            "================================================================================\n",
            "\n",
            "Method                    Threshold    Val F1     Precision    Recall    \n",
            "================================================================================\n",
            "PR Curve                  0.3093       0.5954     0.5311       0.6775     ← BEST\n",
            "Grid Search               0.3100       0.5953     0.5319       0.6759    \n",
            "Percentile Match          0.4055       0.5765     0.5761       0.5770    \n",
            "================================================================================\n",
            "\n",
            "✅ SELECTED: PR Curve\n",
            "✅ Threshold: 0.3093\n",
            "✅ Validation F1: 0.5954\n",
            "\n",
            "Performance at selected threshold:\n",
            "  Precision: 0.5311\n",
            "  Recall:    0.6775\n",
            "  F1:        0.5954\n",
            "\n",
            "================================================================================\n",
            "✓ Threshold selection complete\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# final model training\n"
      ],
      "metadata": {
        "id": "XF9mW6_QbFaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 4: RETRAIN FINAL MODEL ON 85% DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Split: 85% train, 15% calibration\n",
        "X_train_final, X_cal_final, y_train_final, y_cal_final = train_test_split(\n",
        "    X_all, y_all,\n",
        "    test_size=0.15,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_all\n",
        ")\n",
        "\n",
        "print(f\"Final train: {X_train_final.shape[0]} samples\")\n",
        "print(f\"Final calibration: {X_cal_final.shape[0]} samples\")\n",
        "\n",
        "# Internal ES split from train_final (10% for early stopping)\n",
        "X_tr_final, X_es_final, y_tr_final, y_es_final = train_test_split(\n",
        "    X_train_final, y_train_final,\n",
        "    test_size=0.10,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_train_final\n",
        ")\n",
        "\n",
        "cat_features = X_all.select_dtypes(include='category').columns.tolist() if isinstance(X_all, pd.DataFrame) else []\n",
        "\n",
        "# Train with early stopping\n",
        "print(\"\\nTraining final model with early stopping...\")\n",
        "final_base_model = lgb.LGBMClassifier(**best_lgbm_params)\n",
        "final_base_model.fit(\n",
        "    X_tr_final, y_tr_final,\n",
        "    eval_set=[(X_es_final, y_es_final)],\n",
        "    eval_metric=\"auc\",\n",
        "    callbacks=[lgb.early_stopping(200, verbose=False), lgb.log_evaluation(period=0)],\n",
        "    categorical_feature=cat_features if len(cat_features) > 0 else \"auto\"\n",
        ")\n",
        "print(f\"✓ Final model trained (best_iteration: {final_base_model.best_iteration_})\")\n",
        "\n",
        "# Calibrate\n",
        "print(\"Calibrating final model...\")\n",
        "final_calibrated_model = CalibratedClassifierCV(\n",
        "    final_base_model,\n",
        "    method='sigmoid',\n",
        "    cv='prefit'\n",
        ")\n",
        "final_calibrated_model.fit(X_cal_final, y_cal_final)\n",
        "print(\"✓ Final model calibrated\")\n",
        "\n",
        "# Use threshold from Step 3 (calibrated validation)\n",
        "print(f\"\\n✓ Using threshold from Step 3: {optimal_threshold:.4f}\")\n",
        "print(f\"   Validation F1: {best_f1:.4f}\")\n",
        "print(\"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULYJl5IEbH12",
        "outputId": "5c19d15b-025f-4369-8127-8177b35f3800"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 4: RETRAIN FINAL MODEL ON 85% DATA\n",
            "================================================================================\n",
            "Final train: 15299 samples\n",
            "Final calibration: 2700 samples\n",
            "\n",
            "Training final model with early stopping...\n",
            "✓ Final model trained (best_iteration: 46)\n",
            "Calibrating final model...\n",
            "✓ Final model calibrated\n",
            "\n",
            "✓ Using threshold from Step 3: 0.3093\n",
            "   Validation F1: 0.5954\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prediction"
      ],
      "metadata": {
        "id": "4xq4XmnfclWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 5: GENERATING PREDICTIONS ON TEST SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nGenerating calibrated predictions for {X_test_all.shape[0]} test samples...\")\n",
        "\n",
        "# Get calibrated probabilities\n",
        "test_probabilities = final_calibrated_model.predict_proba(X_test_all)[:, 1]\n",
        "\n",
        "# Apply threshold from Step 3\n",
        "test_predictions = (test_probabilities >= optimal_threshold).astype(int)\n",
        "\n",
        "print(f\"✓ Predictions generated\")\n",
        "print(f\"✓ Using threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"\\nPrediction distribution:\")\n",
        "print(f\"  - Class 0 (No subrogation): {(test_predictions == 0).sum()} ({(test_predictions == 0).sum() / len(test_predictions) * 100:.1f}%)\")\n",
        "print(f\"  - Class 1 (Subrogation):    {(test_predictions == 1).sum()} ({(test_predictions == 1).sum() / len(test_predictions) * 100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nProbability statistics:\")\n",
        "print(f\"  - Mean: {test_probabilities.mean():.4f}\")\n",
        "print(f\"  - Std:  {test_probabilities.std():.4f}\")\n",
        "print(f\"  - Min:  {test_probabilities.min():.4f}\")\n",
        "print(f\"  - Max:  {test_probabilities.max():.4f}\")\n",
        "\n",
        "# Create submission\n",
        "submission = pd.DataFrame({\n",
        "    'claim_number': test_fe['claim_number'],\n",
        "    'subrogation': test_predictions\n",
        "})\n",
        "\n",
        "output_filename = 'TriGuard_pr_calval_thresh_3093.csv'\n",
        "submission.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\n{'='*90}\")\n",
        "print(f\"✓ SUBMISSION FILE SAVED: {output_filename}\")\n",
        "print(f\"{'='*90}\")\n",
        "\n",
        "# Download\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_filename)\n",
        "    print(\"✓ File downloaded!\")\n",
        "except:\n",
        "    print(\"(File saved locally)\")\n",
        "\n",
        "print(f\"\\n{'='*90}\")\n",
        "print(\"PIPELINE COMPLETE!\")\n",
        "print(f\"Expected F1: 0.595-0.605 (based on calibrated validation)\")\n",
        "print(f\"{'='*90}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "lH7RRVQguUeG",
        "outputId": "2a16f8d9-0140-4a12-b01d-acdba13d7018"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 5: GENERATING PREDICTIONS ON TEST SET\n",
            "================================================================================\n",
            "\n",
            "Generating calibrated predictions for 12000 test samples...\n",
            "✓ Predictions generated\n",
            "✓ Using threshold: 0.3093\n",
            "\n",
            "Prediction distribution:\n",
            "  - Class 0 (No subrogation): 8200 (68.3%)\n",
            "  - Class 1 (Subrogation):    3800 (31.7%)\n",
            "\n",
            "Probability statistics:\n",
            "  - Mean: 0.2323\n",
            "  - Std:  0.2166\n",
            "  - Min:  0.0322\n",
            "  - Max:  0.7197\n",
            "\n",
            "==========================================================================================\n",
            "✓ SUBMISSION FILE SAVED: TriGuard_pr_calval_thresh_3093.csv\n",
            "==========================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a3f35193-0e30-4e40-9815-f39d1c511ad2\", \"TriGuard_pr_calval_thresh_3093.csv\", 120025)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ File downloaded!\n",
            "\n",
            "==========================================================================================\n",
            "PIPELINE COMPLETE!\n",
            "Expected F1: 0.595-0.605 (based on calibrated validation)\n",
            "==========================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# choosing threshold\n"
      ],
      "metadata": {
        "id": "1QKWidhIV8mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"THRESHOLD ANALYSIS - PREVIEW DISTRIBUTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Your test probabilities (already calculated)\n",
        "print(f\"\\nTest probability statistics:\")\n",
        "print(f\"  Mean: {test_probabilities.mean():.4f}\")\n",
        "print(f\"  Median: {np.median(test_probabilities):.4f}\")\n",
        "print(f\"  Std: {test_probabilities.std():.4f}\")\n",
        "print(f\"  25th percentile: {np.percentile(test_probabilities, 25):.4f}\")\n",
        "print(f\"  75th percentile: {np.percentile(test_probabilities, 75):.4f}\")\n",
        "\n",
        "# Test multiple thresholds and show distributions\n",
        "thresholds_to_test = [0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.5719]\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"{'Threshold':<12} {'N Positive':<12} {'% Positive':<12} {'N Negative':<12} {'% Negative':<12}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "results = []\n",
        "for thresh in thresholds_to_test:\n",
        "    preds = (test_probabilities >= thresh).astype(int)\n",
        "    n_pos = preds.sum()\n",
        "    n_neg = len(preds) - n_pos\n",
        "    pct_pos = n_pos / len(preds) * 100\n",
        "    pct_neg = 100 - pct_pos\n",
        "\n",
        "    results.append({\n",
        "        'threshold': thresh,\n",
        "        'n_positive': n_pos,\n",
        "        'pct_positive': pct_pos,\n",
        "        'n_negative': n_neg,\n",
        "        'pct_negative': pct_neg\n",
        "    })\n",
        "\n",
        "    marker = \"\"\n",
        "    if thresh == 0.3540:\n",
        "        marker = \" ← Validation optimal\"\n",
        "    elif thresh == 0.5719:\n",
        "        marker = \" ← OOF threshold (current)\"\n",
        "\n",
        "    print(f\"{thresh:<12.4f} {n_pos:<12} {pct_pos:<12.1f} {n_neg:<12} {pct_neg:<12.1f}{marker}\")\n",
        "\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Show which look most reasonable\n",
        "print(\"📊 Analysis:\")\n",
        "print(f\"  - Training data has {(y_all == 1).sum() / len(y_all) * 100:.1f}% positive class\")\n",
        "print(f\"  - Validation @0.3540: F1 = 0.5912 (53.3% precision, 66.5% recall)\")\n",
        "print(f\"  - Validation @0.5719: F1 = 0.3936 (66.0% precision, 28.0% recall)\")\n",
        "print(f\"\\n  💡 Thresholds around 0.30-0.40 look most reasonable\")\n",
        "print(f\"     (25-35% positive predictions, similar to training ratio)\")\n",
        "\n",
        "# Ask which ones to save\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Which thresholds do you want to save as CSV files?\")\n",
        "print(\"Options: 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.5719\")\n",
        "print(\"(You can pick multiple, separated by commas)\")\n",
        "print(f\"{'='*80}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA8AwffKV-sK",
        "outputId": "eba9c1b2-bf21-44cf-ab53-880b7b33cb51"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "THRESHOLD ANALYSIS - PREVIEW DISTRIBUTIONS\n",
            "================================================================================\n",
            "\n",
            "Test probability statistics:\n",
            "  Mean: 0.2325\n",
            "  Median: 0.1305\n",
            "  Std: 0.2148\n",
            "  25th percentile: 0.0493\n",
            "  75th percentile: 0.3976\n",
            "\n",
            "================================================================================\n",
            "Threshold    N Positive   % Positive   N Negative   % Negative  \n",
            "================================================================================\n",
            "0.2500       4383         36.5         7617         63.5        \n",
            "0.3000       3888         32.4         8112         67.6        \n",
            "0.3500       3432         28.6         8568         71.4        \n",
            "0.4000       2976         24.8         9024         75.2        \n",
            "0.4500       2587         21.6         9413         78.4        \n",
            "0.5000       2148         17.9         9852         82.1        \n",
            "0.5719       1474         12.3         10526        87.7         ← OOF threshold (current)\n",
            "================================================================================\n",
            "\n",
            "📊 Analysis:\n",
            "  - Training data has 22.9% positive class\n",
            "  - Validation @0.3540: F1 = 0.5912 (53.3% precision, 66.5% recall)\n",
            "  - Validation @0.5719: F1 = 0.3936 (66.0% precision, 28.0% recall)\n",
            "\n",
            "  💡 Thresholds around 0.30-0.40 look most reasonable\n",
            "     (25-35% positive predictions, similar to training ratio)\n",
            "\n",
            "================================================================================\n",
            "Which thresholds do you want to save as CSV files?\n",
            "Options: 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.5719\n",
            "(You can pick multiple, separated by commas)\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}
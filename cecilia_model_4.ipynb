{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7b831fe2c72a4588911eb9aa1186eff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a5cf8a25429483cad8baae279c80e83",
              "IPY_MODEL_088eda24bd24479394140e12e6b07df7",
              "IPY_MODEL_e16b04f06bfc462f9e411dfc2ed79adc"
            ],
            "layout": "IPY_MODEL_ae664d04052b403cad7dce87f1ddbfaf"
          }
        },
        "9a5cf8a25429483cad8baae279c80e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94d9bcab9e924091ba69bfdab1f5e485",
            "placeholder": "​",
            "style": "IPY_MODEL_02993748e828441e86a2e27b466a1955",
            "value": "Best trial: 83. Best value: 0.578244: 100%"
          }
        },
        "088eda24bd24479394140e12e6b07df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a896d2ddac164efc93ba5ac7085a5339",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5b6fb46533f4529b73a94899b2064ff",
            "value": 100
          }
        },
        "e16b04f06bfc462f9e411dfc2ed79adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_197b63ff137c4200bc8451d3c9a90e63",
            "placeholder": "​",
            "style": "IPY_MODEL_1a27c89af3d44160ada107e9f7686531",
            "value": " 100/100 [17:43&lt;00:00,  6.65s/it]"
          }
        },
        "ae664d04052b403cad7dce87f1ddbfaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94d9bcab9e924091ba69bfdab1f5e485": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02993748e828441e86a2e27b466a1955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a896d2ddac164efc93ba5ac7085a5339": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5b6fb46533f4529b73a94899b2064ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "197b63ff137c4200bc8451d3c9a90e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a27c89af3d44160ada107e9f7686531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# setup and configuration"
      ],
      "metadata": {
        "id": "saaCfeUqppjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# --- Setup ---\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 200)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "# --- Random Seed ---\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_STATE)\n",
        "\n",
        "# --- Model/Metrics ---\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, precision_score, recall_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# --- ML Models ---\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "\n",
        "# --- Tuning ---\n",
        "try:\n",
        "    import optuna\n",
        "except ImportError:\n",
        "    print(\"Installing Optuna...\")\n",
        "    !pip install optuna -q\n",
        "    import optuna\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# --- Feature Engineering Helpers ---\n",
        "try:\n",
        "    from pyzipcode import ZipCodeDatabase\n",
        "except ImportError:\n",
        "    print(\"Installing pyzipcode...\")\n",
        "    !pip install pyzipcode -q\n",
        "    from pyzipcode import ZipCodeDatabase\n",
        "\n",
        "print(\"\\n✓ Setup complete. All libraries loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeUGEDgNpt_a",
        "outputId": "b65742c0-fff8-4876-b5a6-327b46786c5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Optuna...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling pyzipcode...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyzipcode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "✓ Setup complete. All libraries loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data loading\n"
      ],
      "metadata": {
        "id": "KC0NdoqFqvBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper ---\n",
        "zcdb = ZipCodeDatabase()\n",
        "def get_state(zip_code):\n",
        "    \"\"\"Converts a zip code to a 2-letter state, handling errors.\"\"\"\n",
        "    try:\n",
        "        if pd.isna(zip_code):\n",
        "            return 'Unknown'\n",
        "        zip_code_str = str(int(float(zip_code))).zfill(5)\n",
        "        return zcdb[zip_code_str].state\n",
        "    except (ValueError, KeyError, AttributeError, TypeError):\n",
        "        return 'Unknown'\n",
        "\n",
        "print(\"✓ State feature helper created.\")\n",
        "\n",
        "# --- Load Data ---\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    try:\n",
        "        train_df = pd.read_csv('Training_TriGuard.csv')\n",
        "        test_df = pd.read_csv('Testing_TriGuard.csv')\n",
        "        print(\"✓ Files loaded from local environment.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Please upload Training_TriGuard.csv:\")\n",
        "        uploaded_train = files.upload()\n",
        "        train_file = list(uploaded_train.keys())[0]\n",
        "        train_df = pd.read_csv(train_file)\n",
        "\n",
        "        print(\"\\nPlease upload Testing_TriGuard.csv:\")\n",
        "        uploaded_test = files.upload()\n",
        "        test_file = list(uploaded_test.keys())[0]\n",
        "        test_df = pd.read_csv(test_file)\n",
        "        print(\"✓ Files uploaded successfully.\")\n",
        "else:\n",
        "    # Local environment\n",
        "    possible_paths = [\n",
        "        'Training_TriGuard.csv',\n",
        "        'data/Training_TriGuard.csv',\n",
        "        '../Training_TriGuard.csv',\n",
        "        './Training_TriGuard.csv'\n",
        "    ]\n",
        "\n",
        "    train_path = None\n",
        "    test_path = None\n",
        "\n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            train_path = path\n",
        "            test_path = path.replace('Training', 'Testing')\n",
        "            if os.path.exists(test_path):\n",
        "                break\n",
        "\n",
        "    if train_path and test_path:\n",
        "        train_df = pd.read_csv(train_path)\n",
        "        test_df = pd.read_csv(test_path)\n",
        "        print(f\"✓ Files loaded from: {train_path} and {test_path}\")\n",
        "    else:\n",
        "        train_path = input(\"Enter path to Training_TriGuard.csv: \").strip()\n",
        "        test_path = input(\"Enter path to Testing_TriGuard.csv: \").strip()\n",
        "        train_df = pd.read_csv(train_path)\n",
        "        test_df = pd.read_csv(test_path)\n",
        "        print(\"✓ Files loaded successfully.\")\n",
        "\n",
        "# --- Critical Cleaning ---\n",
        "initial_train_count = len(train_df)\n",
        "train_df = train_df.dropna(subset=['subrogation'])\n",
        "print(f\"\\nCleaned training data: Removed {initial_train_count - len(train_df)} rows with NaN target.\")\n",
        "\n",
        "train_df['subrogation'] = train_df['subrogation'].astype(int)\n",
        "\n",
        "print(f\"\\n✓ Train shape: {train_df.shape}\")\n",
        "print(f\"✓ Test shape: {test_df.shape}\")\n",
        "print(f\"\\nTarget distribution (after cleaning):\")\n",
        "print(train_df['subrogation'].value_counts(normalize=True).to_string())\n",
        "\n",
        "test_ids = test_df['claim_number'].copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "87vQef5vq4sW",
        "outputId": "7b5a9247-fd51-4d6e-9bb5-f312bf11f98d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ State feature helper created.\n",
            "Please upload Training_TriGuard.csv:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-20f5c4f0-e0fa-44de-995f-88eac58d08ea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-20f5c4f0-e0fa-44de-995f-88eac58d08ea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Training_TriGuard.csv to Training_TriGuard.csv\n",
            "\n",
            "Please upload Testing_TriGuard.csv:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c0502b07-bdba-48c3-a773-ab9504d9396c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c0502b07-bdba-48c3-a773-ab9504d9396c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Testing_TriGuard.csv to Testing_TriGuard.csv\n",
            "✓ Files uploaded successfully.\n",
            "\n",
            "Cleaned training data: Removed 2 rows with NaN target.\n",
            "\n",
            "✓ Train shape: (17999, 29)\n",
            "✓ Test shape: (12000, 28)\n",
            "\n",
            "Target distribution (after cleaning):\n",
            "subrogation\n",
            "0   0.771\n",
            "1   0.229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature engineering\n"
      ],
      "metadata": {
        "id": "sA9j7FNKrEal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineer(df):\n",
        "    \"\"\"Feature engineering WITHOUT vehicle_made_year/vehicle_age (data quality issues)\"\"\"\n",
        "    df_fe = df.copy()\n",
        "\n",
        "    # ========================================================================\n",
        "    # TEMPORAL FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['claim_date'] = pd.to_datetime(df_fe['claim_date'], errors='coerce')\n",
        "    df_fe['claim_year'] = df_fe['claim_date'].dt.year\n",
        "    df_fe['claim_month'] = df_fe['claim_date'].dt.month\n",
        "    df_fe['claim_day'] = df_fe['claim_date'].dt.day\n",
        "    df_fe['claim_quarter'] = df_fe['claim_date'].dt.quarter\n",
        "    df_fe['claim_dayofweek'] = df_fe['claim_date'].dt.dayofweek\n",
        "    df_fe['is_weekend'] = (df_fe['claim_dayofweek'] >= 5).astype(int)\n",
        "    df_fe['is_monday'] = (df_fe['claim_dayofweek'] == 0).astype(int)\n",
        "    df_fe['is_friday'] = (df_fe['claim_dayofweek'] == 4).astype(int)\n",
        "    df_fe['is_q4'] = (df_fe['claim_quarter'] == 4).astype(int)\n",
        "\n",
        "    season_map = {\n",
        "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
        "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
        "        9: 'Fall', 10: 'Fall', 11: 'Fall',\n",
        "        12: 'Winter', 1: 'Winter', 2: 'Winter'\n",
        "    }\n",
        "    df_fe['season'] = df_fe['claim_month'].map(season_map).fillna('Unknown')\n",
        "\n",
        "    # ========================================================================\n",
        "    # DATA CLEANING\n",
        "    # ========================================================================\n",
        "    df_fe.loc[(df_fe['year_of_born'] < 1900) | (df_fe['year_of_born'] > 2025), 'year_of_born'] = np.nan\n",
        "\n",
        "    # ========================================================================\n",
        "    # BINARY CONVERSIONS (for interactions)\n",
        "    # ========================================================================\n",
        "    df_fe['witness_binary'] = (df_fe['witness_present_ind'] == 'Y').astype(int)\n",
        "    df_fe['police_binary'] = df_fe['policy_report_filed_ind']\n",
        "    df_fe['multicar_binary'] = df_fe['accident_type'].isin(['multi_vehicle_clear', 'multi_vehicle_unclear']).astype(int)\n",
        "    df_fe['highrisk_site_binary'] = df_fe['accident_site'].isin(['Highway/Intersection', 'Local']).astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # CRITICAL INTERACTION FEATURES (2-way)\n",
        "    # ========================================================================\n",
        "    df_fe['liab_x_witness'] = df_fe['liab_prct'] * df_fe['witness_binary']\n",
        "    df_fe['liab_x_police'] = df_fe['liab_prct'] * df_fe['police_binary']\n",
        "    df_fe['liab_x_multicar'] = df_fe['liab_prct'] * df_fe['multicar_binary']\n",
        "    df_fe['liab_x_highrisk_site'] = df_fe['liab_prct'] * df_fe['highrisk_site_binary']\n",
        "    df_fe['liab_x_evidence'] = df_fe['liab_prct'] * (df_fe['witness_binary'] + df_fe['police_binary'])\n",
        "    df_fe['liab_x_payout'] = df_fe['liab_prct'] * df_fe['claim_est_payout']\n",
        "    df_fe['liab_x_mileage'] = df_fe['liab_prct'] * df_fe['vehicle_mileage']\n",
        "\n",
        "    df_fe['witness_x_police'] = df_fe['witness_binary'] * df_fe['police_binary']\n",
        "    df_fe['witness_x_multicar'] = df_fe['witness_binary'] * df_fe['multicar_binary']\n",
        "    df_fe['police_x_multicar'] = df_fe['police_binary'] * df_fe['multicar_binary']\n",
        "    df_fe['multicar_x_highrisk'] = df_fe['multicar_binary'] * df_fe['highrisk_site_binary']\n",
        "    df_fe['weekend_highway'] = (df_fe['claim_dayofweek'] >= 5).astype(int) * (df_fe['accident_site'] == 'Highway/Intersection').astype(int)\n",
        "\n",
        "    # 3-way interaction\n",
        "    df_fe['witness_police_multicar'] = df_fe['witness_binary'] * df_fe['police_binary'] * df_fe['multicar_binary']\n",
        "\n",
        "    # ========================================================================\n",
        "    # POLYNOMIAL FEATURES (liability & key variables)\n",
        "    # ========================================================================\n",
        "    df_fe['liab_prct_squared'] = df_fe['liab_prct'] ** 2\n",
        "    df_fe['liab_prct_cubed'] = df_fe['liab_prct'] ** 3\n",
        "    df_fe['liab_prct_sqrt'] = np.sqrt(df_fe['liab_prct'])\n",
        "    df_fe['liab_prct_log'] = np.log1p(df_fe['liab_prct'])\n",
        "    df_fe['liab_inverse'] = 100 - df_fe['liab_prct']\n",
        "    df_fe['liab_inverse_squared'] = (100 - df_fe['liab_prct']) ** 2\n",
        "\n",
        "    df_fe['log_claim_est_payout'] = np.log1p(df_fe['claim_est_payout'])\n",
        "    df_fe['log_vehicle_mileage'] = np.log1p(df_fe['vehicle_mileage'])\n",
        "    df_fe['log_vehicle_price'] = np.log1p(df_fe['vehicle_price'])\n",
        "    df_fe['log_annual_income'] = np.log1p(df_fe['annual_income'])\n",
        "    df_fe['sqrt_vehicle_mileage'] = np.sqrt(df_fe['vehicle_mileage'])\n",
        "\n",
        "    # ========================================================================\n",
        "    # ACCIDENT TYPE FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['is_multi_vehicle_clear'] = (df_fe['accident_type'] == 'multi_vehicle_clear').astype(int)\n",
        "    df_fe['is_multi_vehicle_unclear'] = (df_fe['accident_type'] == 'multi_vehicle_unclear').astype(int)\n",
        "    df_fe['is_single_car'] = (df_fe['accident_type'] == 'single_car').astype(int)\n",
        "    df_fe['has_recovery_target'] = df_fe['multicar_binary']\n",
        "\n",
        "    df_fe['recovery_case_clarity'] = 0\n",
        "    df_fe.loc[df_fe['is_multi_vehicle_clear'] == 1, 'recovery_case_clarity'] = 3\n",
        "    df_fe.loc[df_fe['is_multi_vehicle_unclear'] == 1, 'recovery_case_clarity'] = 1\n",
        "\n",
        "    # ========================================================================\n",
        "    # LIABILITY BUCKETS (fine-grained)\n",
        "    # ========================================================================\n",
        "    df_fe['liab_under_10'] = (df_fe['liab_prct'] < 10).astype(int)\n",
        "    df_fe['liab_10_to_15'] = ((df_fe['liab_prct'] >= 10) & (df_fe['liab_prct'] < 15)).astype(int)\n",
        "    df_fe['liab_15_to_20'] = ((df_fe['liab_prct'] >= 15) & (df_fe['liab_prct'] < 20)).astype(int)\n",
        "    df_fe['liab_20_to_25'] = ((df_fe['liab_prct'] >= 20) & (df_fe['liab_prct'] < 25)).astype(int)\n",
        "    df_fe['liab_25_to_30'] = ((df_fe['liab_prct'] >= 25) & (df_fe['liab_prct'] < 30)).astype(int)\n",
        "    df_fe['liab_30_to_35'] = ((df_fe['liab_prct'] >= 30) & (df_fe['liab_prct'] < 35)).astype(int)\n",
        "    df_fe['liab_35_to_40'] = ((df_fe['liab_prct'] >= 35) & (df_fe['liab_prct'] < 40)).astype(int)\n",
        "    df_fe['liab_40_to_50'] = ((df_fe['liab_prct'] >= 40) & (df_fe['liab_prct'] < 50)).astype(int)\n",
        "    df_fe['liab_over_50'] = (df_fe['liab_prct'] >= 50).astype(int)\n",
        "\n",
        "    df_fe['not_at_fault'] = df_fe['liab_under_10']\n",
        "    df_fe['minimal_fault'] = (df_fe['liab_prct'] < 25).astype(int)\n",
        "    df_fe['low_fault'] = (df_fe['liab_prct'] < 35).astype(int)\n",
        "    df_fe['shared_fault'] = ((df_fe['liab_prct'] >= 35) & (df_fe['liab_prct'] < 50)).astype(int)\n",
        "    df_fe['high_fault'] = (df_fe['liab_prct'] >= 50).astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # EVIDENCE QUALITY FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['witness_present'] = df_fe['witness_binary']\n",
        "    df_fe['police_report'] = df_fe['police_binary']\n",
        "\n",
        "    df_fe['evidence_none'] = ((df_fe['witness_present'] == 0) & (df_fe['police_report'] == 0)).astype(int)\n",
        "    df_fe['evidence_weak'] = (((df_fe['witness_present'] == 1) & (df_fe['police_report'] == 0)) |\n",
        "                              ((df_fe['witness_present'] == 0) & (df_fe['police_report'] == 1))).astype(int)\n",
        "    df_fe['evidence_strong'] = ((df_fe['witness_present'] == 1) & (df_fe['police_report'] == 1)).astype(int)\n",
        "    df_fe['evidence_very_strong'] = ((df_fe['witness_present'] == 1) & (df_fe['police_report'] == 1) &\n",
        "                                      (df_fe['liab_prct'] < 20)).astype(int)\n",
        "    df_fe['evidence_score'] = df_fe['witness_present'] + df_fe['police_report']\n",
        "\n",
        "    # ========================================================================\n",
        "    # ACCIDENT SITE FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['high_risk_site'] = df_fe['highrisk_site_binary']\n",
        "    df_fe['parking_accident'] = (df_fe['accident_site'] == 'Parking Area').astype(int)\n",
        "    df_fe['unknown_site'] = (df_fe['accident_site'] == 'Unknown').astype(int)\n",
        "    df_fe['highway_accident'] = (df_fe['accident_site'] == 'Highway/Intersection').astype(int)\n",
        "    df_fe['local_accident'] = (df_fe['accident_site'] == 'Local').astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # DRIVER AGE & EXPERIENCE\n",
        "    # ========================================================================\n",
        "    df_fe['driver_age'] = df_fe['claim_year'] - df_fe['year_of_born']\n",
        "    df_fe.loc[(df_fe['driver_age'] < 16) | (df_fe['driver_age'] > 100), 'driver_age'] = np.nan\n",
        "\n",
        "    df_fe['young_driver'] = ((df_fe['driver_age'] >= 16) & (df_fe['driver_age'] <= 25)).astype(int)\n",
        "    df_fe['prime_driver'] = ((df_fe['driver_age'] > 25) & (df_fe['driver_age'] <= 45)).astype(int)\n",
        "    df_fe['middle_age_driver'] = ((df_fe['driver_age'] > 45) & (df_fe['driver_age'] <= 65)).astype(int)\n",
        "    df_fe['senior_driver'] = (df_fe['driver_age'] > 65).astype(int)\n",
        "\n",
        "    df_fe['driving_experience'] = (df_fe['driver_age'] - df_fe['age_of_DL']).clip(lower=0)\n",
        "    df_fe.loc[df_fe['driving_experience'] < 0, 'driving_experience'] = np.nan\n",
        "\n",
        "    df_fe['novice_driver'] = (df_fe['driving_experience'] < 3).astype(int)\n",
        "    df_fe['experienced_driver'] = ((df_fe['driving_experience'] >= 3) & (df_fe['driving_experience'] <= 10)).astype(int)\n",
        "    df_fe['veteran_driver'] = (df_fe['driving_experience'] > 10).astype(int)\n",
        "\n",
        "    df_fe['experience_x_safety'] = df_fe['driving_experience'] * df_fe['safety_rating']\n",
        "    df_fe['driver_age_x_safety'] = df_fe['driver_age'] * df_fe['safety_rating']\n",
        "\n",
        "    # ========================================================================\n",
        "    # VEHICLE FEATURES (without vehicle_age)\n",
        "    # ========================================================================\n",
        "    df_fe['luxury_vehicle'] = (df_fe['vehicle_price'] > 50000).astype(int)\n",
        "    df_fe['mid_price_vehicle'] = ((df_fe['vehicle_price'] >= 20000) & (df_fe['vehicle_price'] <= 50000)).astype(int)\n",
        "    df_fe['economy_vehicle'] = (df_fe['vehicle_price'] < 20000).astype(int)\n",
        "\n",
        "    df_fe['heavy_vehicle'] = (df_fe['vehicle_weight'] > 30000).astype(int)\n",
        "    df_fe['light_vehicle'] = (df_fe['vehicle_weight'] < 15000).astype(int)\n",
        "    df_fe['medium_weight'] = ((df_fe['vehicle_weight'] >= 15000) & (df_fe['vehicle_weight'] <= 30000)).astype(int)\n",
        "\n",
        "    df_fe['is_large_vehicle'] = (df_fe['vehicle_category'] == 'Large').astype(int)\n",
        "    df_fe['is_compact_vehicle'] = (df_fe['vehicle_category'] == 'Compact').astype(int)\n",
        "    df_fe['is_medium_vehicle'] = (df_fe['vehicle_category'] == 'Medium').astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # CLAIM CHARACTERISTICS\n",
        "    # ========================================================================\n",
        "    df_fe['high_mileage'] = (df_fe['vehicle_mileage'] > 100000).astype(int)\n",
        "    df_fe['low_mileage'] = (df_fe['vehicle_mileage'] < 50000).astype(int)\n",
        "    df_fe['very_high_mileage'] = (df_fe['vehicle_mileage'] > 150000).astype(int)\n",
        "    df_fe['medium_mileage'] = ((df_fe['vehicle_mileage'] >= 50000) & (df_fe['vehicle_mileage'] <= 100000)).astype(int)\n",
        "\n",
        "    df_fe['frequent_claimer'] = (df_fe['past_num_of_claims'] > 5).astype(int)\n",
        "    df_fe['moderate_claimer'] = ((df_fe['past_num_of_claims'] >= 1) & (df_fe['past_num_of_claims'] <= 5)).astype(int)\n",
        "    df_fe['first_time_claimer'] = (df_fe['past_num_of_claims'] == 0).astype(int)\n",
        "    df_fe['very_frequent_claimer'] = (df_fe['past_num_of_claims'] > 10).astype(int)\n",
        "\n",
        "    df_fe['large_payout'] = (df_fe['claim_est_payout'] > 5000).astype(int)\n",
        "    df_fe['medium_payout'] = ((df_fe['claim_est_payout'] >= 2000) & (df_fe['claim_est_payout'] <= 5000)).astype(int)\n",
        "    df_fe['small_payout'] = (df_fe['claim_est_payout'] < 2000).astype(int)\n",
        "    df_fe['very_large_payout'] = (df_fe['claim_est_payout'] > 8000).astype(int)\n",
        "\n",
        "    df_fe['safety_x_prior_claims'] = df_fe['safety_rating'] / (1 + df_fe['past_num_of_claims'])\n",
        "    df_fe['mileage_x_claims'] = df_fe['vehicle_mileage'] * df_fe['past_num_of_claims']\n",
        "\n",
        "    # ========================================================================\n",
        "    # RATIO FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['payout_to_price_ratio'] = df_fe['claim_est_payout'] / (df_fe['vehicle_price'] + 1)\n",
        "    df_fe['severe_damage'] = (df_fe['payout_to_price_ratio'] > 0.3).astype(int)\n",
        "    df_fe['moderate_damage'] = ((df_fe['payout_to_price_ratio'] >= 0.1) & (df_fe['payout_to_price_ratio'] <= 0.3)).astype(int)\n",
        "    df_fe['minor_damage'] = (df_fe['payout_to_price_ratio'] < 0.1).astype(int)\n",
        "\n",
        "    df_fe['income_to_vehicle_price'] = df_fe['annual_income'] / (df_fe['vehicle_price'] + 1)\n",
        "    df_fe['can_afford_vehicle'] = (df_fe['income_to_vehicle_price'] >= 0.5).astype(int)\n",
        "    df_fe['expensive_for_income'] = (df_fe['income_to_vehicle_price'] < 0.3).astype(int)\n",
        "\n",
        "    df_fe['claims_per_year_driving'] = df_fe['past_num_of_claims'] / (df_fe['driving_experience'] + 1)\n",
        "    df_fe['claim_frequency_high'] = (df_fe['claims_per_year_driving'] > 0.5).astype(int)\n",
        "\n",
        "    df_fe['safety_to_liability'] = df_fe['safety_rating'] / (df_fe['liab_prct'] + 1)\n",
        "    df_fe['payout_to_income'] = df_fe['claim_est_payout'] / (df_fe['annual_income'] + 1)\n",
        "    df_fe['mileage_to_price'] = df_fe['vehicle_mileage'] / (df_fe['vehicle_price'] + 1)\n",
        "    df_fe['weight_to_price'] = df_fe['vehicle_weight'] / (df_fe['vehicle_price'] + 1)\n",
        "\n",
        "    # ========================================================================\n",
        "    # POLICYHOLDER CHARACTERISTICS\n",
        "    # ========================================================================\n",
        "    df_fe['high_income'] = (df_fe['annual_income'] > 70000).astype(int)\n",
        "    df_fe['mid_income'] = ((df_fe['annual_income'] >= 40000) & (df_fe['annual_income'] <= 70000)).astype(int)\n",
        "    df_fe['low_income'] = (df_fe['annual_income'] < 40000).astype(int)\n",
        "    df_fe['very_high_income'] = (df_fe['annual_income'] > 100000).astype(int)\n",
        "\n",
        "    df_fe['high_safety_rating'] = (df_fe['safety_rating'] > 80).astype(int)\n",
        "    df_fe['low_safety_rating'] = (df_fe['safety_rating'] < 60).astype(int)\n",
        "    df_fe['very_high_safety'] = (df_fe['safety_rating'] > 90).astype(int)\n",
        "    df_fe['medium_safety'] = ((df_fe['safety_rating'] >= 60) & (df_fe['safety_rating'] <= 80)).astype(int)\n",
        "\n",
        "    df_fe['contact_available'] = df_fe['email_or_tel_available']\n",
        "    df_fe['has_education'] = df_fe['high_education_ind']\n",
        "    df_fe['recent_move'] = df_fe['address_change_ind']\n",
        "    df_fe['home_owner'] = (df_fe['living_status'] == 'Own').astype(int)\n",
        "    df_fe['renter'] = (df_fe['living_status'] == 'Rent').astype(int)\n",
        "    df_fe['female'] = (df_fe['gender'] == 'F').astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # CHANNEL FEATURES\n",
        "    # ========================================================================\n",
        "    df_fe['via_broker'] = (df_fe['channel'] == 'Broker').astype(int)\n",
        "    df_fe['via_online'] = (df_fe['channel'] == 'Online').astype(int)\n",
        "    df_fe['via_phone'] = (df_fe['channel'] == 'Phone').astype(int)\n",
        "    df_fe['in_network_repair'] = (df_fe['in_network_bodyshop'] == 'yes').astype(int)\n",
        "    df_fe['out_network_repair'] = (df_fe['in_network_bodyshop'] == 'no').astype(int)\n",
        "\n",
        "    # ========================================================================\n",
        "    # COMPOSITE RECOVERY SCORE\n",
        "    # ========================================================================\n",
        "    liability_score = np.sqrt((100 - df_fe['liab_prct']) / 100.0)\n",
        "    evidence_score = (df_fe['evidence_none'] * 0.0 + df_fe['evidence_weak'] * 0.4 +\n",
        "                      df_fe['evidence_strong'] * 0.7 + df_fe['evidence_very_strong'] * 1.0)\n",
        "    clarity_score = df_fe['recovery_case_clarity'] / 3.0\n",
        "    site_score = df_fe['high_risk_site'] * 0.7 + (1 - df_fe['unknown_site']) * 0.3\n",
        "\n",
        "    df_fe['recovery_feasibility_score'] = (0.35 * liability_score + 0.30 * df_fe['has_recovery_target'] +\n",
        "                                           0.20 * evidence_score + 0.10 * clarity_score + 0.05 * site_score)\n",
        "\n",
        "    # ========================================================================\n",
        "    # DOMAIN LOGIC FLAGS (CRITICAL FOR F1)\n",
        "    # ========================================================================\n",
        "    df_fe['perfect_case'] = ((df_fe['liab_prct'] < 15) & (df_fe['witness_present'] == 1) &\n",
        "                             (df_fe['police_report'] == 1) & (df_fe['has_recovery_target'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['strong_case'] = ((df_fe['liab_prct'] < 25) & (df_fe['evidence_strong'] == 1) &\n",
        "                            (df_fe['has_recovery_target'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['good_case'] = ((df_fe['liab_prct'] < 35) & (df_fe['evidence_score'] >= 1) &\n",
        "                          (df_fe['has_recovery_target'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['weak_case'] = ((df_fe['liab_prct'] > 40) | (df_fe['is_single_car'] == 1) |\n",
        "                          (df_fe['evidence_none'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['no_case'] = ((df_fe['liab_prct'] > 60) | ((df_fe['is_single_car'] == 1) & (df_fe['evidence_none'] == 1))).astype(int)\n",
        "\n",
        "    df_fe['high_value_opportunity'] = ((df_fe['claim_est_payout'] > 3000) & (df_fe['liab_prct'] < 30) &\n",
        "                                       (df_fe['has_recovery_target'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['slam_dunk_case'] = ((df_fe['liab_prct'] < 10) & (df_fe['witness_present'] == 1) &\n",
        "                               (df_fe['police_report'] == 1) & (df_fe['multicar_binary'] == 1) &\n",
        "                               (df_fe['high_risk_site'] == 1)).astype(int)\n",
        "\n",
        "    df_fe['low_liab_high_payout'] = ((df_fe['liab_prct'] < 20) & (df_fe['claim_est_payout'] > 5000)).astype(int)\n",
        "    df_fe['clear_fault_case'] = ((df_fe['liab_prct'] < 15) & (df_fe['multicar_binary'] == 1)).astype(int)\n",
        "    df_fe['high_mileage_low_fault'] = ((df_fe['vehicle_mileage'] > 100000) & (df_fe['liab_prct'] < 30)).astype(int)\n",
        "\n",
        "    # --- Temporal & Behavior Dynamics ---\n",
        "    df_fe['claim_early_in_year'] = (df_fe['claim_month'] <= 3).astype(int)\n",
        "    df_fe['claim_end_of_year'] = (df_fe['claim_month'] >= 10).astype(int)\n",
        "    df_fe['weekend_parking'] = df_fe['is_weekend'] * (df_fe['accident_site'] == 'Parking Area').astype(int)\n",
        "    df_fe['winter_claim_high_payout'] = ((df_fe['season'] == 'Winter') & (df_fe['claim_est_payout'] > 5000)).astype(int)\n",
        "\n",
        "    # --- Vehicle Utilization Proxies (without vehicle_age) ---\n",
        "    df_fe['mileage_x_weight'] = df_fe['vehicle_mileage'] * df_fe['vehicle_weight']\n",
        "    df_fe['mileage_per_dollar'] = df_fe['vehicle_mileage'] / (df_fe['vehicle_price'] + 1)\n",
        "    df_fe['payout_to_weight'] = df_fe['claim_est_payout'] / (df_fe['vehicle_weight'] + 1)\n",
        "\n",
        "    # --- Policyholder Risk Profile ---\n",
        "    df_fe['unstable_policyholder'] = ((df_fe['recent_move'] == 1) & (df_fe['renter'] == 1)).astype(int)\n",
        "    df_fe['financial_stress_risk'] = ((df_fe['expensive_for_income'] == 1) & (df_fe['large_payout'] == 1)).astype(int)\n",
        "    df_fe['young_driver_highway'] = df_fe['young_driver'] * df_fe['highway_accident']\n",
        "    df_fe['senior_driver_parking'] = df_fe['senior_driver'] * df_fe['parking_accident']\n",
        "\n",
        "    # --- Liability & Evidence Interaction Insights ---\n",
        "    df_fe['low_liab_weak_evidence'] = ((df_fe['liab_prct'] < 20) & (df_fe['evidence_weak'] == 1)).astype(int)\n",
        "    df_fe['high_liab_strong_evidence'] = ((df_fe['liab_prct'] > 50) & (df_fe['evidence_strong'] == 1)).astype(int)\n",
        "\n",
        "    # Composite confidence / case quality index\n",
        "    df_fe['case_confidence_score'] = (\n",
        "        0.4 * (100 - df_fe['liab_prct']) / 100 +\n",
        "        0.4 * df_fe['evidence_score'] / 2 +\n",
        "        0.2 * df_fe['recovery_case_clarity'] / 3\n",
        "    )\n",
        "\n",
        "    # --- Statistical Normalization & Percentile Features ---\n",
        "    for col in ['claim_est_payout', 'vehicle_mileage', 'annual_income']:\n",
        "        df_fe[f'{col}_z'] = (df_fe[col] - df_fe[col].mean()) / (df_fe[col].std() + 1e-9)\n",
        "\n",
        "    try:\n",
        "        df_fe['liab_percentile'] = pd.qcut(df_fe['liab_prct'], 10, labels=False, duplicates='drop')\n",
        "        df_fe['payout_percentile'] = pd.qcut(df_fe['claim_est_payout'], 10, labels=False, duplicates='drop')\n",
        "    except Exception:\n",
        "        # If there aren't enough unique values to bin\n",
        "        df_fe['liab_percentile'] = np.nan\n",
        "        df_fe['payout_percentile'] = np.nan\n",
        "\n",
        "    # --- Aggregate / Hybrid Indices ---\n",
        "    df_fe['case_strength_index'] = df_fe['evidence_score'] * (1 - df_fe['liab_prct'] / 100)\n",
        "    df_fe['financial_exposure_index'] = (\n",
        "        (df_fe['claim_est_payout'] / (df_fe['annual_income'] + 1)) * (1 + df_fe['liab_prct'] / 100)\n",
        "    )\n",
        "    df_fe['behavioral_risk_index'] = (\n",
        "        df_fe['claims_per_year_driving'] * (100 - df_fe['safety_rating']) / 100\n",
        "    )\n",
        "\n",
        "    return df_fe\n",
        "\n",
        "print(\"✓ Feature engineering function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGix2VMUrOME",
        "outputId": "ac44a71e-cf80-4eff-eb19-8e03021cb98a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Feature engineering function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pre-modeling with target encoding\n"
      ],
      "metadata": {
        "id": "tU8YI01crqp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Running Feature Engineering on train and test data...\")\n",
        "\n",
        "train_fe = feature_engineer(train_df)\n",
        "test_fe = feature_engineer(test_df)\n",
        "print(\"✓ Feature engineering complete.\")\n",
        "\n",
        "# Define Categorical Feature Lists - FIXED\n",
        "features_to_target_encode = [\n",
        "    'gender', 'living_status', 'accident_site',\n",
        "    'channel', 'vehicle_category', 'vehicle_color', 'accident_type',\n",
        "    'in_network_bodyshop', 'season', 'zip_code'  # Removed 'state' and 'claim_day_of_week'\n",
        "]\n",
        "\n",
        "# Apply Target Encoding\n",
        "print(f\"\\nApplying Smoothed Target Encoding to {len(features_to_target_encode)} features...\")\n",
        "global_mean = train_fe['subrogation'].mean()\n",
        "categorical_features_for_lgbm = []\n",
        "\n",
        "for col in features_to_target_encode:\n",
        "    target_mean = train_fe.groupby(col)['subrogation'].mean()\n",
        "    category_counts = train_fe.groupby(col).size()\n",
        "    smoothing = 20\n",
        "\n",
        "    smoothed_mean = (target_mean * category_counts + global_mean * smoothing) / (category_counts + smoothing)\n",
        "\n",
        "    new_col_name = f'{col}_target_enc'\n",
        "    train_fe[new_col_name] = train_fe[col].map(smoothed_mean)\n",
        "    test_fe[new_col_name] = test_fe[col].map(smoothed_mean)\n",
        "\n",
        "    test_fe[new_col_name] = test_fe[new_col_name].fillna(global_mean)\n",
        "\n",
        "    categorical_features_for_lgbm.append(new_col_name)\n",
        "\n",
        "print(\"✓ Target encoding complete.\")\n",
        "\n",
        "# Create Final X, y, and X_test\n",
        "y_all = train_fe['subrogation'].copy()\n",
        "\n",
        "drop_cols = [\n",
        "    'subrogation', 'claim_number', 'claim_date', 'year_of_born',\n",
        "    'witness_present_ind', 'policy_report_filed_ind',\n",
        "    'vehicle_made_year'  # Bad data quality\n",
        "]\n",
        "drop_cols.extend(features_to_target_encode)\n",
        "\n",
        "feature_cols = [col for col in train_fe.columns if col not in drop_cols]\n",
        "X_all = train_fe[feature_cols].copy()\n",
        "X_test_all = test_fe[feature_cols].copy()\n",
        "\n",
        "# Apply Label Encoding (if any object columns remain)\n",
        "other_cat_cols = X_all.select_dtypes(include='object').columns.tolist()\n",
        "if other_cat_cols:\n",
        "    print(f\"\\nApplying Label Encoding to {len(other_cat_cols)} remaining features...\")\n",
        "    for col in other_cat_cols:\n",
        "        le = LabelEncoder()\n",
        "        all_values = pd.concat([X_all[col].astype(str), X_test_all[col].astype(str)]).unique()\n",
        "        le.fit(all_values)\n",
        "        X_all[col] = le.transform(X_all[col].astype(str))\n",
        "        X_test_all[col] = le.transform(X_test_all[col].astype(str))\n",
        "    print(\"✓ Label encoding complete.\")\n",
        "\n",
        "# Impute NaN values with median\n",
        "print(\"\\nImputing NaN values with the median from the training data...\")\n",
        "X_all_median = X_all.median()\n",
        "X_all = X_all.fillna(X_all_median)\n",
        "X_test_all = X_test_all.fillna(X_all_median)\n",
        "print(\"✓ NaN values imputed.\")\n",
        "\n",
        "# Calculate scale_pos_weight\n",
        "scale_pos_weight = (y_all == 0).sum() / (y_all == 1).sum()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRE-MODELING COMPLETE\")\n",
        "print(f\"✓ X_all shape: {X_all.shape}\")\n",
        "print(f\"✓ y_all shape: {y_all.shape}\")\n",
        "print(f\"✓ X_test_all shape: {X_test_all.shape}\")\n",
        "print(f\"✓ Total features: {len(feature_cols)}\")\n",
        "print(f\"✓ scale_pos_weight (for F1 score): {scale_pos_weight:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84kTpL7TrtRZ",
        "outputId": "df7137f9-1902-4ead-d9c9-b2fdf3ffc7b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Running Feature Engineering on train and test data...\n",
            "✓ Feature engineering complete.\n",
            "\n",
            "Applying Smoothed Target Encoding to 10 features...\n",
            "✓ Target encoding complete.\n",
            "\n",
            "Applying Label Encoding to 1 remaining features...\n",
            "✓ Label encoding complete.\n",
            "\n",
            "Imputing NaN values with the median from the training data...\n",
            "✓ NaN values imputed.\n",
            "\n",
            "================================================================================\n",
            "PRE-MODELING COMPLETE\n",
            "✓ X_all shape: (17999, 190)\n",
            "✓ y_all shape: (17999,)\n",
            "✓ X_test_all shape: (12000, 190)\n",
            "✓ Total features: 190\n",
            "✓ scale_pos_weight (for F1 score): 3.3740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training lightGBM model\n",
        "\n"
      ],
      "metadata": {
        "id": "ylBTKRGnsYTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 1: OPTUNA HYPERPARAMETER OPTIMIZATION (70/30 SPLIT)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create a stable 70/30 validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_all, y_all,\n",
        "    test_size=0.30,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_all\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain: {X_train.shape}, Validation: {X_val.shape}\")\n",
        "print(f\"Train target distribution: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Val target distribution: {y_val.value_counts().to_dict()}\")\n",
        "\n",
        "# Define objective function with THRESHOLD OPTIMIZATION\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
        "        'max_depth': trial.suggest_int('max_depth', 4, 20),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
        "        'scale_pos_weight': scale_pos_weight,\n",
        "        'random_state': RANDOM_STATE,\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    pred_proba = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    # Test multiple thresholds to find optimal F1\n",
        "    best_f1 = 0\n",
        "    for thresh in np.arange(0.25, 0.76, 0.02):\n",
        "        preds = (pred_proba >= thresh).astype(int)\n",
        "        f1 = f1_score(y_val, preds)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "\n",
        "    return best_f1\n",
        "\n",
        "# Run optimization\n",
        "print(\"\\nRunning Optuna optimization (100 trials)...\")\n",
        "print(\"This will test different parameter combinations with threshold optimization...\\n\")\n",
        "\n",
        "study = optuna.create_study(direction='maximize', study_name='lgb_f1_optimization')\n",
        "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OPTUNA TUNING COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n🏆 Best F1 Score: {study.best_value:.4f}\")\n",
        "print(f\"\\n📊 Best Parameters:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "best_lgbm_params = study.best_params.copy()\n",
        "best_lgbm_params['scale_pos_weight'] = scale_pos_weight\n",
        "best_lgbm_params['random_state'] = RANDOM_STATE\n",
        "best_lgbm_params['verbose'] = -1\n",
        "\n",
        "print(\"\\n✓ Best parameters saved to best_lgbm_params\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585,
          "referenced_widgets": [
            "7b831fe2c72a4588911eb9aa1186eff3",
            "9a5cf8a25429483cad8baae279c80e83",
            "088eda24bd24479394140e12e6b07df7",
            "e16b04f06bfc462f9e411dfc2ed79adc",
            "ae664d04052b403cad7dce87f1ddbfaf",
            "94d9bcab9e924091ba69bfdab1f5e485",
            "02993748e828441e86a2e27b466a1955",
            "a896d2ddac164efc93ba5ac7085a5339",
            "f5b6fb46533f4529b73a94899b2064ff",
            "197b63ff137c4200bc8451d3c9a90e63",
            "1a27c89af3d44160ada107e9f7686531"
          ]
        },
        "id": "wY2Beo4lsdcS",
        "outputId": "22ce687d-9cc0-49b6-b6a9-80bca8e1e033"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 1: OPTUNA HYPERPARAMETER OPTIMIZATION (70/30 SPLIT)\n",
            "================================================================================\n",
            "\n",
            "Train: (12599, 190), Validation: (5400, 190)\n",
            "Train target distribution: {0: 9719, 1: 2880}\n",
            "Val target distribution: {0: 4165, 1: 1235}\n",
            "\n",
            "Running Optuna optimization (100 trials)...\n",
            "This will test different parameter combinations with threshold optimization...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b831fe2c72a4588911eb9aa1186eff3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "OPTUNA TUNING COMPLETE\n",
            "================================================================================\n",
            "\n",
            "🏆 Best F1 Score: 0.5782\n",
            "\n",
            "📊 Best Parameters:\n",
            "  n_estimators: 507\n",
            "  learning_rate: 0.011203676525861687\n",
            "  num_leaves: 23\n",
            "  max_depth: 7\n",
            "  min_child_samples: 80\n",
            "  subsample: 0.5609478583630461\n",
            "  colsample_bytree: 0.7751922173938319\n",
            "  reg_alpha: 1.601863804687629\n",
            "  reg_lambda: 1.3202000908833544\n",
            "\n",
            "✓ Best parameters saved to best_lgbm_params\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-fold cv"
      ],
      "metadata": {
        "id": "S4rxwdJ1XL52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*90)\n",
        "print(\"STEP 2: 5-FOLD CROSS-VALIDATION WITH BEST LIGHTGBM PARAMETERS\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    f1_score, roc_auc_score, average_precision_score,\n",
        "    precision_score, recall_score, precision_recall_curve\n",
        ")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(f\"\\nRunning 5-Fold Stratified Cross-Validation using tuned parameters...\")\n",
        "print(f\"Each fold finds its optimal threshold from PR curve to maximize F1\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# SETUP\n",
        "# ============================================================================\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "fold_results = []\n",
        "fold_thresholds = []\n",
        "fold_f1_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_all, y_all), 1):\n",
        "    print(f\"\\n{'-'*40}\")\n",
        "    print(f\"Fold {fold}/5\")\n",
        "    print(f\"{'-'*40}\")\n",
        "\n",
        "    X_train_fold, X_val_fold = X_all.iloc[train_idx], X_all.iloc[val_idx]\n",
        "    y_train_fold, y_val_fold = y_all.iloc[train_idx], y_all.iloc[val_idx]\n",
        "\n",
        "    # Train model on this fold\n",
        "    model_fold = lgb.LGBMClassifier(**best_lgbm_params)\n",
        "    model_fold.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Predicted probabilities\n",
        "    y_prob = model_fold.predict_proba(X_val_fold)[:, 1]\n",
        "\n",
        "    # ========================================================================\n",
        "    # OPTIMAL THRESHOLD VIA PRECISION-RECALL CURVE\n",
        "    # ========================================================================\n",
        "    precision, recall, thresholds = precision_recall_curve(y_val_fold, y_prob)\n",
        "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "    best_idx = np.nanargmax(f1_scores)\n",
        "    best_thresh = thresholds[max(0, best_idx - 1)]\n",
        "    best_f1 = f1_scores[best_idx]\n",
        "\n",
        "    fold_thresholds.append(best_thresh)\n",
        "\n",
        "    # Final predictions at optimal threshold\n",
        "    y_pred = (y_prob >= best_thresh).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    f1 = f1_score(y_val_fold, y_pred)\n",
        "    roc = roc_auc_score(y_val_fold, y_prob)\n",
        "    pr_auc = average_precision_score(y_val_fold, y_prob)\n",
        "    precision_val = precision_score(y_val_fold, y_pred)\n",
        "    recall_val = recall_score(y_val_fold, y_pred)\n",
        "    acc_val = (y_pred == y_val_fold).mean()\n",
        "\n",
        "    fold_f1_scores.append(f1)\n",
        "    fold_results.append({\n",
        "        'Fold': fold,\n",
        "        'F1': f1,\n",
        "        'ROC_AUC': roc,\n",
        "        'PR_AUC': pr_auc,\n",
        "        'Precision': precision_val,\n",
        "        'Recall': recall_val,\n",
        "        'Accuracy': acc_val,\n",
        "        'Optimal_Threshold': best_thresh\n",
        "    })\n",
        "\n",
        "    print(f\"  ✅ F1: {f1:.4f} | ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f}\")\n",
        "    print(f\"  Precision: {precision_val:.4f} | Recall: {recall_val:.4f} | Threshold: {best_thresh:.3f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# AGGREGATE RESULTS\n",
        "# ============================================================================\n",
        "results_df = pd.DataFrame(fold_results)\n",
        "mean_f1 = np.mean(fold_f1_scores)\n",
        "std_f1 = np.std(fold_f1_scores)\n",
        "avg_optimal_threshold = np.mean(fold_thresholds)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*90)\n",
        "print(\"CROSS-VALIDATION RESULTS SUMMARY\")\n",
        "print(\"=\"*90)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(f\"\\n\" + \"=\"*90)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\"*90)\n",
        "print(f\"Mean F1:        {mean_f1:.4f}\")\n",
        "print(f\"Std F1:         {std_f1:.4f}\")\n",
        "print(f\"95% CI (approx): [{mean_f1 - 1.96*std_f1:.4f}, {mean_f1 + 1.96*std_f1:.4f}]\")\n",
        "print(f\"\\n✅ Average Optimal Threshold from CV: {avg_optimal_threshold:.3f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL THRESHOLD & EXPECTED PERFORMANCE\n",
        "# ============================================================================\n",
        "final_threshold = avg_optimal_threshold\n",
        "print(f\"\\n🎯 Final Threshold to Use for Predictions: {final_threshold:.3f}\")\n",
        "print(f\"🎯 Expected Leaderboard F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
        "print(f\"{'='*90}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYeeoWatXOPo",
        "outputId": "c107a5b2-4983-4468-afde-e735b23c274e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "STEP 2: 5-FOLD CROSS-VALIDATION WITH BEST LIGHTGBM PARAMETERS\n",
            "==========================================================================================\n",
            "\n",
            "Running 5-Fold Stratified Cross-Validation using tuned parameters...\n",
            "Each fold finds its optimal threshold from PR curve to maximize F1\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Fold 1/5\n",
            "----------------------------------------\n",
            "  ✅ F1: 0.6128 | ROC-AUC: 0.8504 | PR-AUC: 0.6113\n",
            "  Precision: 0.5693 | Recall: 0.6634 | Threshold: 0.631\n",
            "\n",
            "----------------------------------------\n",
            "Fold 2/5\n",
            "----------------------------------------\n",
            "  ✅ F1: 0.5809 | ROC-AUC: 0.8278 | PR-AUC: 0.5963\n",
            "  Precision: 0.4996 | Recall: 0.6938 | Threshold: 0.591\n",
            "\n",
            "----------------------------------------\n",
            "Fold 3/5\n",
            "----------------------------------------\n",
            "  ✅ F1: 0.6022 | ROC-AUC: 0.8456 | PR-AUC: 0.6000\n",
            "  Precision: 0.5071 | Recall: 0.7412 | Threshold: 0.546\n",
            "\n",
            "----------------------------------------\n",
            "Fold 4/5\n",
            "----------------------------------------\n",
            "  ✅ F1: 0.6149 | ROC-AUC: 0.8442 | PR-AUC: 0.6112\n",
            "  Precision: 0.5529 | Recall: 0.6926 | Threshold: 0.604\n",
            "\n",
            "----------------------------------------\n",
            "Fold 5/5\n",
            "----------------------------------------\n",
            "  ✅ F1: 0.5892 | ROC-AUC: 0.8377 | PR-AUC: 0.5945\n",
            "  Precision: 0.4911 | Recall: 0.7363 | Threshold: 0.539\n",
            "\n",
            "==========================================================================================\n",
            "CROSS-VALIDATION RESULTS SUMMARY\n",
            "==========================================================================================\n",
            " Fold    F1  ROC_AUC  PR_AUC  Precision  Recall  Accuracy  Optimal_Threshold\n",
            "    1 0.613    0.850   0.611      0.569   0.663     0.808              0.631\n",
            "    2 0.581    0.828   0.596      0.500   0.694     0.771              0.591\n",
            "    3 0.602    0.846   0.600      0.507   0.741     0.776              0.546\n",
            "    4 0.615    0.844   0.611      0.553   0.693     0.802              0.604\n",
            "    5 0.589    0.838   0.594      0.491   0.736     0.765              0.539\n",
            "\n",
            "==========================================================================================\n",
            "SUMMARY STATISTICS\n",
            "==========================================================================================\n",
            "Mean F1:        0.6000\n",
            "Std F1:         0.0132\n",
            "95% CI (approx): [0.5741, 0.6259]\n",
            "\n",
            "✅ Average Optimal Threshold from CV: 0.582\n",
            "\n",
            "🎯 Final Threshold to Use for Predictions: 0.582\n",
            "🎯 Expected Leaderboard F1: 0.6000 ± 0.0132\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# global OOF threshold optimization?"
      ],
      "metadata": {
        "id": "HXFN2Slp-oiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 2B: GLOBAL OOF THRESHOLD OPTIMIZATION (FIND SINGLE BEST THRESHOLD)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"GLOBAL OOF THRESHOLD OPTIMIZATION\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# To compute global threshold, we need to collect OOF predictions and true labels\n",
        "oof_probs = np.zeros_like(y_all, dtype=float)  # placeholder for out-of-fold probabilities\n",
        "oof_true = np.array(y_all)\n",
        "\n",
        "# Second pass through folds to generate OOF probabilities\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_all, y_all), 1):\n",
        "    X_train_fold, X_val_fold = X_all.iloc[train_idx], X_all.iloc[val_idx]\n",
        "    y_train_fold = y_all.iloc[train_idx]\n",
        "\n",
        "    model_fold = lgb.LGBMClassifier(**best_lgbm_params)\n",
        "    model_fold.fit(X_train_fold, y_train_fold)\n",
        "    oof_probs[val_idx] = model_fold.predict_proba(X_val_fold)[:, 1]\n",
        "\n",
        "# Now, search globally for the threshold that maximizes F1 on all OOF predictions\n",
        "thresholds = np.arange(0.25, 0.76, 0.01)\n",
        "best_thresh_global = 0.5\n",
        "best_f1_global = 0\n",
        "\n",
        "for t in thresholds:\n",
        "    preds = (oof_probs >= t).astype(int)\n",
        "    f1 = f1_score(oof_true, preds)\n",
        "    if f1 > best_f1_global:\n",
        "        best_f1_global = f1\n",
        "        best_thresh_global = t\n",
        "\n",
        "print(f\"✅ Best Global Threshold: {best_thresh_global:.3f}\")\n",
        "print(f\"✅ Best Global F1 (OOF): {best_f1_global:.4f}\")\n",
        "\n",
        "# For reference, compare it to your average fold threshold\n",
        "print(f\"\\nAverage Fold Threshold: {avg_optimal_threshold:.3f}\")\n",
        "print(f\"Difference: {best_thresh_global - avg_optimal_threshold:+.3f}\")\n",
        "\n",
        "# Decide final threshold (global one is more robust)\n",
        "final_threshold = best_thresh_global\n",
        "\n",
        "print(f\"\\n🎯 Using Final Threshold = {final_threshold:.3f} (based on global OOF optimization)\")\n",
        "print(f\"{'='*90}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFljKjaf-tU0",
        "outputId": "1f7344ca-e0b9-4a12-aa70-a327ab3b660b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "GLOBAL OOF THRESHOLD OPTIMIZATION\n",
            "==========================================================================================\n",
            "✅ Best Global Threshold: 0.560\n",
            "✅ Best Global F1 (OOF): 0.5947\n",
            "\n",
            "Average Fold Threshold: 0.582\n",
            "Difference: -0.022\n",
            "\n",
            "🎯 Using Final Threshold = 0.560 (based on global OOF optimization)\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predicting\n"
      ],
      "metadata": {
        "id": "XF9mW6_QbFaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3: FINAL MODEL - TRAINING ON 100% DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"STEP 3: FINAL MODEL - TRAINING ON 100% DATA\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(f\"\\nTraining final model on full dataset: {X_all.shape[0]} samples...\")\n",
        "print(\"Using best parameters from Optuna (via CV tuning)\")\n",
        "print(f\"Using final threshold from GLOBAL OOF optimization: {final_threshold:.3f}\")\n",
        "\n",
        "# Ensure columns are strings for LightGBM consistency\n",
        "X_all.columns = X_all.columns.astype(str)\n",
        "X_test_all.columns = X_test_all.columns.astype(str)\n",
        "\n",
        "# ============================================================================\n",
        "# Train final LightGBM model on all available data\n",
        "# ============================================================================\n",
        "lgb_final_model = lgb.LGBMClassifier(**best_lgbm_params)\n",
        "lgb_final_model.fit(X_all, y_all)\n",
        "\n",
        "print(\"✓ Final model successfully trained on 100% of data.\")\n",
        "\n",
        "# ============================================================================\n",
        "# Generate predictions on test data\n",
        "# ============================================================================\n",
        "print(\"\\nGenerating final predictions on test set...\")\n",
        "\n",
        "# Make sure test columns align with training\n",
        "X_test_all = X_test_all[X_all.columns]\n",
        "\n",
        "# Get predicted probabilities\n",
        "test_probas = lgb_final_model.predict_proba(X_test_all)[:, 1]\n",
        "\n",
        "# Apply the globally optimized final threshold\n",
        "test_predictions_final = (test_probas >= final_threshold).astype(int)\n",
        "\n",
        "print(f\"✓ Test predictions generated using final threshold = {final_threshold:.3f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Create and save submission file\n",
        "# ============================================================================\n",
        "submission_df = pd.DataFrame({\n",
        "    'claim_number': test_ids,\n",
        "    'subrogation': test_predictions_final\n",
        "})\n",
        "\n",
        "submission_filename = 'submission_final_global_thresh.csv'\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"SUBMISSION FILE CREATED\")\n",
        "print(\"=\"*90)\n",
        "print(f\"✓ File saved as: {submission_filename}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Distribution and sanity check\n",
        "# ============================================================================\n",
        "print(\"\\nFinal prediction distribution:\")\n",
        "print(submission_df['subrogation'].value_counts(normalize=True).to_string())\n",
        "print(f\"\\nTotal positive predictions: {submission_df['subrogation'].sum()} / {len(submission_df)}\")\n",
        "\n",
        "# Optional: Auto-download in Colab environment\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import files\n",
        "    files.download(submission_filename)\n",
        "    print(f\"\\n✓ Downloading {submission_filename}...\")\n",
        "except ImportError:\n",
        "    print(f\"\\n✓ Script complete. Find your file at: {submission_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "ULYJl5IEbH12",
        "outputId": "6563419a-6def-4381-ad9c-436585069741"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "STEP 3: FINAL MODEL - TRAINING ON 100% DATA\n",
            "==========================================================================================\n",
            "\n",
            "Training final model on full dataset: 17999 samples...\n",
            "Using best parameters from Optuna (via CV tuning)\n",
            "Using final threshold from GLOBAL OOF optimization: 0.560\n",
            "✓ Final model successfully trained on 100% of data.\n",
            "\n",
            "Generating final predictions on test set...\n",
            "✓ Test predictions generated using final threshold = 0.560\n",
            "\n",
            "==========================================================================================\n",
            "SUBMISSION FILE CREATED\n",
            "==========================================================================================\n",
            "✓ File saved as: submission_final_global_thresh.csv\n",
            "\n",
            "Final prediction distribution:\n",
            "subrogation\n",
            "0   0.674\n",
            "1   0.326\n",
            "\n",
            "Total positive predictions: 3908 / 12000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7e64d0e4-0764-42c7-87ae-bcb70795dba6\", \"submission_final_global_thresh.csv\", 120025)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Downloading submission_final_global_thresh.csv...\n"
          ]
        }
      ]
    }
  ]
}
---
title: "Predicting Opportunity of Subrogation with Real Claim Data"
subtitle: "Model Citizens, 2025 Travelers UMC"
author: "Wenjie Gong, Cecilia Liu, Simeng Wu, Carol Zhou, Franklin Zhou"
institute: "Department of Statistical Science, Duke University"
format: 
    beamer:
        navigation: horizontal
header-includes:
  - \usepackage{tabularx}
  - \usepackage[none]{hyphenat}
  - \setbeamertemplate{itemize items}[ball]
---

## Introduction

**Business Context**  
Subrogation is a critical part of the claim lifecycle. When a third party is liable, recovery reduces net incurred loss, improves loss ratios, and enhances reserving accuracy—making subrogation a key financial and loss-mitigation lever.

**The Core Challenge**  
Current subrogation identification relies heavily on adjuster judgment and manual file review. This process is slow, inconsistent, and often results in missed recovery opportunities across thousands of claims.


## Subrogation Modeling Framework

**Our Mission**  
Build a predictive model using 2020–2021 first-party physical damage claims to flag potential subrogation opportunities, identify key indicators, and provide recommendations for operational use.

**Modeling Objective**  
Predict a binary outcome *Subrogation Opportunity*  
(1 = likely recovery, 0 = not likely).  
Evaluation Metric: \textcolor{red}{F1 score} (balances precision and recall due to asymmetric business costs).

**Business Value**

- Improve recovery rates and reduce net incurred losses  
- Help subrogation specialists prioritize high-value cases  
- Reduce time spent on low-likelihood opportunities  
- Support data-driven decision-making in the claims process  


## Data Overview
\small

::: columns
::: column

![](num_corr.png){width=100%}

![](target_var.png){width=100%}

:::

::: column

**Dataset Summary**  
- Training data: 18,000 rows with subrogation indicator (0/1)  
- Test data: 12,000 rows without the indicator  
- Features include policyholder, driver, vehicle, accident details, and estimated payout  

**Data Quality Steps**  
- Removed 2 rows with missing subrogation indicator  
- Removed 1 row with all values missing  
- Test dataset contains no NAs  
- Dropped `vehicle_made_year` (post-claim dates $\rightarrow$ impossible)  
- Excluded `age_of_vehicle` (unreliable reporting)  

:::
:::

## Why We Used Multiple Preprocessing Pipelines?
\footnotesize

Each model family has its own optimized pipeline, in accordance with their algorithm characteristics.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.25}
\begin{tabularx}{\textwidth}{l|X|X|X|X|X}
\hline
\textbf{Model} 
& \textbf{TabM} 
& \textbf{Linear Model} 
& \textbf{CatBoost} 
& \textbf{LightGBM} 
& \textbf{XGBoost} \\
\hline
\textbf{Numerical} 
& Normalize (quantile or z-score)
& Normalize (z-score)
& Works well with raw data
& Works well with raw data
& Works well with raw data \\
\hline
\textbf{Categorical} 
& One-Hot Encoding
& One-Hot Encoding
& Native handling
& Native handling
& One-Hot Encoding \\
\hline
\end{tabularx}
\end{table}

## We Leverages Multiple Model Families

- **Linear Model**: Logistic Regression model with LASSO
- **Tree-based Model**: XGBoost, CatBoost, LightGBM
- **MLP**: TabM

| Model          | Private Score | Public Score |
|----------------|---------------|--------------|
| TabM           | 0.58325       | 0.60298      |
| CatBoost       | 0.58215       | 0.60180      |
| XGBoost        | 0.57798       | 0.60352      |
| LogReg + Lasso | 0.57698       | 0.59407      |
| LightGBM       | 0.57467       | 0.59093      |

*Additional models considered*: LogitBoost, Random Forest

## Motivation for Majority Voting: Wisdom of the Crowd

- While we obtained decent individual models, aggregating opinions from multiple models is a good way to optimize bias and variance

\begin{theorem}[Condorcet's Jury Theorem]
If each invidual model outperforms random guessing ($p>0.5$) and is independent, the probability of majority voting being correct approaches 1 as such models are added.
\end{theorem}

- According to Condorcet's Jury Theorem, the majority voting is likely to produce a better prediction than individual models
- The systematic error on specific feature subspaces of each individual model is now overridden by the consensus of others, leading to a smaller model bias
- Overfitting is mitigated as the a single model might obsess over a tiny, unimportant detail in the data, while the group ignores these details and focuses on the main trends

## Majority Voting Ensemble

- Placeholder
- Placeholder

## Why We Choose Majority Voting Ensemble 

Majority Voting is better then stacking, because

- We trained each individual model with a tailor-made preprocessing pipeline, to optimize the individual model performance
- Stacking requires a unified input feature matrix
- If we use one unified preprocessing pipeline to retrain individual models, their performance will greatly degrade

Majority Voting is better than weighted majority, because

- For validation set performance, XGBoost > CatBoost > TabM
- For test set performance (per private score): TabM > CatBoost > XGBoost
- Majority voting algorithm tends to give XGBoost the highest weight
- The resulting ensemble is an "expert" with validation set but a "failure" with test set

## Conclusion

- Placeholder
